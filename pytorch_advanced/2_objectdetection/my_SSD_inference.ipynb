{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SSDの推論の実施\n",
    "pytorchのversion<=1.4が望ましい"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 目標\n",
    "1. 学習して保存したSSDモデルを使用してSSDの推論を実装できるようになる (画像中の物体検出を行う)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## モデルを組み立てて学習済みパラメータをロードする\n",
    "cpuで推論するみたい"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.ssd_model import SSD\n",
    "\n",
    "# SSD300の設定\n",
    "ssd_cfg = {\n",
    "    'num_classes' : 21,                                                                   #　背景クラスを含めた合計クラス数\n",
    "    'input_size' : 300,                                                                     # 画像の入力サイズ\n",
    "    'bbox_aspect_num' : [4, 6, 6, 6, 4, 4],                                  # 出力するDBoxのアスペクト比の種類\n",
    "    'feature_maps' : [38, 19, 10, 5, 3, 1],                                   # 各source（feature map）の画像サイズ　\n",
    "    'steps' : [8, 16, 32, 64, 100, 300],                                        # DBOXの大きさを決める\n",
    "    'min_sizes' : [30, 60, 111, 162, 213, 264],                         # \n",
    "    'max_sizes' : [60, 111, 162, 213, 264, 315],\n",
    "    'aspect_ratios' : [[2], [2, 3], [2, 3], [2, 3], [2], [2]],\n",
    "}\n",
    "\n",
    "# ネットワークモデル\n",
    "net = SSD(phase='train', cfg=ssd_cfg)\n",
    "\n",
    "# SSDの重みの初期値を設定\n",
    "net_weights = torch.load('./weights/ssd300_50.pth')\n",
    "net.load_state_dict(net_weights)\n",
    "\n",
    "# GPUが使えるかを確認\n",
    "# device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "# print('使用デバイス：　', device)\n",
    "\n",
    "print('ネットワーク設定完了：　学習済みの重みをロードしました')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 画像を読み込み前処理をしてSSDモデルで推論"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.ssd_model import DataTramsform\n",
    "\n",
    "# 画像読み込み\n",
    "image_file_path = './data/cowboy-757575_640.jpg'\n",
    "img = cv2.imread(image_file_path)  # 高さ、幅、色[BGR]\n",
    "height, width, channels = img.shape\n",
    "\n",
    "# 元画像を表示\n",
    "plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "plt.show()\n",
    "\n",
    "# 前処理にかける\n",
    "color_mean = (104, 117, 123)  # B,G,Rの平均値\n",
    "input_size = 300\n",
    "transform = DataTramsform(input_size, color_mean)\n",
    "phase = 'val'\n",
    "image_transformed, boxes, labels = transform(img, phase, \"\",\"\")  # アノテーションないので\"\"にする　<- 学習データならboxesとlabels必要だけど\n",
    "img = torch.from_numpy(image_transformed[:,:,(2,1,0)]).permute(2, 0,1)  # BGR->RGBにして、高さ、幅、色-> 色、高さ、幅\n",
    "\n",
    "net.eval()  # 推論モードへ\n",
    "x = img.unsqueeze(0)  # 次元を増やしてミニバッチ化　　torch.Size([1, 3, 300, 300])\n",
    "detections = net(x)  # torch.Size([batch_num, 21, 200(confのtop200), 5(確信度、xmin, ymin, xmax, ymax)])\n",
    "print(detections.shape)\n",
    "print(detections)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 出力テンソルから確信度が一定の閾値以上のBBoxのみを取り出し元画像上に描画して表示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.ssd_predict_show import SSDPredictShow\n",
    "\n",
    "image_file_path = './data/cowboy-757575_640.jpg'\n",
    "\n",
    "# 予測友と画像を表示\n",
    "ssd = SSDPredictShow(eval_categories=voc_classes, net=net)\n",
    "ssd.show(image_file_path, data_confidence_level=0.6)   # 確信度0.6以上の結果のみ表示"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_pytorch_p36)",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
