{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 学習と検証\n",
    "p2.xlargeで12時間\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 目標\n",
    "1. PSPNetの学習と検証の実装\n",
    "2. セマンティックセグメンテーションのファインチューニングを理解"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# パッケージのimport\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.utils.data as data\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 初期設定\n",
    "# Setup seeds\n",
    "torch.manual_seed(1234)\n",
    "np.random.seed(1234)\n",
    "random.seed(1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataLoader作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.dataloader import make_datapath_list, DataTransform, VOCDataset\n",
    "\n",
    "# ファイルパスリスト作成\n",
    "# ２章で使ったディレクトリにアクセスする\n",
    "rootpath = \"../2_objectdetection/data/VOCdevkit/VOC2012/\"\n",
    "train_img_list, train_anno_list, val_img_list, val_anno_list = make_datapath_list(rootpath=rootpath)\n",
    "\n",
    "# Dataset作成\n",
    "# (RGB)の色の平均値と標準偏差\n",
    "color_mean = (0.485, 0.456, 0.406)\n",
    "color_std = (0.229, 0.224, 0.225)\n",
    "\n",
    "train_dataset = VOCDataset(train_img_list, train_anno_list, phase=\"train\", transform=DataTransform(\n",
    "    input_size=475, color_mean=color_mean, color_std=color_std))\n",
    "\n",
    "val_dataset = VOCDataset(val_img_list, val_anno_list, phase=\"val\", transform=DataTransform(\n",
    "    input_size=475, color_mean=color_mean, color_std=color_std))\n",
    "\n",
    "# DataLoader作成\n",
    "batch_size = 4\n",
    "\n",
    "train_dataloader = data.DataLoader(\n",
    "    train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "val_dataloader = data.DataLoader(\n",
    "    val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# 辞書型変数にまとめる\n",
    "dataloaders_dict = {\"train\": train_dataloader, \"val\": val_dataloader}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ネットワークモデル作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ネットワーク設定完了：学習済みモデルの重みをロードしました\n"
     ]
    }
   ],
   "source": [
    "from utils.pspnet import PSPNet\n",
    "\n",
    "# ADE20Kでは１５０クラス分類\n",
    "# モデルの外側を作ってADE20Kの重みをダウンロードして最後のclassificatonの層を付け替える\n",
    "# 付け替えた層の重みをxavierの初期値を使って初期化  今回はクラス分類なのでReLUではなくシグモイドを活性化関数に使うから\n",
    "# ファインチューニングする\n",
    "\n",
    "net = PSPNet(n_classes=150)\n",
    "state_dict = torch.load('./weights/pspnet50_ADE20K.pth')   # aws上にダウンロードしてあれば良い\n",
    "\n",
    "n_classes = 21\n",
    "net.decode_feature.classification = nn.Conv2d(in_channels=512, out_channels=n_classes, kernel_size=1, stride=1, padding=0)\n",
    "net.aux.classification = nn.Conv2d(in_channels=256, out_channels=n_classes, kernel_size=1, stride=1, padding=0)\n",
    "\n",
    "def weights_init(m):\n",
    "    if isinstance(m, nn.Conv2d):\n",
    "        nn.init.xavier_normal_(m.weight.data)\n",
    "        if m.bias is not None:  # バイアス項がある場合\n",
    "            nn.init.constant_(m.bias, 0.0)\n",
    "            \n",
    "net.decode_feature.classification.apply(weights_init)\n",
    "net.aux.classification.apply(weights_init)\n",
    "\n",
    "print('ネットワーク設定完了：学習済みモデルの重みをロードしました') \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PSPNet(\n",
       "  (feature_conv): FeatureMap_convolution(\n",
       "    (cbnr_1): conv2DBatchNormRelu(\n",
       "      (conv): Conv2d(3, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (batchnorm): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (cbnr_2): conv2DBatchNormRelu(\n",
       "      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (batchnorm): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (cbnr_3): conv2DBatchNormRelu(\n",
       "      (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (batchnorm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (feature_res_1): ResidualBlockPSP(\n",
       "    (block1): bottleNeckPSP(\n",
       "      (cbr_1): conv2DBatchNormRelu(\n",
       "        (conv): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (batchnorm): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (cbr_2): conv2DBatchNormRelu(\n",
       "        (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (batchnorm): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (cb_3): conv2DBatchNorm(\n",
       "        (conv): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (batchnorm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (cb_residual): conv2DBatchNorm(\n",
       "        (conv): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (batchnorm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (block2): bottleNeckIdentifyPSP(\n",
       "      (cbr_1): conv2DBatchNormRelu(\n",
       "        (conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (batchnorm): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (cbr_2): conv2DBatchNormRelu(\n",
       "        (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (batchnorm): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (cb_3): conv2DBatchNorm(\n",
       "        (conv): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (batchnorm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (block3): bottleNeckIdentifyPSP(\n",
       "      (cbr_1): conv2DBatchNormRelu(\n",
       "        (conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (batchnorm): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (cbr_2): conv2DBatchNormRelu(\n",
       "        (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (batchnorm): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (cb_3): conv2DBatchNorm(\n",
       "        (conv): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (batchnorm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (feature_res_2): ResidualBlockPSP(\n",
       "    (block1): bottleNeckPSP(\n",
       "      (cbr_1): conv2DBatchNormRelu(\n",
       "        (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (batchnorm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (cbr_2): conv2DBatchNormRelu(\n",
       "        (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (batchnorm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (cb_3): conv2DBatchNorm(\n",
       "        (conv): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (batchnorm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (cb_residual): conv2DBatchNorm(\n",
       "        (conv): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (batchnorm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (block2): bottleNeckIdentifyPSP(\n",
       "      (cbr_1): conv2DBatchNormRelu(\n",
       "        (conv): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (batchnorm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (cbr_2): conv2DBatchNormRelu(\n",
       "        (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (batchnorm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (cb_3): conv2DBatchNorm(\n",
       "        (conv): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (batchnorm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (block3): bottleNeckIdentifyPSP(\n",
       "      (cbr_1): conv2DBatchNormRelu(\n",
       "        (conv): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (batchnorm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (cbr_2): conv2DBatchNormRelu(\n",
       "        (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (batchnorm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (cb_3): conv2DBatchNorm(\n",
       "        (conv): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (batchnorm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (block4): bottleNeckIdentifyPSP(\n",
       "      (cbr_1): conv2DBatchNormRelu(\n",
       "        (conv): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (batchnorm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (cbr_2): conv2DBatchNormRelu(\n",
       "        (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (batchnorm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (cb_3): conv2DBatchNorm(\n",
       "        (conv): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (batchnorm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (feature_dilated_res_1): ResidualBlockPSP(\n",
       "    (block1): bottleNeckPSP(\n",
       "      (cbr_1): conv2DBatchNormRelu(\n",
       "        (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (batchnorm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (cbr_2): conv2DBatchNormRelu(\n",
       "        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
       "        (batchnorm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (cb_3): conv2DBatchNorm(\n",
       "        (conv): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (batchnorm): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (cb_residual): conv2DBatchNorm(\n",
       "        (conv): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (batchnorm): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (block2): bottleNeckIdentifyPSP(\n",
       "      (cbr_1): conv2DBatchNormRelu(\n",
       "        (conv): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (batchnorm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (cbr_2): conv2DBatchNormRelu(\n",
       "        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
       "        (batchnorm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (cb_3): conv2DBatchNorm(\n",
       "        (conv): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (batchnorm): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (block3): bottleNeckIdentifyPSP(\n",
       "      (cbr_1): conv2DBatchNormRelu(\n",
       "        (conv): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (batchnorm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (cbr_2): conv2DBatchNormRelu(\n",
       "        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
       "        (batchnorm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (cb_3): conv2DBatchNorm(\n",
       "        (conv): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (batchnorm): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (block4): bottleNeckIdentifyPSP(\n",
       "      (cbr_1): conv2DBatchNormRelu(\n",
       "        (conv): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (batchnorm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (cbr_2): conv2DBatchNormRelu(\n",
       "        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
       "        (batchnorm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (cb_3): conv2DBatchNorm(\n",
       "        (conv): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (batchnorm): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (block5): bottleNeckIdentifyPSP(\n",
       "      (cbr_1): conv2DBatchNormRelu(\n",
       "        (conv): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (batchnorm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (cbr_2): conv2DBatchNormRelu(\n",
       "        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
       "        (batchnorm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (cb_3): conv2DBatchNorm(\n",
       "        (conv): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (batchnorm): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (block6): bottleNeckIdentifyPSP(\n",
       "      (cbr_1): conv2DBatchNormRelu(\n",
       "        (conv): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (batchnorm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (cbr_2): conv2DBatchNormRelu(\n",
       "        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
       "        (batchnorm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (cb_3): conv2DBatchNorm(\n",
       "        (conv): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (batchnorm): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (feature_dilated_res_2): ResidualBlockPSP(\n",
       "    (block1): bottleNeckPSP(\n",
       "      (cbr_1): conv2DBatchNormRelu(\n",
       "        (conv): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (batchnorm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (cbr_2): conv2DBatchNormRelu(\n",
       "        (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(4, 4), dilation=(4, 4), bias=False)\n",
       "        (batchnorm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (cb_3): conv2DBatchNorm(\n",
       "        (conv): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (batchnorm): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (cb_residual): conv2DBatchNorm(\n",
       "        (conv): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (batchnorm): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (block2): bottleNeckIdentifyPSP(\n",
       "      (cbr_1): conv2DBatchNormRelu(\n",
       "        (conv): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (batchnorm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (cbr_2): conv2DBatchNormRelu(\n",
       "        (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(4, 4), dilation=(4, 4), bias=False)\n",
       "        (batchnorm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (cb_3): conv2DBatchNorm(\n",
       "        (conv): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (batchnorm): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (block3): bottleNeckIdentifyPSP(\n",
       "      (cbr_1): conv2DBatchNormRelu(\n",
       "        (conv): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (batchnorm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (cbr_2): conv2DBatchNormRelu(\n",
       "        (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(4, 4), dilation=(4, 4), bias=False)\n",
       "        (batchnorm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (cb_3): conv2DBatchNorm(\n",
       "        (conv): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (batchnorm): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (pyramid_pooling): PyramidPooling(\n",
       "    (avpool_1): AdaptiveAvgPool2d(output_size=6)\n",
       "    (cbr_1): conv2DBatchNormRelu(\n",
       "      (conv): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (batchnorm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (avpool_2): AdaptiveAvgPool2d(output_size=3)\n",
       "    (cbr_2): conv2DBatchNormRelu(\n",
       "      (conv): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (batchnorm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (avpool_3): AdaptiveAvgPool2d(output_size=2)\n",
       "    (cbr_3): conv2DBatchNormRelu(\n",
       "      (conv): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (batchnorm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (avpool_4): AdaptiveAvgPool2d(output_size=1)\n",
       "    (cbr_4): conv2DBatchNormRelu(\n",
       "      (conv): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (batchnorm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (decode_feature): DecodePSPFeature(\n",
       "    (cbr): conv2DBatchNormRelu(\n",
       "      (conv): Conv2d(4096, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (batchnorm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (dropout): Dropout2d(p=0.1, inplace=False)\n",
       "    (classification): Conv2d(512, 21, kernel_size=(1, 1), stride=(1, 1))\n",
       "  )\n",
       "  (aux): AuxiliaryPSPlayers(\n",
       "    (cbr): conv2DBatchNormRelu(\n",
       "      (conv): Conv2d(1024, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (batchnorm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (dropout): Dropout2d(p=0.1, inplace=False)\n",
       "    (classification): Conv2d(256, 21, kernel_size=(1, 1), stride=(1, 1))\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 損失関数定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PSPLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    PSPNetの損失関数クラス\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, aux_weight=0.4):\n",
    "        super(PSPLoss, self).__init__()\n",
    "        self.aux_weight = aux_weight\n",
    "        \n",
    "    def forward(self, outputs, targets):\n",
    "        \"\"\"\n",
    "        損失関数の計算\n",
    "        \n",
    "        Parameters\n",
    "        ----------------\n",
    "        outputs : PSPNetの出力(tuple)\n",
    "            (output=torch.Size([num_batch, 21, 475, 475]), output_aux=torch.Size([num_batch, 21, 475, 475]))。\n",
    "        \n",
    "        targets : [num_batch, 475, 475]\n",
    "            正解のアノテーション情報\n",
    "            \n",
    "        Returns\n",
    "        ----------------\n",
    "        loss : テンソル\n",
    "            損失の値（普通のlossとauxのlossを足したもの）\n",
    "            \n",
    "        \"\"\"\n",
    "        \n",
    "        loss = F.cross_entropy(outputs[0], targets, reduction='mean')\n",
    "        aux_loss = F.cross_entropy(outputs[1], targets, reduction='mean')\n",
    "        \n",
    "        return loss + self.aux_weight*aux_loss\n",
    "    \n",
    "criterion = PSPLoss(aux_weight=0.4)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 最適化手法定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ファインチューニングなので学習率は小さめに設定しておく\n",
    "optimizer = optim.SGD([\n",
    "    {'params': net.feature_conv.parameters(), 'lr': 1e-3},\n",
    "    {'params': net.feature_res_1.parameters(), 'lr': 1e-3},\n",
    "    {'params': net.feature_res_2.parameters(), 'lr': 1e-3},\n",
    "    {'params': net.feature_dilated_res_1.parameters(), 'lr': 1e-3},\n",
    "    {'params': net.feature_dilated_res_2.parameters(), 'lr': 1e-3},\n",
    "    {'params': net.pyramid_pooling.parameters(), 'lr': 1e-3},\n",
    "    {'params': net.decode_feature.parameters(), 'lr': 1e-2},\n",
    "    {'params': net.aux.parameters(), 'lr': 1e-2},\n",
    "], momentum=0.9, weight_decay=0.0001)\n",
    "\n",
    "# スケジューラの設定\n",
    " # 今回はepochごとに学習率を小さくしていく\n",
    "def lambda_epoch(epoch):\n",
    "    max_epoch=30\n",
    "    return math.pow((1-epoch/max_epoch), 0.9)\n",
    "\n",
    "scheduler = optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda_epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 学習・検証"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# モデルを学習させる関数\n",
    "def train_model(net, detaloaders_dict, criterion, scheduler, optimizer, num_epoch):\n",
    "    \n",
    "    # gpu or cpu\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(\"使用デバイス：\", device)\n",
    "\n",
    "    # ネットワークをGPUへ\n",
    "    net.to(device)\n",
    "\n",
    "    # 高速化させる\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    \n",
    "    # 画像の枚数\n",
    "    num_train_imgs = len(dataloaders_dict['train'].dataset)\n",
    "    num_val_imgs = len(dataloaders_dict['val'].dataset)\n",
    "    batch_size = dataloaders_dict['train'].batch_size\n",
    "    \n",
    "    iteration = 1\n",
    "    logs = []\n",
    "    batch_multiplier = 3\n",
    "    \n",
    "    # epochのループ\n",
    "    for epoch in range(num_epochs):\n",
    "        \n",
    "        # 開始時刻の保存\n",
    "        t_epoch_start = time.time()\n",
    "        t_iter_start = time.time()\n",
    "        epoch_train_loss = 0.0\n",
    "        epoch_val_loss = 0.0\n",
    "        \n",
    "        print('-------------')\n",
    "        print('Epoch {}/{}'.format(epoch+1, num_epochs))\n",
    "        print('-------------')\n",
    "        \n",
    "        # trainとvalのループ\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                net.train()\n",
    "                scheduler.step()  # 最適化schedulerの更新\n",
    "                optimizer.zero_grad()\n",
    "                print('(train)')\n",
    "                \n",
    "            else:\n",
    "                # 検証は５回に１回だけ行う\n",
    "                if ((epoch+1)%5 == 0):\n",
    "                    net.eval()\n",
    "                    print('-------------')\n",
    "                    print('(val)')\n",
    "                else:\n",
    "                    continue\n",
    "            \n",
    "            # minibatchを取り出すループ\n",
    "            count = 0\n",
    "            for images, anno_class_images, in dataloaders_dict[phase]:\n",
    "                if images.size()[0] == 1:\n",
    "                    continue\n",
    "                    \n",
    "                # gpuにデータを送る\n",
    "                images = images.to(device)\n",
    "                anno_class_images = anno_class_images.to(device)\n",
    "                \n",
    "                # multiple minibatchでのパラメータ更新\n",
    "                if (phase == 'train') and (count==0):\n",
    "                    optimizer.step()\n",
    "                    optimizer.zero_grad()\n",
    "                    count = batch_multiplier  # 3\n",
    "                    \n",
    "                # forwardの計算\n",
    "                with torch.set_grad_enabled(phase=='train'):\n",
    "                    outputs = net(images)\n",
    "                    loss = criterion(outputs, anno_class_images.long()) / batch_multiplier\n",
    "                    \n",
    "                    # 訓練時はbackprop\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        count -= 1\n",
    "                        \n",
    "                        if iteration%10 == 0:\n",
    "                            t_iter_finish = time.time()\n",
    "                            duration = t_iter_finish - t_iter_start\n",
    "                            print('イテレーション{} || Loss: {:.4f} || 10iter: {:.4f} sec'.format(iteration, loss.item()/batch_size*batch_multiplier, duration))\n",
    "                            t_iter_start = time.time()\n",
    "                            \n",
    "                        epoch_train_loss += loss.item()*batch_multiplier\n",
    "                        iteration += 1\n",
    "                        \n",
    "                    # 検証時はloss加算するだけ\n",
    "                    epoch_val_loss += loss.item() * batch_multiplier\n",
    "                    \n",
    "        # epochのphaseごとのlossと時間\n",
    "        t_epoch_finish = time.time()\n",
    "        print('------------')\n",
    "        print('epoch {} || Epoch_TRAIN_Loss:{:.4f} || Epoch_VAL_Loss: {:.4f}'.format(epoch+1, epoch_train_loss/num_train_imgs, epoch_val_loss/num_val_imgs))\n",
    "        print('timer: {:.4f} sec'.format(t_epoch_finish - t_epoch_start))\n",
    "        t_epoch_start = time.time()\n",
    "        \n",
    "        # ログの保存\n",
    "        log_epoch = {'epoch': epoch+1, \n",
    "                     'train_loss': epoch_train_loss/num_train_imgs,\n",
    "                    'val_loss': epoch_val_loss/num_val_imgs}\n",
    "        logs.append(log_epoch)\n",
    "        df = pd.DataFrame(logs)\n",
    "        df.to_csv('log_output.csv')\n",
    "        \n",
    "    # 最後のネットワークを保存する\n",
    "    torch.save(net.state_dict(), 'weights/pspnet50_'+str(epoch+1)+'.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "使用デバイス： cuda:0\n",
      "-------------\n",
      "Epoch 1/30\n",
      "-------------\n",
      "(train)\n",
      "イテレーション10 || Loss: 0.5707 || 10iter: 26.0309 sec\n",
      "イテレーション20 || Loss: 0.5270 || 10iter: 26.0447 sec\n",
      "イテレーション30 || Loss: 0.5000 || 10iter: 26.0801 sec\n",
      "イテレーション40 || Loss: 0.4331 || 10iter: 26.0617 sec\n",
      "イテレーション50 || Loss: 0.3069 || 10iter: 26.0655 sec\n",
      "イテレーション60 || Loss: 0.5151 || 10iter: 26.0040 sec\n",
      "イテレーション70 || Loss: 0.4980 || 10iter: 26.0672 sec\n",
      "イテレーション80 || Loss: 0.2151 || 10iter: 26.0345 sec\n",
      "イテレーション90 || Loss: 0.3101 || 10iter: 26.0964 sec\n",
      "イテレーション100 || Loss: 0.5349 || 10iter: 26.0574 sec\n",
      "イテレーション110 || Loss: 0.3302 || 10iter: 26.0324 sec\n",
      "イテレーション120 || Loss: 0.3808 || 10iter: 26.1123 sec\n",
      "イテレーション130 || Loss: 0.2976 || 10iter: 26.0656 sec\n",
      "イテレーション140 || Loss: 0.4505 || 10iter: 26.1194 sec\n",
      "イテレーション150 || Loss: 0.2103 || 10iter: 26.1251 sec\n",
      "イテレーション160 || Loss: 0.3192 || 10iter: 26.2047 sec\n",
      "イテレーション170 || Loss: 0.4700 || 10iter: 26.1627 sec\n",
      "イテレーション180 || Loss: 0.2571 || 10iter: 26.1640 sec\n",
      "イテレーション190 || Loss: 0.2808 || 10iter: 26.0886 sec\n",
      "イテレーション200 || Loss: 0.5153 || 10iter: 26.1466 sec\n",
      "イテレーション210 || Loss: 0.2317 || 10iter: 26.0903 sec\n",
      "イテレーション220 || Loss: 0.1894 || 10iter: 26.0372 sec\n",
      "イテレーション230 || Loss: 0.3929 || 10iter: 26.0441 sec\n",
      "イテレーション240 || Loss: 0.2634 || 10iter: 26.0846 sec\n",
      "イテレーション250 || Loss: 0.2141 || 10iter: 26.0836 sec\n",
      "イテレーション260 || Loss: 0.4560 || 10iter: 26.0277 sec\n",
      "イテレーション270 || Loss: 0.1818 || 10iter: 26.0540 sec\n",
      "イテレーション280 || Loss: 0.5415 || 10iter: 26.0914 sec\n",
      "イテレーション290 || Loss: 0.3297 || 10iter: 26.0576 sec\n",
      "イテレーション300 || Loss: 0.2680 || 10iter: 26.0824 sec\n",
      "イテレーション310 || Loss: 0.1795 || 10iter: 26.0984 sec\n",
      "イテレーション320 || Loss: 0.5248 || 10iter: 26.1251 sec\n",
      "イテレーション330 || Loss: 0.3927 || 10iter: 26.1306 sec\n",
      "イテレーション340 || Loss: 0.1186 || 10iter: 26.0522 sec\n",
      "イテレーション350 || Loss: 0.1983 || 10iter: 26.0889 sec\n",
      "イテレーション360 || Loss: 0.3413 || 10iter: 26.1286 sec\n",
      "------------\n",
      "epoch 1 || Epoch_TRAIN_Loss:0.3754 || Epoch_VAL_Loss: 0.3793\n",
      "timer: 1054.8006 sec\n",
      "-------------\n",
      "Epoch 2/30\n",
      "-------------\n",
      "(train)\n",
      "イテレーション370 || Loss: 0.6765 || 10iter: 8.7837 sec\n",
      "イテレーション380 || Loss: 0.2911 || 10iter: 26.0568 sec\n",
      "イテレーション390 || Loss: 0.6636 || 10iter: 26.1499 sec\n",
      "イテレーション400 || Loss: 0.4726 || 10iter: 26.1063 sec\n",
      "イテレーション410 || Loss: 0.3835 || 10iter: 26.0928 sec\n",
      "イテレーション420 || Loss: 0.3063 || 10iter: 26.0753 sec\n",
      "イテレーション430 || Loss: 0.4962 || 10iter: 26.0776 sec\n",
      "イテレーション440 || Loss: 0.3721 || 10iter: 26.0498 sec\n",
      "イテレーション450 || Loss: 0.4248 || 10iter: 26.0745 sec\n",
      "イテレーション460 || Loss: 0.4587 || 10iter: 26.1440 sec\n",
      "イテレーション470 || Loss: 0.3711 || 10iter: 26.1110 sec\n",
      "イテレーション480 || Loss: 0.2674 || 10iter: 26.1189 sec\n",
      "イテレーション490 || Loss: 0.4300 || 10iter: 26.1723 sec\n",
      "イテレーション500 || Loss: 0.2339 || 10iter: 26.0929 sec\n",
      "イテレーション510 || Loss: 0.3405 || 10iter: 26.1922 sec\n",
      "イテレーション520 || Loss: 0.3691 || 10iter: 26.2402 sec\n",
      "イテレーション530 || Loss: 0.8519 || 10iter: 26.3954 sec\n",
      "イテレーション540 || Loss: 0.4816 || 10iter: 26.1476 sec\n",
      "イテレーション550 || Loss: 0.3220 || 10iter: 26.3140 sec\n",
      "イテレーション560 || Loss: 0.3945 || 10iter: 26.2099 sec\n",
      "イテレーション570 || Loss: 0.4537 || 10iter: 26.3772 sec\n",
      "イテレーション580 || Loss: 0.1857 || 10iter: 26.1486 sec\n",
      "イテレーション590 || Loss: 0.5879 || 10iter: 26.1444 sec\n",
      "イテレーション600 || Loss: 0.4579 || 10iter: 26.0881 sec\n",
      "イテレーション610 || Loss: 0.2298 || 10iter: 27.5329 sec\n",
      "イテレーション620 || Loss: 0.2141 || 10iter: 27.8267 sec\n",
      "イテレーション630 || Loss: 0.4804 || 10iter: 27.8638 sec\n",
      "イテレーション640 || Loss: 0.4404 || 10iter: 27.8815 sec\n",
      "イテレーション650 || Loss: 0.3707 || 10iter: 26.1443 sec\n",
      "イテレーション660 || Loss: 0.1495 || 10iter: 26.0470 sec\n",
      "イテレーション670 || Loss: 0.3456 || 10iter: 26.3531 sec\n",
      "イテレーション680 || Loss: 0.1441 || 10iter: 26.0664 sec\n",
      "イテレーション690 || Loss: 0.2359 || 10iter: 26.0620 sec\n",
      "イテレーション700 || Loss: 0.2396 || 10iter: 26.0628 sec\n",
      "イテレーション710 || Loss: 0.4347 || 10iter: 27.4475 sec\n",
      "イテレーション720 || Loss: 0.5620 || 10iter: 27.3615 sec\n",
      "イテレーション730 || Loss: 0.3698 || 10iter: 27.5439 sec\n",
      "------------\n",
      "epoch 2 || Epoch_TRAIN_Loss:0.3677 || Epoch_VAL_Loss: 0.3715\n",
      "timer: 1069.3715 sec\n",
      "-------------\n",
      "Epoch 3/30\n",
      "-------------\n",
      "(train)\n",
      "イテレーション740 || Loss: 0.5214 || 10iter: 21.1932 sec\n",
      "イテレーション750 || Loss: 0.1631 || 10iter: 25.9841 sec\n",
      "イテレーション760 || Loss: 0.5103 || 10iter: 25.9671 sec\n",
      "イテレーション770 || Loss: 0.1840 || 10iter: 25.9536 sec\n",
      "イテレーション780 || Loss: 0.3648 || 10iter: 25.9644 sec\n",
      "イテレーション790 || Loss: 0.3422 || 10iter: 25.9825 sec\n",
      "イテレーション800 || Loss: 0.1943 || 10iter: 25.8768 sec\n",
      "イテレーション810 || Loss: 0.3859 || 10iter: 25.9771 sec\n",
      "イテレーション820 || Loss: 0.1905 || 10iter: 26.0268 sec\n",
      "イテレーション830 || Loss: 0.4221 || 10iter: 25.9997 sec\n",
      "イテレーション840 || Loss: 0.2353 || 10iter: 26.0049 sec\n",
      "イテレーション850 || Loss: 0.2174 || 10iter: 25.9525 sec\n",
      "イテレーション860 || Loss: 0.4120 || 10iter: 25.9782 sec\n",
      "イテレーション870 || Loss: 0.5456 || 10iter: 25.9877 sec\n",
      "イテレーション880 || Loss: 0.4424 || 10iter: 25.9691 sec\n",
      "イテレーション890 || Loss: 0.2975 || 10iter: 25.9898 sec\n",
      "イテレーション900 || Loss: 0.4087 || 10iter: 26.0292 sec\n",
      "イテレーション910 || Loss: 0.3910 || 10iter: 25.9741 sec\n",
      "イテレーション920 || Loss: 0.2879 || 10iter: 25.9802 sec\n",
      "イテレーション930 || Loss: 0.4933 || 10iter: 25.9976 sec\n",
      "イテレーション940 || Loss: 0.2952 || 10iter: 26.0212 sec\n",
      "イテレーション950 || Loss: 0.5627 || 10iter: 26.0376 sec\n",
      "イテレーション960 || Loss: 0.3416 || 10iter: 25.9940 sec\n",
      "イテレーション970 || Loss: 0.1758 || 10iter: 26.0091 sec\n",
      "イテレーション980 || Loss: 0.1987 || 10iter: 26.0179 sec\n",
      "イテレーション990 || Loss: 0.1887 || 10iter: 26.0016 sec\n",
      "イテレーション1000 || Loss: 0.3279 || 10iter: 26.0039 sec\n",
      "イテレーション1010 || Loss: 0.4702 || 10iter: 26.0761 sec\n",
      "イテレーション1020 || Loss: 0.4655 || 10iter: 26.0231 sec\n",
      "イテレーション1030 || Loss: 0.2161 || 10iter: 26.0405 sec\n",
      "イテレーション1040 || Loss: 0.4768 || 10iter: 26.0137 sec\n",
      "イテレーション1050 || Loss: 0.2866 || 10iter: 25.9958 sec\n",
      "イテレーション1060 || Loss: 0.3977 || 10iter: 25.9870 sec\n",
      "イテレーション1070 || Loss: 0.2243 || 10iter: 25.9475 sec\n",
      "イテレーション1080 || Loss: 0.3667 || 10iter: 25.9967 sec\n",
      "イテレーション1090 || Loss: 0.3169 || 10iter: 26.0313 sec\n",
      "------------\n",
      "epoch 3 || Epoch_TRAIN_Loss:0.3549 || Epoch_VAL_Loss: 0.3585\n",
      "timer: 1052.0754 sec\n",
      "-------------\n",
      "Epoch 4/30\n",
      "-------------\n",
      "(train)\n",
      "イテレーション1100 || Loss: 0.3540 || 10iter: 3.0100 sec\n",
      "イテレーション1110 || Loss: 0.5343 || 10iter: 26.0493 sec\n",
      "イテレーション1120 || Loss: 0.4402 || 10iter: 26.0056 sec\n",
      "イテレーション1130 || Loss: 0.5282 || 10iter: 26.0664 sec\n",
      "イテレーション1140 || Loss: 0.5077 || 10iter: 26.0626 sec\n",
      "イテレーション1150 || Loss: 0.5175 || 10iter: 26.0794 sec\n",
      "イテレーション1160 || Loss: 0.4376 || 10iter: 26.0356 sec\n",
      "イテレーション1170 || Loss: 0.4032 || 10iter: 26.0727 sec\n",
      "イテレーション1180 || Loss: 0.1982 || 10iter: 26.0141 sec\n",
      "イテレーション1190 || Loss: 0.2277 || 10iter: 26.0149 sec\n",
      "イテレーション1200 || Loss: 0.5069 || 10iter: 26.0967 sec\n",
      "イテレーション1210 || Loss: 0.5853 || 10iter: 25.9937 sec\n",
      "イテレーション1220 || Loss: 0.2690 || 10iter: 25.9376 sec\n",
      "イテレーション1230 || Loss: 0.3035 || 10iter: 26.0110 sec\n",
      "イテレーション1240 || Loss: 0.3371 || 10iter: 26.0097 sec\n",
      "イテレーション1250 || Loss: 0.3061 || 10iter: 25.9825 sec\n",
      "イテレーション1260 || Loss: 0.3751 || 10iter: 25.9840 sec\n",
      "イテレーション1270 || Loss: 0.2275 || 10iter: 25.9587 sec\n",
      "イテレーション1280 || Loss: 0.4220 || 10iter: 25.9260 sec\n",
      "イテレーション1290 || Loss: 0.3695 || 10iter: 26.0009 sec\n",
      "イテレーション1300 || Loss: 0.5575 || 10iter: 25.9602 sec\n",
      "イテレーション1310 || Loss: 0.3097 || 10iter: 25.8758 sec\n",
      "イテレーション1320 || Loss: 0.2757 || 10iter: 25.9260 sec\n",
      "イテレーション1330 || Loss: 0.4422 || 10iter: 25.9872 sec\n",
      "イテレーション1340 || Loss: 0.1163 || 10iter: 25.9278 sec\n",
      "イテレーション1350 || Loss: 0.7477 || 10iter: 25.9721 sec\n",
      "イテレーション1360 || Loss: 0.6788 || 10iter: 26.0062 sec\n",
      "イテレーション1370 || Loss: 0.4568 || 10iter: 25.9570 sec\n",
      "イテレーション1380 || Loss: 0.4553 || 10iter: 26.0054 sec\n",
      "イテレーション1390 || Loss: 0.2709 || 10iter: 25.9448 sec\n",
      "イテレーション1400 || Loss: 0.4216 || 10iter: 25.9555 sec\n",
      "イテレーション1410 || Loss: 0.2293 || 10iter: 25.9496 sec\n",
      "イテレーション1420 || Loss: 0.1433 || 10iter: 25.9394 sec\n",
      "イテレーション1430 || Loss: 0.4513 || 10iter: 25.9252 sec\n",
      "イテレーション1440 || Loss: 0.4300 || 10iter: 25.9614 sec\n",
      "イテレーション1450 || Loss: 0.2426 || 10iter: 25.9971 sec\n",
      "イテレーション1460 || Loss: 0.2797 || 10iter: 26.0115 sec\n",
      "------------\n",
      "epoch 4 || Epoch_TRAIN_Loss:0.3606 || Epoch_VAL_Loss: 0.3643\n",
      "timer: 1050.9468 sec\n",
      "-------------\n",
      "Epoch 5/30\n",
      "-------------\n",
      "(train)\n",
      "イテレーション1470 || Loss: 0.2277 || 10iter: 14.5433 sec\n",
      "イテレーション1480 || Loss: 0.2148 || 10iter: 26.0913 sec\n",
      "イテレーション1490 || Loss: 0.1990 || 10iter: 26.0780 sec\n",
      "イテレーション1500 || Loss: 0.3856 || 10iter: 26.0991 sec\n",
      "イテレーション1510 || Loss: 0.4330 || 10iter: 26.0625 sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "イテレーション1520 || Loss: 0.2571 || 10iter: 26.0602 sec\n",
      "イテレーション1530 || Loss: 0.2218 || 10iter: 26.0685 sec\n",
      "イテレーション1540 || Loss: 0.3203 || 10iter: 26.0490 sec\n",
      "イテレーション1550 || Loss: 0.4831 || 10iter: 26.0845 sec\n",
      "イテレーション1560 || Loss: 0.4073 || 10iter: 26.0239 sec\n",
      "イテレーション1570 || Loss: 0.2026 || 10iter: 26.0859 sec\n",
      "イテレーション1580 || Loss: 0.5153 || 10iter: 26.0438 sec\n",
      "イテレーション1590 || Loss: 0.2155 || 10iter: 26.1078 sec\n",
      "イテレーション1600 || Loss: 0.3649 || 10iter: 26.0865 sec\n",
      "イテレーション1610 || Loss: 0.4950 || 10iter: 26.0903 sec\n",
      "イテレーション1620 || Loss: 0.9208 || 10iter: 26.1016 sec\n",
      "イテレーション1630 || Loss: 0.2263 || 10iter: 26.1272 sec\n",
      "イテレーション1640 || Loss: 0.4235 || 10iter: 26.1845 sec\n",
      "イテレーション1650 || Loss: 0.3600 || 10iter: 26.1603 sec\n",
      "イテレーション1660 || Loss: 0.3208 || 10iter: 26.1414 sec\n",
      "イテレーション1670 || Loss: 0.7818 || 10iter: 26.1273 sec\n",
      "イテレーション1680 || Loss: 0.1860 || 10iter: 26.0987 sec\n",
      "イテレーション1690 || Loss: 0.3069 || 10iter: 26.0989 sec\n",
      "イテレーション1700 || Loss: 0.3534 || 10iter: 26.0660 sec\n",
      "イテレーション1710 || Loss: 0.3849 || 10iter: 26.0795 sec\n",
      "イテレーション1720 || Loss: 0.2793 || 10iter: 26.0697 sec\n",
      "イテレーション1730 || Loss: 0.3694 || 10iter: 26.0369 sec\n",
      "イテレーション1740 || Loss: 0.3741 || 10iter: 26.0045 sec\n",
      "イテレーション1750 || Loss: 0.4753 || 10iter: 26.0617 sec\n",
      "イテレーション1760 || Loss: 0.3360 || 10iter: 26.0262 sec\n",
      "イテレーション1770 || Loss: 0.5061 || 10iter: 26.1622 sec\n",
      "イテレーション1780 || Loss: 0.3802 || 10iter: 26.1087 sec\n",
      "イテレーション1790 || Loss: 0.2783 || 10iter: 26.1114 sec\n",
      "イテレーション1800 || Loss: 0.3840 || 10iter: 26.0726 sec\n",
      "イテレーション1810 || Loss: 0.2789 || 10iter: 26.0246 sec\n",
      "イテレーション1820 || Loss: 0.2268 || 10iter: 26.0430 sec\n",
      "イテレーション1830 || Loss: 0.4965 || 10iter: 26.0682 sec\n",
      "-------------\n",
      "(val)\n",
      "------------\n",
      "epoch 5 || Epoch_TRAIN_Loss:0.3540 || Epoch_VAL_Loss: 0.7530\n",
      "timer: 1437.0201 sec\n",
      "-------------\n",
      "Epoch 6/30\n",
      "-------------\n",
      "(train)\n",
      "イテレーション1840 || Loss: 0.4801 || 10iter: 26.2195 sec\n",
      "イテレーション1850 || Loss: 0.2863 || 10iter: 26.2280 sec\n",
      "イテレーション1860 || Loss: 0.1621 || 10iter: 26.1430 sec\n",
      "イテレーション1870 || Loss: 0.3790 || 10iter: 26.1663 sec\n",
      "イテレーション1880 || Loss: 0.3965 || 10iter: 26.2519 sec\n",
      "イテレーション1890 || Loss: 0.2284 || 10iter: 26.1365 sec\n",
      "イテレーション1900 || Loss: 0.4791 || 10iter: 26.1017 sec\n",
      "イテレーション1910 || Loss: 0.2009 || 10iter: 26.1184 sec\n",
      "イテレーション1920 || Loss: 0.2988 || 10iter: 26.1564 sec\n",
      "イテレーション1930 || Loss: 0.4789 || 10iter: 26.1463 sec\n",
      "イテレーション1940 || Loss: 0.6315 || 10iter: 26.0859 sec\n",
      "イテレーション1950 || Loss: 0.4164 || 10iter: 26.0782 sec\n",
      "イテレーション1960 || Loss: 0.2863 || 10iter: 26.0933 sec\n",
      "イテレーション1970 || Loss: 0.2460 || 10iter: 26.0767 sec\n",
      "イテレーション1980 || Loss: 0.2165 || 10iter: 26.0748 sec\n",
      "イテレーション1990 || Loss: 0.2103 || 10iter: 26.1141 sec\n",
      "イテレーション2000 || Loss: 0.2399 || 10iter: 26.0753 sec\n",
      "イテレーション2010 || Loss: 0.8111 || 10iter: 26.1218 sec\n",
      "イテレーション2020 || Loss: 0.3579 || 10iter: 26.1048 sec\n",
      "イテレーション2030 || Loss: 0.4490 || 10iter: 26.0876 sec\n",
      "イテレーション2040 || Loss: 0.3955 || 10iter: 26.0718 sec\n",
      "イテレーション2050 || Loss: 0.2122 || 10iter: 26.1112 sec\n",
      "イテレーション2060 || Loss: 0.2741 || 10iter: 26.1218 sec\n",
      "イテレーション2070 || Loss: 0.4493 || 10iter: 26.1033 sec\n",
      "イテレーション2080 || Loss: 0.3729 || 10iter: 26.0797 sec\n",
      "イテレーション2090 || Loss: 0.3533 || 10iter: 26.0989 sec\n",
      "イテレーション2100 || Loss: 0.2478 || 10iter: 26.1643 sec\n",
      "イテレーション2110 || Loss: 0.2545 || 10iter: 26.1424 sec\n",
      "イテレーション2120 || Loss: 0.3208 || 10iter: 26.2272 sec\n",
      "イテレーション2130 || Loss: 0.1321 || 10iter: 26.1874 sec\n",
      "イテレーション2140 || Loss: 0.3580 || 10iter: 26.1077 sec\n",
      "イテレーション2150 || Loss: 0.3777 || 10iter: 26.1254 sec\n",
      "イテレーション2160 || Loss: 0.5582 || 10iter: 26.1480 sec\n",
      "イテレーション2170 || Loss: 0.3101 || 10iter: 26.0586 sec\n",
      "イテレーション2180 || Loss: 0.5626 || 10iter: 26.0751 sec\n",
      "イテレーション2190 || Loss: 0.3608 || 10iter: 26.0700 sec\n",
      "------------\n",
      "epoch 6 || Epoch_TRAIN_Loss:0.3497 || Epoch_VAL_Loss: 0.3533\n",
      "timer: 1056.3578 sec\n",
      "-------------\n",
      "Epoch 7/30\n",
      "-------------\n",
      "(train)\n",
      "イテレーション2200 || Loss: 0.3063 || 10iter: 8.7655 sec\n",
      "イテレーション2210 || Loss: 0.2806 || 10iter: 25.9851 sec\n",
      "イテレーション2220 || Loss: 0.1983 || 10iter: 25.9901 sec\n",
      "イテレーション2230 || Loss: 0.3067 || 10iter: 25.9445 sec\n",
      "イテレーション2240 || Loss: 0.1452 || 10iter: 25.9951 sec\n",
      "イテレーション2250 || Loss: 0.1521 || 10iter: 26.0208 sec\n",
      "イテレーション2260 || Loss: 0.4100 || 10iter: 26.0433 sec\n",
      "イテレーション2270 || Loss: 0.2267 || 10iter: 26.0013 sec\n",
      "イテレーション2280 || Loss: 0.6000 || 10iter: 25.9838 sec\n",
      "イテレーション2290 || Loss: 0.3103 || 10iter: 25.9929 sec\n",
      "イテレーション2300 || Loss: 0.2628 || 10iter: 26.0120 sec\n",
      "イテレーション2310 || Loss: 0.2980 || 10iter: 26.0503 sec\n",
      "イテレーション2320 || Loss: 0.3300 || 10iter: 26.0268 sec\n",
      "イテレーション2330 || Loss: 0.2548 || 10iter: 26.0669 sec\n",
      "イテレーション2340 || Loss: 0.5460 || 10iter: 26.0373 sec\n",
      "イテレーション2350 || Loss: 0.1606 || 10iter: 26.0156 sec\n",
      "イテレーション2360 || Loss: 0.2298 || 10iter: 26.0085 sec\n",
      "イテレーション2370 || Loss: 0.4228 || 10iter: 26.0299 sec\n",
      "イテレーション2380 || Loss: 0.4316 || 10iter: 26.0340 sec\n",
      "イテレーション2390 || Loss: 0.1944 || 10iter: 26.0379 sec\n",
      "イテレーション2400 || Loss: 0.2871 || 10iter: 25.9741 sec\n",
      "イテレーション2410 || Loss: 0.5634 || 10iter: 26.0834 sec\n",
      "イテレーション2420 || Loss: 0.3583 || 10iter: 26.0663 sec\n",
      "イテレーション2430 || Loss: 0.3616 || 10iter: 26.0114 sec\n",
      "イテレーション2440 || Loss: 0.2218 || 10iter: 26.0262 sec\n",
      "イテレーション2450 || Loss: 0.2282 || 10iter: 26.0825 sec\n",
      "イテレーション2460 || Loss: 0.3461 || 10iter: 26.0782 sec\n",
      "イテレーション2470 || Loss: 0.3112 || 10iter: 26.0833 sec\n",
      "イテレーション2480 || Loss: 0.5767 || 10iter: 26.0718 sec\n",
      "イテレーション2490 || Loss: 0.4042 || 10iter: 26.0733 sec\n",
      "イテレーション2500 || Loss: 0.3203 || 10iter: 26.0730 sec\n",
      "イテレーション2510 || Loss: 0.5188 || 10iter: 26.1126 sec\n",
      "イテレーション2520 || Loss: 0.1686 || 10iter: 26.0316 sec\n",
      "イテレーション2530 || Loss: 0.2057 || 10iter: 26.0162 sec\n",
      "イテレーション2540 || Loss: 0.2393 || 10iter: 26.0742 sec\n",
      "イテレーション2550 || Loss: 0.3895 || 10iter: 26.0737 sec\n",
      "イテレーション2560 || Loss: 0.4387 || 10iter: 26.0410 sec\n",
      "------------\n",
      "epoch 7 || Epoch_TRAIN_Loss:0.3442 || Epoch_VAL_Loss: 0.3477\n",
      "timer: 1052.7238 sec\n",
      "-------------\n",
      "Epoch 8/30\n",
      "-------------\n",
      "(train)\n",
      "イテレーション2570 || Loss: 0.2567 || 10iter: 20.3026 sec\n",
      "イテレーション2580 || Loss: 0.3401 || 10iter: 26.0518 sec\n",
      "イテレーション2590 || Loss: 0.1832 || 10iter: 26.0144 sec\n",
      "イテレーション2600 || Loss: 0.4263 || 10iter: 26.0518 sec\n",
      "イテレーション2610 || Loss: 0.4928 || 10iter: 26.0540 sec\n",
      "イテレーション2620 || Loss: 0.3614 || 10iter: 26.0609 sec\n",
      "イテレーション2630 || Loss: 0.1309 || 10iter: 26.0586 sec\n",
      "イテレーション2640 || Loss: 0.2982 || 10iter: 25.9281 sec\n",
      "イテレーション2650 || Loss: 0.2588 || 10iter: 25.9776 sec\n",
      "イテレーション2660 || Loss: 0.3594 || 10iter: 26.0461 sec\n",
      "イテレーション2670 || Loss: 0.4076 || 10iter: 26.0112 sec\n",
      "イテレーション2680 || Loss: 0.1635 || 10iter: 26.0183 sec\n",
      "イテレーション2690 || Loss: 0.5022 || 10iter: 26.0698 sec\n",
      "イテレーション2700 || Loss: 0.3133 || 10iter: 26.0480 sec\n",
      "イテレーション2710 || Loss: 0.1620 || 10iter: 25.9967 sec\n",
      "イテレーション2720 || Loss: 0.1015 || 10iter: 26.0492 sec\n",
      "イテレーション2730 || Loss: 0.3237 || 10iter: 26.0404 sec\n",
      "イテレーション2740 || Loss: 0.3182 || 10iter: 26.0795 sec\n",
      "イテレーション2750 || Loss: 0.1686 || 10iter: 26.0783 sec\n",
      "イテレーション2760 || Loss: 0.2823 || 10iter: 26.0521 sec\n",
      "イテレーション2770 || Loss: 0.4422 || 10iter: 26.0448 sec\n",
      "イテレーション2780 || Loss: 0.2983 || 10iter: 26.0658 sec\n",
      "イテレーション2790 || Loss: 0.4599 || 10iter: 26.0688 sec\n",
      "イテレーション2800 || Loss: 0.1913 || 10iter: 26.0045 sec\n",
      "イテレーション2810 || Loss: 0.2847 || 10iter: 26.0419 sec\n",
      "イテレーション2820 || Loss: 0.1301 || 10iter: 26.0279 sec\n",
      "イテレーション2830 || Loss: 0.4306 || 10iter: 26.0377 sec\n",
      "イテレーション2840 || Loss: 0.2631 || 10iter: 25.9781 sec\n",
      "イテレーション2850 || Loss: 0.4017 || 10iter: 25.9585 sec\n",
      "イテレーション2860 || Loss: 0.2896 || 10iter: 26.0119 sec\n",
      "イテレーション2870 || Loss: 0.1441 || 10iter: 26.0156 sec\n",
      "イテレーション2880 || Loss: 0.4285 || 10iter: 25.9530 sec\n",
      "イテレーション2890 || Loss: 0.1404 || 10iter: 26.0724 sec\n",
      "イテレーション2900 || Loss: 0.1983 || 10iter: 26.0745 sec\n",
      "イテレーション2910 || Loss: 0.2532 || 10iter: 26.0542 sec\n",
      "イテレーション2920 || Loss: 0.2766 || 10iter: 26.0062 sec\n",
      "------------\n",
      "epoch 8 || Epoch_TRAIN_Loss:0.3412 || Epoch_VAL_Loss: 0.3448\n",
      "timer: 1052.7728 sec\n",
      "-------------\n",
      "Epoch 9/30\n",
      "-------------\n",
      "(train)\n",
      "イテレーション2930 || Loss: 0.2872 || 10iter: 3.0364 sec\n",
      "イテレーション2940 || Loss: 0.2203 || 10iter: 26.0762 sec\n",
      "イテレーション2950 || Loss: 0.1576 || 10iter: 26.1802 sec\n",
      "イテレーション2960 || Loss: 0.4174 || 10iter: 26.1317 sec\n",
      "イテレーション2970 || Loss: 0.5092 || 10iter: 26.1552 sec\n",
      "イテレーション2980 || Loss: 0.3752 || 10iter: 26.1626 sec\n",
      "イテレーション2990 || Loss: 0.4396 || 10iter: 26.1430 sec\n",
      "イテレーション3000 || Loss: 0.3280 || 10iter: 26.1766 sec\n",
      "イテレーション3010 || Loss: 0.3904 || 10iter: 26.1774 sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "イテレーション3020 || Loss: 0.2994 || 10iter: 26.2445 sec\n",
      "イテレーション3030 || Loss: 0.4164 || 10iter: 26.1399 sec\n",
      "イテレーション3040 || Loss: 0.3660 || 10iter: 26.1302 sec\n",
      "イテレーション3050 || Loss: 0.1568 || 10iter: 26.1072 sec\n",
      "イテレーション3060 || Loss: 0.5461 || 10iter: 26.1340 sec\n",
      "イテレーション3070 || Loss: 0.3479 || 10iter: 26.0901 sec\n",
      "イテレーション3080 || Loss: 0.2710 || 10iter: 26.1083 sec\n",
      "イテレーション3090 || Loss: 0.3096 || 10iter: 26.0954 sec\n",
      "イテレーション3100 || Loss: 0.2932 || 10iter: 26.0594 sec\n",
      "イテレーション3110 || Loss: 0.4932 || 10iter: 26.0693 sec\n",
      "イテレーション3120 || Loss: 0.1875 || 10iter: 26.1299 sec\n",
      "イテレーション3130 || Loss: 0.2573 || 10iter: 26.1399 sec\n",
      "イテレーション3140 || Loss: 0.1554 || 10iter: 26.0900 sec\n",
      "イテレーション3150 || Loss: 0.7201 || 10iter: 26.1158 sec\n",
      "イテレーション3160 || Loss: 0.2337 || 10iter: 26.0539 sec\n",
      "イテレーション3170 || Loss: 0.2181 || 10iter: 26.1540 sec\n",
      "イテレーション3180 || Loss: 0.3635 || 10iter: 26.2114 sec\n",
      "イテレーション3190 || Loss: 0.2121 || 10iter: 26.2020 sec\n",
      "イテレーション3200 || Loss: 0.3746 || 10iter: 26.1478 sec\n",
      "イテレーション3210 || Loss: 0.3889 || 10iter: 26.1716 sec\n",
      "イテレーション3220 || Loss: 0.2611 || 10iter: 26.1748 sec\n",
      "イテレーション3230 || Loss: 0.4542 || 10iter: 26.1962 sec\n",
      "イテレーション3240 || Loss: 0.1779 || 10iter: 26.1256 sec\n",
      "イテレーション3250 || Loss: 0.1435 || 10iter: 26.0786 sec\n",
      "イテレーション3260 || Loss: 0.1963 || 10iter: 26.1649 sec\n",
      "イテレーション3270 || Loss: 0.3858 || 10iter: 26.1509 sec\n",
      "イテレーション3280 || Loss: 0.2432 || 10iter: 26.0895 sec\n",
      "イテレーション3290 || Loss: 0.5785 || 10iter: 26.1464 sec\n",
      "------------\n",
      "epoch 9 || Epoch_TRAIN_Loss:0.3337 || Epoch_VAL_Loss: 0.3371\n",
      "timer: 1057.0602 sec\n",
      "-------------\n",
      "Epoch 10/30\n",
      "-------------\n",
      "(train)\n",
      "イテレーション3300 || Loss: 0.1619 || 10iter: 14.5931 sec\n",
      "イテレーション3310 || Loss: 0.4707 || 10iter: 26.1483 sec\n",
      "イテレーション3320 || Loss: 0.3075 || 10iter: 26.1221 sec\n",
      "イテレーション3330 || Loss: 0.2135 || 10iter: 26.0655 sec\n",
      "イテレーション3340 || Loss: 0.5550 || 10iter: 26.1030 sec\n",
      "イテレーション3350 || Loss: 0.3924 || 10iter: 26.1147 sec\n",
      "イテレーション3360 || Loss: 0.4078 || 10iter: 26.1116 sec\n",
      "イテレーション3370 || Loss: 0.1476 || 10iter: 26.1021 sec\n",
      "イテレーション3380 || Loss: 0.5471 || 10iter: 26.0426 sec\n",
      "イテレーション3390 || Loss: 0.3749 || 10iter: 26.1535 sec\n",
      "イテレーション3400 || Loss: 0.1452 || 10iter: 26.1621 sec\n",
      "イテレーション3410 || Loss: 0.2600 || 10iter: 26.1229 sec\n",
      "イテレーション3420 || Loss: 0.4797 || 10iter: 26.1722 sec\n",
      "イテレーション3430 || Loss: 0.3450 || 10iter: 26.0708 sec\n",
      "イテレーション3440 || Loss: 0.2873 || 10iter: 26.0859 sec\n",
      "イテレーション3450 || Loss: 0.1325 || 10iter: 26.0630 sec\n",
      "イテレーション3460 || Loss: 0.1090 || 10iter: 26.0777 sec\n",
      "イテレーション3470 || Loss: 0.2782 || 10iter: 26.0327 sec\n",
      "イテレーション3480 || Loss: 0.2458 || 10iter: 26.1063 sec\n",
      "イテレーション3490 || Loss: 0.1110 || 10iter: 26.1337 sec\n",
      "イテレーション3500 || Loss: 0.3928 || 10iter: 26.0576 sec\n",
      "イテレーション3510 || Loss: 0.4935 || 10iter: 26.1914 sec\n",
      "イテレーション3520 || Loss: 0.2986 || 10iter: 26.1589 sec\n",
      "イテレーション3530 || Loss: 0.3977 || 10iter: 26.0634 sec\n",
      "イテレーション3540 || Loss: 0.4303 || 10iter: 26.0735 sec\n",
      "イテレーション3550 || Loss: 0.2770 || 10iter: 26.1225 sec\n",
      "イテレーション3560 || Loss: 0.3862 || 10iter: 26.0825 sec\n",
      "イテレーション3570 || Loss: 0.1294 || 10iter: 26.1940 sec\n",
      "イテレーション3580 || Loss: 0.3178 || 10iter: 26.0852 sec\n",
      "イテレーション3590 || Loss: 0.3297 || 10iter: 26.1425 sec\n",
      "イテレーション3600 || Loss: 0.3464 || 10iter: 26.0514 sec\n",
      "イテレーション3610 || Loss: 0.3358 || 10iter: 26.0834 sec\n",
      "イテレーション3620 || Loss: 0.2331 || 10iter: 26.1772 sec\n",
      "イテレーション3630 || Loss: 0.2661 || 10iter: 26.1960 sec\n",
      "イテレーション3640 || Loss: 0.2282 || 10iter: 26.1537 sec\n",
      "イテレーション3650 || Loss: 0.3921 || 10iter: 26.1163 sec\n",
      "イテレーション3660 || Loss: 0.1999 || 10iter: 26.1555 sec\n",
      "-------------\n",
      "(val)\n",
      "------------\n",
      "epoch 10 || Epoch_TRAIN_Loss:0.3247 || Epoch_VAL_Loss: 0.7251\n",
      "timer: 1434.5753 sec\n",
      "-------------\n",
      "Epoch 11/30\n",
      "-------------\n",
      "(train)\n",
      "イテレーション3670 || Loss: 0.2595 || 10iter: 26.2256 sec\n",
      "イテレーション3680 || Loss: 0.2549 || 10iter: 26.2115 sec\n",
      "イテレーション3690 || Loss: 0.3291 || 10iter: 26.1718 sec\n",
      "イテレーション3700 || Loss: 0.5862 || 10iter: 26.6211 sec\n",
      "イテレーション3710 || Loss: 0.3018 || 10iter: 26.0729 sec\n",
      "イテレーション3720 || Loss: 0.2799 || 10iter: 26.0671 sec\n",
      "イテレーション3730 || Loss: 0.2861 || 10iter: 27.3434 sec\n",
      "イテレーション3740 || Loss: 0.2672 || 10iter: 28.2403 sec\n",
      "イテレーション3750 || Loss: 0.4286 || 10iter: 28.1526 sec\n",
      "イテレーション3760 || Loss: 0.5681 || 10iter: 28.2813 sec\n",
      "イテレーション3770 || Loss: 0.3103 || 10iter: 26.3168 sec\n",
      "イテレーション3780 || Loss: 0.4999 || 10iter: 26.1627 sec\n",
      "イテレーション3790 || Loss: 0.5388 || 10iter: 26.0860 sec\n",
      "イテレーション3800 || Loss: 0.5021 || 10iter: 26.1189 sec\n",
      "イテレーション3810 || Loss: 0.2002 || 10iter: 26.0371 sec\n",
      "イテレーション3820 || Loss: 0.3268 || 10iter: 26.0750 sec\n",
      "イテレーション3830 || Loss: 0.2959 || 10iter: 26.0555 sec\n",
      "イテレーション3840 || Loss: 0.3099 || 10iter: 26.0791 sec\n",
      "イテレーション3850 || Loss: 0.2452 || 10iter: 26.0679 sec\n",
      "イテレーション3860 || Loss: 0.5609 || 10iter: 26.1273 sec\n",
      "イテレーション3870 || Loss: 0.2473 || 10iter: 26.1280 sec\n",
      "イテレーション3880 || Loss: 0.2127 || 10iter: 26.0820 sec\n",
      "イテレーション3890 || Loss: 0.3436 || 10iter: 26.0821 sec\n",
      "イテレーション3900 || Loss: 0.5001 || 10iter: 26.0851 sec\n",
      "イテレーション3910 || Loss: 0.3951 || 10iter: 26.1161 sec\n",
      "イテレーション3920 || Loss: 0.4453 || 10iter: 26.1704 sec\n",
      "イテレーション3930 || Loss: 0.6187 || 10iter: 26.1988 sec\n",
      "イテレーション3940 || Loss: 0.2986 || 10iter: 26.1620 sec\n",
      "イテレーション3950 || Loss: 0.2709 || 10iter: 26.1788 sec\n",
      "イテレーション3960 || Loss: 0.4029 || 10iter: 26.2404 sec\n",
      "イテレーション3970 || Loss: 0.4884 || 10iter: 26.1133 sec\n",
      "イテレーション3980 || Loss: 0.4858 || 10iter: 26.1002 sec\n",
      "イテレーション3990 || Loss: 0.2495 || 10iter: 26.1632 sec\n",
      "イテレーション4000 || Loss: 0.4022 || 10iter: 26.1765 sec\n",
      "イテレーション4010 || Loss: 0.3108 || 10iter: 26.2404 sec\n",
      "イテレーション4020 || Loss: 0.0955 || 10iter: 26.2106 sec\n",
      "------------\n",
      "epoch 11 || Epoch_TRAIN_Loss:0.3183 || Epoch_VAL_Loss: 0.3216\n",
      "timer: 1065.9967 sec\n",
      "-------------\n",
      "Epoch 12/30\n",
      "-------------\n",
      "(train)\n",
      "イテレーション4030 || Loss: 0.7262 || 10iter: 8.8224 sec\n",
      "イテレーション4040 || Loss: 0.2579 || 10iter: 26.0577 sec\n",
      "イテレーション4050 || Loss: 0.3269 || 10iter: 26.1677 sec\n",
      "イテレーション4060 || Loss: 0.1778 || 10iter: 26.6686 sec\n",
      "イテレーション4070 || Loss: 0.2007 || 10iter: 26.0888 sec\n",
      "イテレーション4080 || Loss: 0.3011 || 10iter: 26.1517 sec\n",
      "イテレーション4090 || Loss: 0.1570 || 10iter: 26.3200 sec\n",
      "イテレーション4100 || Loss: 0.2132 || 10iter: 28.1947 sec\n",
      "イテレーション4110 || Loss: 0.3162 || 10iter: 28.2582 sec\n",
      "イテレーション4120 || Loss: 0.2902 || 10iter: 36.1657 sec\n",
      "イテレーション4130 || Loss: 0.4099 || 10iter: 34.3579 sec\n",
      "イテレーション4140 || Loss: 0.2799 || 10iter: 28.7609 sec\n",
      "イテレーション4150 || Loss: 0.2791 || 10iter: 27.0077 sec\n",
      "イテレーション4160 || Loss: 0.2079 || 10iter: 26.0873 sec\n",
      "イテレーション4170 || Loss: 0.1763 || 10iter: 26.0864 sec\n",
      "イテレーション4180 || Loss: 0.3043 || 10iter: 26.1053 sec\n",
      "イテレーション4190 || Loss: 0.3505 || 10iter: 26.0659 sec\n",
      "イテレーション4200 || Loss: 0.4106 || 10iter: 26.0274 sec\n",
      "イテレーション4210 || Loss: 0.2315 || 10iter: 26.0183 sec\n",
      "イテレーション4220 || Loss: 0.2272 || 10iter: 26.0433 sec\n",
      "イテレーション4230 || Loss: 0.1849 || 10iter: 26.0253 sec\n",
      "イテレーション4240 || Loss: 0.4054 || 10iter: 26.0784 sec\n",
      "イテレーション4250 || Loss: 0.2531 || 10iter: 26.1080 sec\n",
      "イテレーション4260 || Loss: 0.5347 || 10iter: 26.0921 sec\n",
      "イテレーション4270 || Loss: 0.4281 || 10iter: 26.1394 sec\n",
      "イテレーション4280 || Loss: 0.3485 || 10iter: 26.0587 sec\n",
      "イテレーション4290 || Loss: 0.3332 || 10iter: 26.0844 sec\n",
      "イテレーション4300 || Loss: 0.2322 || 10iter: 26.5362 sec\n",
      "イテレーション4310 || Loss: 0.5216 || 10iter: 26.0492 sec\n",
      "イテレーション4320 || Loss: 0.3144 || 10iter: 26.0595 sec\n",
      "イテレーション4330 || Loss: 0.1727 || 10iter: 26.0651 sec\n",
      "イテレーション4340 || Loss: 0.2448 || 10iter: 28.2720 sec\n",
      "イテレーション4350 || Loss: 0.3245 || 10iter: 28.2360 sec\n",
      "イテレーション4360 || Loss: 0.2720 || 10iter: 28.7718 sec\n",
      "イテレーション4370 || Loss: 0.1840 || 10iter: 28.4326 sec\n",
      "イテレーション4380 || Loss: 0.3184 || 10iter: 26.1327 sec\n",
      "イテレーション4390 || Loss: 0.1193 || 10iter: 26.1136 sec\n",
      "------------\n",
      "epoch 12 || Epoch_TRAIN_Loss:0.3274 || Epoch_VAL_Loss: 0.3308\n",
      "timer: 1094.9592 sec\n",
      "-------------\n",
      "Epoch 13/30\n",
      "-------------\n",
      "(train)\n",
      "イテレーション4400 || Loss: 0.3641 || 10iter: 20.4267 sec\n",
      "イテレーション4410 || Loss: 0.3579 || 10iter: 26.2407 sec\n",
      "イテレーション4420 || Loss: 0.3737 || 10iter: 26.2562 sec\n",
      "イテレーション4430 || Loss: 0.4765 || 10iter: 26.1802 sec\n",
      "イテレーション4440 || Loss: 0.3745 || 10iter: 26.1893 sec\n",
      "イテレーション4450 || Loss: 0.1354 || 10iter: 26.2571 sec\n",
      "イテレーション4460 || Loss: 0.2836 || 10iter: 26.1875 sec\n",
      "イテレーション4470 || Loss: 0.5335 || 10iter: 26.1296 sec\n",
      "イテレーション4480 || Loss: 0.2339 || 10iter: 26.1092 sec\n",
      "イテレーション4490 || Loss: 0.2629 || 10iter: 26.0935 sec\n",
      "イテレーション4500 || Loss: 0.2182 || 10iter: 26.0718 sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "イテレーション4510 || Loss: 0.3168 || 10iter: 26.1592 sec\n",
      "イテレーション4520 || Loss: 0.4088 || 10iter: 26.1714 sec\n",
      "イテレーション4530 || Loss: 0.3581 || 10iter: 26.1854 sec\n",
      "イテレーション4540 || Loss: 0.2786 || 10iter: 26.2058 sec\n",
      "イテレーション4550 || Loss: 0.1851 || 10iter: 26.2334 sec\n",
      "イテレーション4560 || Loss: 0.4967 || 10iter: 26.1234 sec\n",
      "イテレーション4570 || Loss: 0.2589 || 10iter: 26.2148 sec\n",
      "イテレーション4580 || Loss: 0.2863 || 10iter: 26.2266 sec\n",
      "イテレーション4590 || Loss: 0.2343 || 10iter: 26.1086 sec\n",
      "イテレーション4600 || Loss: 0.2887 || 10iter: 26.1055 sec\n",
      "イテレーション4610 || Loss: 0.2711 || 10iter: 26.2016 sec\n",
      "イテレーション4620 || Loss: 0.4723 || 10iter: 26.1551 sec\n",
      "イテレーション4630 || Loss: 0.5936 || 10iter: 26.2270 sec\n",
      "イテレーション4640 || Loss: 0.2254 || 10iter: 26.2866 sec\n",
      "イテレーション4650 || Loss: 0.3610 || 10iter: 26.2277 sec\n",
      "イテレーション4660 || Loss: 0.3458 || 10iter: 26.2209 sec\n",
      "イテレーション4670 || Loss: 0.3119 || 10iter: 26.0648 sec\n",
      "イテレーション4680 || Loss: 0.3804 || 10iter: 26.0926 sec\n",
      "イテレーション4690 || Loss: 0.1816 || 10iter: 26.1242 sec\n",
      "イテレーション4700 || Loss: 0.3478 || 10iter: 26.1753 sec\n",
      "イテレーション4710 || Loss: 0.2439 || 10iter: 26.1411 sec\n",
      "イテレーション4720 || Loss: 0.2406 || 10iter: 26.1795 sec\n",
      "イテレーション4730 || Loss: 0.3714 || 10iter: 26.2060 sec\n",
      "イテレーション4740 || Loss: 0.2566 || 10iter: 26.2375 sec\n",
      "イテレーション4750 || Loss: 0.5745 || 10iter: 26.2544 sec\n",
      "------------\n",
      "epoch 13 || Epoch_TRAIN_Loss:0.3173 || Epoch_VAL_Loss: 0.3206\n",
      "timer: 1058.6391 sec\n",
      "-------------\n",
      "Epoch 14/30\n",
      "-------------\n",
      "(train)\n",
      "イテレーション4760 || Loss: 0.2056 || 10iter: 3.1020 sec\n",
      "イテレーション4770 || Loss: 0.4865 || 10iter: 26.2095 sec\n",
      "イテレーション4780 || Loss: 0.1277 || 10iter: 26.1711 sec\n",
      "イテレーション4790 || Loss: 0.5831 || 10iter: 26.2070 sec\n",
      "イテレーション4800 || Loss: 0.4903 || 10iter: 26.1461 sec\n",
      "イテレーション4810 || Loss: 0.5856 || 10iter: 26.1431 sec\n",
      "イテレーション4820 || Loss: 0.2206 || 10iter: 26.1092 sec\n",
      "イテレーション4830 || Loss: 0.2241 || 10iter: 26.2108 sec\n",
      "イテレーション4840 || Loss: 0.1978 || 10iter: 26.1008 sec\n",
      "イテレーション4850 || Loss: 0.1942 || 10iter: 26.1860 sec\n",
      "イテレーション4860 || Loss: 0.4820 || 10iter: 26.1198 sec\n",
      "イテレーション4870 || Loss: 0.2123 || 10iter: 26.1457 sec\n",
      "イテレーション4880 || Loss: 0.5415 || 10iter: 26.1449 sec\n",
      "イテレーション4890 || Loss: 0.3033 || 10iter: 26.2545 sec\n",
      "イテレーション4900 || Loss: 0.2926 || 10iter: 26.2523 sec\n",
      "イテレーション4910 || Loss: 0.2579 || 10iter: 26.2242 sec\n",
      "イテレーション4920 || Loss: 0.2548 || 10iter: 26.2190 sec\n",
      "イテレーション4930 || Loss: 0.5636 || 10iter: 26.2534 sec\n",
      "イテレーション4940 || Loss: 0.2831 || 10iter: 26.1692 sec\n",
      "イテレーション4950 || Loss: 0.2350 || 10iter: 26.1338 sec\n",
      "イテレーション4960 || Loss: 0.1272 || 10iter: 26.1233 sec\n",
      "イテレーション4970 || Loss: 0.4124 || 10iter: 26.1771 sec\n",
      "イテレーション4980 || Loss: 0.2217 || 10iter: 26.2441 sec\n",
      "イテレーション4990 || Loss: 0.3618 || 10iter: 26.2443 sec\n",
      "イテレーション5000 || Loss: 0.4383 || 10iter: 26.2223 sec\n",
      "イテレーション5010 || Loss: 0.3732 || 10iter: 26.1678 sec\n",
      "イテレーション5020 || Loss: 0.3464 || 10iter: 26.1728 sec\n",
      "イテレーション5030 || Loss: 0.5133 || 10iter: 26.1250 sec\n",
      "イテレーション5040 || Loss: 0.5032 || 10iter: 26.1927 sec\n",
      "イテレーション5050 || Loss: 0.2693 || 10iter: 26.1842 sec\n",
      "イテレーション5060 || Loss: 0.5018 || 10iter: 26.1246 sec\n",
      "イテレーション5070 || Loss: 0.3501 || 10iter: 26.1717 sec\n",
      "イテレーション5080 || Loss: 0.4848 || 10iter: 26.1585 sec\n",
      "イテレーション5090 || Loss: 0.3115 || 10iter: 26.1904 sec\n",
      "イテレーション5100 || Loss: 0.5147 || 10iter: 26.1565 sec\n",
      "イテレーション5110 || Loss: 0.3488 || 10iter: 26.1909 sec\n",
      "イテレーション5120 || Loss: 0.1684 || 10iter: 26.1463 sec\n",
      "------------\n",
      "epoch 14 || Epoch_TRAIN_Loss:0.3206 || Epoch_VAL_Loss: 0.3239\n",
      "timer: 1058.6691 sec\n",
      "-------------\n",
      "Epoch 15/30\n",
      "-------------\n",
      "(train)\n",
      "イテレーション5130 || Loss: 0.3095 || 10iter: 14.5888 sec\n",
      "イテレーション5140 || Loss: 0.4377 || 10iter: 26.1430 sec\n",
      "イテレーション5150 || Loss: 0.2507 || 10iter: 26.1347 sec\n",
      "イテレーション5160 || Loss: 0.3518 || 10iter: 26.1018 sec\n",
      "イテレーション5170 || Loss: 0.4000 || 10iter: 26.1559 sec\n",
      "イテレーション5180 || Loss: 0.3054 || 10iter: 26.0738 sec\n",
      "イテレーション5190 || Loss: 0.1894 || 10iter: 26.1694 sec\n",
      "イテレーション5200 || Loss: 0.3309 || 10iter: 26.1419 sec\n",
      "イテレーション5210 || Loss: 0.3447 || 10iter: 26.0460 sec\n",
      "イテレーション5220 || Loss: 0.3425 || 10iter: 26.0730 sec\n",
      "イテレーション5230 || Loss: 0.3262 || 10iter: 26.0725 sec\n",
      "イテレーション5240 || Loss: 0.3691 || 10iter: 26.0447 sec\n",
      "イテレーション5250 || Loss: 0.6251 || 10iter: 26.0746 sec\n",
      "イテレーション5260 || Loss: 0.2173 || 10iter: 26.1100 sec\n",
      "イテレーション5270 || Loss: 0.1546 || 10iter: 26.0242 sec\n",
      "イテレーション5280 || Loss: 0.2011 || 10iter: 26.0593 sec\n",
      "イテレーション5290 || Loss: 0.3869 || 10iter: 26.1710 sec\n",
      "イテレーション5300 || Loss: 0.2565 || 10iter: 26.1343 sec\n",
      "イテレーション5310 || Loss: 0.2402 || 10iter: 26.0611 sec\n",
      "イテレーション5320 || Loss: 0.1623 || 10iter: 26.0951 sec\n",
      "イテレーション5330 || Loss: 0.4420 || 10iter: 26.1416 sec\n",
      "イテレーション5340 || Loss: 0.1622 || 10iter: 26.1055 sec\n",
      "イテレーション5350 || Loss: 0.1945 || 10iter: 26.2370 sec\n",
      "イテレーション5360 || Loss: 0.2197 || 10iter: 26.1734 sec\n",
      "イテレーション5370 || Loss: 0.2173 || 10iter: 26.1647 sec\n",
      "イテレーション5380 || Loss: 0.5027 || 10iter: 26.1654 sec\n",
      "イテレーション5390 || Loss: 0.5863 || 10iter: 26.1469 sec\n",
      "イテレーション5400 || Loss: 0.5398 || 10iter: 26.2065 sec\n",
      "イテレーション5410 || Loss: 0.3156 || 10iter: 26.1932 sec\n",
      "イテレーション5420 || Loss: 0.2090 || 10iter: 26.1453 sec\n",
      "イテレーション5430 || Loss: 0.1769 || 10iter: 26.0586 sec\n",
      "イテレーション5440 || Loss: 0.3329 || 10iter: 26.0378 sec\n",
      "イテレーション5450 || Loss: 0.5343 || 10iter: 26.0861 sec\n",
      "イテレーション5460 || Loss: 0.2115 || 10iter: 26.0343 sec\n",
      "イテレーション5470 || Loss: 0.3582 || 10iter: 26.0860 sec\n",
      "イテレーション5480 || Loss: 0.3067 || 10iter: 26.1599 sec\n",
      "イテレーション5490 || Loss: 0.1884 || 10iter: 26.1315 sec\n",
      "-------------\n",
      "(val)\n",
      "------------\n",
      "epoch 15 || Epoch_TRAIN_Loss:0.3123 || Epoch_VAL_Loss: 0.6881\n",
      "timer: 1430.5745 sec\n",
      "-------------\n",
      "Epoch 16/30\n",
      "-------------\n",
      "(train)\n",
      "イテレーション5500 || Loss: 0.2691 || 10iter: 26.0753 sec\n",
      "イテレーション5510 || Loss: 0.1751 || 10iter: 26.0761 sec\n",
      "イテレーション5520 || Loss: 0.6586 || 10iter: 26.1511 sec\n",
      "イテレーション5530 || Loss: 0.2240 || 10iter: 26.1151 sec\n",
      "イテレーション5540 || Loss: 0.3183 || 10iter: 26.1346 sec\n",
      "イテレーション5550 || Loss: 0.3883 || 10iter: 26.1036 sec\n",
      "イテレーション5560 || Loss: 0.2169 || 10iter: 26.0763 sec\n",
      "イテレーション5570 || Loss: 0.4533 || 10iter: 26.0703 sec\n",
      "イテレーション5580 || Loss: 0.3838 || 10iter: 26.0762 sec\n",
      "イテレーション5590 || Loss: 0.6734 || 10iter: 26.0628 sec\n",
      "イテレーション5600 || Loss: 0.3573 || 10iter: 26.1103 sec\n",
      "イテレーション5610 || Loss: 0.3900 || 10iter: 26.0789 sec\n",
      "イテレーション5620 || Loss: 0.4160 || 10iter: 26.1307 sec\n",
      "イテレーション5630 || Loss: 0.4746 || 10iter: 26.0816 sec\n",
      "イテレーション5640 || Loss: 0.4062 || 10iter: 26.1336 sec\n",
      "イテレーション5650 || Loss: 0.3715 || 10iter: 26.1678 sec\n",
      "イテレーション5660 || Loss: 0.1196 || 10iter: 26.1590 sec\n",
      "イテレーション5670 || Loss: 0.2237 || 10iter: 26.1500 sec\n",
      "イテレーション5680 || Loss: 0.3419 || 10iter: 26.0386 sec\n",
      "イテレーション5690 || Loss: 0.2360 || 10iter: 26.0524 sec\n",
      "イテレーション5700 || Loss: 0.2217 || 10iter: 26.0680 sec\n",
      "イテレーション5710 || Loss: 0.2033 || 10iter: 26.0468 sec\n",
      "イテレーション5720 || Loss: 0.2360 || 10iter: 26.1446 sec\n",
      "イテレーション5730 || Loss: 0.2831 || 10iter: 26.0902 sec\n",
      "イテレーション5740 || Loss: 0.4772 || 10iter: 26.1639 sec\n",
      "イテレーション5750 || Loss: 0.4139 || 10iter: 26.1748 sec\n",
      "イテレーション5760 || Loss: 0.2062 || 10iter: 26.1242 sec\n",
      "イテレーション5770 || Loss: 0.3140 || 10iter: 26.0917 sec\n",
      "イテレーション5780 || Loss: 0.5007 || 10iter: 26.2075 sec\n",
      "イテレーション5790 || Loss: 0.3279 || 10iter: 26.0744 sec\n",
      "イテレーション5800 || Loss: 0.2571 || 10iter: 26.0940 sec\n",
      "イテレーション5810 || Loss: 0.1850 || 10iter: 26.0652 sec\n",
      "イテレーション5820 || Loss: 0.1545 || 10iter: 26.0950 sec\n",
      "イテレーション5830 || Loss: 0.4626 || 10iter: 26.1152 sec\n",
      "イテレーション5840 || Loss: 0.3267 || 10iter: 26.1048 sec\n",
      "イテレーション5850 || Loss: 0.2185 || 10iter: 26.1538 sec\n",
      "------------\n",
      "epoch 16 || Epoch_TRAIN_Loss:0.3137 || Epoch_VAL_Loss: 0.3169\n",
      "timer: 1055.6863 sec\n",
      "-------------\n",
      "Epoch 17/30\n",
      "-------------\n",
      "(train)\n",
      "イテレーション5860 || Loss: 0.3385 || 10iter: 8.8555 sec\n",
      "イテレーション5870 || Loss: 0.2405 || 10iter: 26.1907 sec\n",
      "イテレーション5880 || Loss: 0.5691 || 10iter: 26.2655 sec\n",
      "イテレーション5890 || Loss: 0.4759 || 10iter: 26.3668 sec\n",
      "イテレーション5900 || Loss: 0.1948 || 10iter: 26.2793 sec\n",
      "イテレーション5910 || Loss: 0.2415 || 10iter: 26.2717 sec\n",
      "イテレーション5920 || Loss: 0.6842 || 10iter: 26.2477 sec\n",
      "イテレーション5930 || Loss: 0.3814 || 10iter: 26.1572 sec\n",
      "イテレーション5940 || Loss: 0.2877 || 10iter: 26.1975 sec\n",
      "イテレーション5950 || Loss: 0.3579 || 10iter: 26.1923 sec\n",
      "イテレーション5960 || Loss: 0.1652 || 10iter: 26.1574 sec\n",
      "イテレーション5970 || Loss: 0.2889 || 10iter: 26.2696 sec\n",
      "イテレーション5980 || Loss: 0.2400 || 10iter: 26.1885 sec\n",
      "イテレーション5990 || Loss: 0.2899 || 10iter: 26.2557 sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "イテレーション6000 || Loss: 0.5357 || 10iter: 26.2122 sec\n",
      "イテレーション6010 || Loss: 0.4339 || 10iter: 26.2124 sec\n",
      "イテレーション6020 || Loss: 0.5601 || 10iter: 26.2641 sec\n",
      "イテレーション6030 || Loss: 0.3964 || 10iter: 26.3767 sec\n",
      "イテレーション6040 || Loss: 0.1470 || 10iter: 26.3005 sec\n",
      "イテレーション6050 || Loss: 0.2678 || 10iter: 26.3473 sec\n",
      "イテレーション6060 || Loss: 0.4901 || 10iter: 26.2560 sec\n",
      "イテレーション6070 || Loss: 0.2146 || 10iter: 26.2719 sec\n",
      "イテレーション6080 || Loss: 0.2398 || 10iter: 26.2415 sec\n",
      "イテレーション6090 || Loss: 0.1137 || 10iter: 26.2103 sec\n",
      "イテレーション6100 || Loss: 0.2454 || 10iter: 26.2710 sec\n",
      "イテレーション6110 || Loss: 0.6101 || 10iter: 26.1941 sec\n",
      "イテレーション6120 || Loss: 0.2380 || 10iter: 26.1912 sec\n",
      "イテレーション6130 || Loss: 0.2492 || 10iter: 26.2572 sec\n",
      "イテレーション6140 || Loss: 0.2386 || 10iter: 26.2363 sec\n",
      "イテレーション6150 || Loss: 0.2901 || 10iter: 26.2240 sec\n",
      "イテレーション6160 || Loss: 0.2001 || 10iter: 26.2795 sec\n",
      "イテレーション6170 || Loss: 0.1823 || 10iter: 26.2404 sec\n",
      "イテレーション6180 || Loss: 0.3208 || 10iter: 26.2381 sec\n",
      "イテレーション6190 || Loss: 0.2490 || 10iter: 26.2864 sec\n",
      "イテレーション6200 || Loss: 0.3330 || 10iter: 26.2362 sec\n",
      "イテレーション6210 || Loss: 0.1000 || 10iter: 26.2493 sec\n",
      "イテレーション6220 || Loss: 0.1252 || 10iter: 26.2961 sec\n",
      "------------\n",
      "epoch 17 || Epoch_TRAIN_Loss:0.3117 || Epoch_VAL_Loss: 0.3149\n",
      "timer: 1061.4833 sec\n",
      "-------------\n",
      "Epoch 18/30\n",
      "-------------\n",
      "(train)\n",
      "イテレーション6230 || Loss: 0.5820 || 10iter: 20.4442 sec\n",
      "イテレーション6240 || Loss: 0.2562 || 10iter: 26.2388 sec\n",
      "イテレーション6250 || Loss: 0.3147 || 10iter: 26.2762 sec\n",
      "イテレーション6260 || Loss: 0.3027 || 10iter: 26.2080 sec\n",
      "イテレーション6270 || Loss: 0.3294 || 10iter: 26.2590 sec\n",
      "イテレーション6280 || Loss: 0.1956 || 10iter: 26.2814 sec\n",
      "イテレーション6290 || Loss: 0.2597 || 10iter: 26.2504 sec\n",
      "イテレーション6300 || Loss: 0.3526 || 10iter: 26.2621 sec\n",
      "イテレーション6310 || Loss: 0.3948 || 10iter: 26.2225 sec\n",
      "イテレーション6320 || Loss: 0.2404 || 10iter: 26.2523 sec\n",
      "イテレーション6330 || Loss: 0.3000 || 10iter: 26.3693 sec\n",
      "イテレーション6340 || Loss: 0.2222 || 10iter: 26.3223 sec\n",
      "イテレーション6350 || Loss: 0.1815 || 10iter: 26.2220 sec\n",
      "イテレーション6360 || Loss: 0.4962 || 10iter: 26.3362 sec\n",
      "イテレーション6370 || Loss: 0.4847 || 10iter: 26.3339 sec\n",
      "イテレーション6380 || Loss: 0.4709 || 10iter: 26.2915 sec\n",
      "イテレーション6390 || Loss: 0.1103 || 10iter: 26.2744 sec\n",
      "イテレーション6400 || Loss: 0.4013 || 10iter: 26.3385 sec\n",
      "イテレーション6410 || Loss: 0.2061 || 10iter: 26.2778 sec\n",
      "イテレーション6420 || Loss: 0.1158 || 10iter: 26.2289 sec\n",
      "イテレーション6430 || Loss: 0.2271 || 10iter: 26.2383 sec\n",
      "イテレーション6440 || Loss: 0.3166 || 10iter: 26.2496 sec\n",
      "イテレーション6450 || Loss: 0.2498 || 10iter: 26.1754 sec\n",
      "イテレーション6460 || Loss: 0.2067 || 10iter: 26.2527 sec\n",
      "イテレーション6470 || Loss: 0.4050 || 10iter: 26.2286 sec\n",
      "イテレーション6480 || Loss: 0.3950 || 10iter: 26.2463 sec\n",
      "イテレーション6490 || Loss: 0.2440 || 10iter: 26.2805 sec\n",
      "イテレーション6500 || Loss: 0.3423 || 10iter: 26.2668 sec\n",
      "イテレーション6510 || Loss: 0.3412 || 10iter: 26.2511 sec\n",
      "イテレーション6520 || Loss: 0.2639 || 10iter: 26.2607 sec\n",
      "イテレーション6530 || Loss: 0.3051 || 10iter: 26.2826 sec\n",
      "イテレーション6540 || Loss: 0.2500 || 10iter: 26.2161 sec\n",
      "イテレーション6550 || Loss: 0.1823 || 10iter: 26.2043 sec\n",
      "イテレーション6560 || Loss: 0.2345 || 10iter: 26.2460 sec\n",
      "イテレーション6570 || Loss: 0.2621 || 10iter: 26.1875 sec\n",
      "イテレーション6580 || Loss: 0.1902 || 10iter: 26.2352 sec\n",
      "------------\n",
      "epoch 18 || Epoch_TRAIN_Loss:0.3138 || Epoch_VAL_Loss: 0.3171\n",
      "timer: 1061.9081 sec\n",
      "-------------\n",
      "Epoch 19/30\n",
      "-------------\n",
      "(train)\n",
      "イテレーション6590 || Loss: 0.3785 || 10iter: 3.0951 sec\n",
      "イテレーション6600 || Loss: 0.4573 || 10iter: 26.1019 sec\n",
      "イテレーション6610 || Loss: 0.3325 || 10iter: 26.1525 sec\n",
      "イテレーション6620 || Loss: 0.2687 || 10iter: 26.1382 sec\n",
      "イテレーション6630 || Loss: 0.5385 || 10iter: 26.1452 sec\n",
      "イテレーション6640 || Loss: 0.5287 || 10iter: 26.1617 sec\n",
      "イテレーション6650 || Loss: 0.1877 || 10iter: 26.2239 sec\n",
      "イテレーション6660 || Loss: 0.2863 || 10iter: 26.1647 sec\n",
      "イテレーション6670 || Loss: 0.2545 || 10iter: 26.1548 sec\n",
      "イテレーション6680 || Loss: 0.2020 || 10iter: 26.1305 sec\n",
      "イテレーション6690 || Loss: 0.2134 || 10iter: 26.1904 sec\n",
      "イテレーション6700 || Loss: 0.1879 || 10iter: 26.1465 sec\n",
      "イテレーション6710 || Loss: 0.2818 || 10iter: 26.2319 sec\n",
      "イテレーション6720 || Loss: 0.2301 || 10iter: 26.1670 sec\n",
      "イテレーション6730 || Loss: 0.2152 || 10iter: 26.1752 sec\n",
      "イテレーション6740 || Loss: 0.2963 || 10iter: 26.1765 sec\n",
      "イテレーション6750 || Loss: 0.2102 || 10iter: 26.1673 sec\n",
      "イテレーション6760 || Loss: 0.1587 || 10iter: 26.2190 sec\n",
      "イテレーション6770 || Loss: 0.3152 || 10iter: 26.1836 sec\n",
      "イテレーション6780 || Loss: 0.2837 || 10iter: 26.2116 sec\n",
      "イテレーション6790 || Loss: 0.3757 || 10iter: 26.1692 sec\n",
      "イテレーション6800 || Loss: 0.3303 || 10iter: 26.1896 sec\n",
      "イテレーション6810 || Loss: 0.1546 || 10iter: 26.1770 sec\n",
      "イテレーション6820 || Loss: 0.3172 || 10iter: 26.1698 sec\n",
      "イテレーション6830 || Loss: 0.3345 || 10iter: 26.2032 sec\n",
      "イテレーション6840 || Loss: 0.2552 || 10iter: 26.2234 sec\n",
      "イテレーション6850 || Loss: 0.6187 || 10iter: 26.2080 sec\n",
      "イテレーション6860 || Loss: 0.1647 || 10iter: 26.2272 sec\n",
      "イテレーション6870 || Loss: 0.2577 || 10iter: 26.2463 sec\n",
      "イテレーション6880 || Loss: 0.1589 || 10iter: 26.2892 sec\n",
      "イテレーション6890 || Loss: 0.2884 || 10iter: 26.2356 sec\n",
      "イテレーション6900 || Loss: 0.2475 || 10iter: 26.3188 sec\n",
      "イテレーション6910 || Loss: 0.3845 || 10iter: 26.2378 sec\n",
      "イテレーション6920 || Loss: 0.2411 || 10iter: 26.3038 sec\n",
      "イテレーション6930 || Loss: 0.1700 || 10iter: 26.1721 sec\n",
      "イテレーション6940 || Loss: 0.3099 || 10iter: 26.2230 sec\n",
      "イテレーション6950 || Loss: 0.2348 || 10iter: 26.2498 sec\n",
      "------------\n",
      "epoch 19 || Epoch_TRAIN_Loss:0.3036 || Epoch_VAL_Loss: 0.3068\n",
      "timer: 1059.4153 sec\n",
      "-------------\n",
      "Epoch 20/30\n",
      "-------------\n",
      "(train)\n",
      "イテレーション6960 || Loss: 0.1160 || 10iter: 14.5688 sec\n",
      "イテレーション6970 || Loss: 0.4290 || 10iter: 26.2413 sec\n",
      "イテレーション6980 || Loss: 0.3132 || 10iter: 26.1885 sec\n",
      "イテレーション6990 || Loss: 0.2831 || 10iter: 26.2184 sec\n",
      "イテレーション7000 || Loss: 0.2918 || 10iter: 26.1427 sec\n",
      "イテレーション7010 || Loss: 0.3036 || 10iter: 26.1650 sec\n",
      "イテレーション7020 || Loss: 0.1816 || 10iter: 26.2153 sec\n",
      "イテレーション7030 || Loss: 0.3334 || 10iter: 26.1744 sec\n",
      "イテレーション7040 || Loss: 0.1783 || 10iter: 26.1999 sec\n",
      "イテレーション7050 || Loss: 0.2600 || 10iter: 26.1733 sec\n",
      "イテレーション7060 || Loss: 0.1843 || 10iter: 26.1522 sec\n",
      "イテレーション7070 || Loss: 0.3895 || 10iter: 26.1092 sec\n",
      "イテレーション7080 || Loss: 0.5115 || 10iter: 26.1173 sec\n",
      "イテレーション7090 || Loss: 0.1942 || 10iter: 26.1091 sec\n",
      "イテレーション7100 || Loss: 0.4403 || 10iter: 26.1530 sec\n",
      "イテレーション7110 || Loss: 0.2479 || 10iter: 26.1261 sec\n",
      "イテレーション7120 || Loss: 0.2464 || 10iter: 26.1730 sec\n",
      "イテレーション7130 || Loss: 0.2378 || 10iter: 26.1575 sec\n",
      "イテレーション7140 || Loss: 0.1780 || 10iter: 26.1571 sec\n",
      "イテレーション7150 || Loss: 0.4085 || 10iter: 26.1457 sec\n",
      "イテレーション7160 || Loss: 0.1661 || 10iter: 26.1216 sec\n",
      "イテレーション7170 || Loss: 0.1345 || 10iter: 26.1198 sec\n",
      "イテレーション7180 || Loss: 0.1376 || 10iter: 26.2522 sec\n",
      "イテレーション7190 || Loss: 0.2537 || 10iter: 26.2094 sec\n",
      "イテレーション7200 || Loss: 0.2589 || 10iter: 26.1660 sec\n",
      "イテレーション7210 || Loss: 0.6295 || 10iter: 26.2341 sec\n",
      "イテレーション7220 || Loss: 0.5040 || 10iter: 26.1782 sec\n",
      "イテレーション7230 || Loss: 0.5551 || 10iter: 26.2498 sec\n",
      "イテレーション7240 || Loss: 0.2146 || 10iter: 26.1696 sec\n",
      "イテレーション7250 || Loss: 0.4102 || 10iter: 26.1394 sec\n",
      "イテレーション7260 || Loss: 0.3752 || 10iter: 26.1299 sec\n",
      "イテレーション7270 || Loss: 0.3884 || 10iter: 26.1873 sec\n",
      "イテレーション7280 || Loss: 0.2566 || 10iter: 26.2259 sec\n",
      "イテレーション7290 || Loss: 0.2659 || 10iter: 26.1476 sec\n",
      "イテレーション7300 || Loss: 0.5125 || 10iter: 26.2059 sec\n",
      "イテレーション7310 || Loss: 0.4223 || 10iter: 26.1268 sec\n",
      "イテレーション7320 || Loss: 0.2284 || 10iter: 26.1426 sec\n",
      "-------------\n",
      "(val)\n",
      "------------\n",
      "epoch 20 || Epoch_TRAIN_Loss:0.2972 || Epoch_VAL_Loss: 0.6614\n",
      "timer: 1427.0876 sec\n",
      "-------------\n",
      "Epoch 21/30\n",
      "-------------\n",
      "(train)\n",
      "イテレーション7330 || Loss: 0.2341 || 10iter: 26.3827 sec\n",
      "イテレーション7340 || Loss: 0.2964 || 10iter: 26.3897 sec\n",
      "イテレーション7350 || Loss: 0.2399 || 10iter: 27.5735 sec\n",
      "イテレーション7360 || Loss: 0.2529 || 10iter: 28.9057 sec\n",
      "イテレーション7370 || Loss: 0.2341 || 10iter: 29.0748 sec\n",
      "イテレーション7380 || Loss: 0.4054 || 10iter: 29.4772 sec\n",
      "イテレーション7390 || Loss: 0.2339 || 10iter: 26.9758 sec\n",
      "イテレーション7400 || Loss: 0.3516 || 10iter: 26.4149 sec\n",
      "イテレーション7410 || Loss: 0.6734 || 10iter: 26.2903 sec\n",
      "イテレーション7420 || Loss: 0.2890 || 10iter: 26.5144 sec\n",
      "イテレーション7430 || Loss: 0.3048 || 10iter: 26.3551 sec\n",
      "イテレーション7440 || Loss: 0.2448 || 10iter: 26.2584 sec\n",
      "イテレーション7450 || Loss: 0.3126 || 10iter: 26.2771 sec\n",
      "イテレーション7460 || Loss: 0.3313 || 10iter: 26.3719 sec\n",
      "イテレーション7470 || Loss: 0.2130 || 10iter: 26.2812 sec\n",
      "イテレーション7480 || Loss: 0.2575 || 10iter: 26.3093 sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "イテレーション7490 || Loss: 0.3176 || 10iter: 26.2666 sec\n",
      "イテレーション7500 || Loss: 0.1408 || 10iter: 26.2849 sec\n",
      "イテレーション7510 || Loss: 0.1922 || 10iter: 26.3461 sec\n",
      "イテレーション7520 || Loss: 0.3006 || 10iter: 26.2498 sec\n",
      "イテレーション7530 || Loss: 0.1393 || 10iter: 26.2987 sec\n",
      "イテレーション7540 || Loss: 0.3310 || 10iter: 26.3661 sec\n",
      "イテレーション7550 || Loss: 0.0943 || 10iter: 26.2889 sec\n",
      "イテレーション7560 || Loss: 0.4140 || 10iter: 26.3381 sec\n",
      "イテレーション7570 || Loss: 0.2076 || 10iter: 26.3610 sec\n",
      "イテレーション7580 || Loss: 0.1983 || 10iter: 26.3454 sec\n",
      "イテレーション7590 || Loss: 0.1204 || 10iter: 26.3734 sec\n",
      "イテレーション7600 || Loss: 0.3913 || 10iter: 26.3750 sec\n",
      "イテレーション7610 || Loss: 0.4252 || 10iter: 26.3759 sec\n",
      "イテレーション7620 || Loss: 0.3901 || 10iter: 26.3351 sec\n",
      "イテレーション7630 || Loss: 0.3187 || 10iter: 26.3760 sec\n",
      "イテレーション7640 || Loss: 0.3378 || 10iter: 26.4442 sec\n",
      "イテレーション7650 || Loss: 0.3254 || 10iter: 26.4197 sec\n",
      "イテレーション7660 || Loss: 0.1207 || 10iter: 26.4084 sec\n",
      "イテレーション7670 || Loss: 0.1762 || 10iter: 26.3332 sec\n",
      "イテレーション7680 || Loss: 0.2615 || 10iter: 26.3651 sec\n",
      "------------\n",
      "epoch 21 || Epoch_TRAIN_Loss:0.3040 || Epoch_VAL_Loss: 0.3072\n",
      "timer: 1077.1146 sec\n",
      "-------------\n",
      "Epoch 22/30\n",
      "-------------\n",
      "(train)\n",
      "イテレーション7690 || Loss: 0.3782 || 10iter: 8.9561 sec\n",
      "イテレーション7700 || Loss: 0.3422 || 10iter: 26.3402 sec\n",
      "イテレーション7710 || Loss: 0.3696 || 10iter: 27.3352 sec\n",
      "イテレーション7720 || Loss: 0.2408 || 10iter: 26.2864 sec\n",
      "イテレーション7730 || Loss: 0.2078 || 10iter: 26.3379 sec\n",
      "イテレーション7740 || Loss: 0.2775 || 10iter: 26.3267 sec\n",
      "イテレーション7750 || Loss: 0.4489 || 10iter: 29.2709 sec\n",
      "イテレーション7760 || Loss: 0.3136 || 10iter: 29.4216 sec\n",
      "イテレーション7770 || Loss: 0.1574 || 10iter: 29.6254 sec\n",
      "イテレーション7780 || Loss: 0.2532 || 10iter: 28.7948 sec\n",
      "イテレーション7790 || Loss: 0.2882 || 10iter: 26.3762 sec\n",
      "イテレーション7800 || Loss: 0.5397 || 10iter: 26.3851 sec\n",
      "イテレーション7810 || Loss: 0.3412 || 10iter: 27.0451 sec\n",
      "イテレーション7820 || Loss: 0.4394 || 10iter: 26.3225 sec\n",
      "イテレーション7830 || Loss: 0.5767 || 10iter: 26.2229 sec\n",
      "イテレーション7840 || Loss: 0.2764 || 10iter: 26.9502 sec\n",
      "イテレーション7850 || Loss: 0.1707 || 10iter: 29.0782 sec\n",
      "イテレーション7860 || Loss: 0.1895 || 10iter: 29.0211 sec\n",
      "イテレーション7870 || Loss: 0.2672 || 10iter: 29.0605 sec\n",
      "イテレーション7880 || Loss: 0.3802 || 10iter: 27.6431 sec\n",
      "イテレーション7890 || Loss: 0.3931 || 10iter: 26.3670 sec\n",
      "イテレーション7900 || Loss: 0.2334 || 10iter: 26.3717 sec\n",
      "イテレーション7910 || Loss: 0.2071 || 10iter: 26.2568 sec\n",
      "イテレーション7920 || Loss: 0.3665 || 10iter: 26.3709 sec\n",
      "イテレーション7930 || Loss: 0.1782 || 10iter: 26.3143 sec\n",
      "イテレーション7940 || Loss: 0.2023 || 10iter: 26.2898 sec\n",
      "イテレーション7950 || Loss: 0.2768 || 10iter: 26.2807 sec\n",
      "イテレーション7960 || Loss: 0.3625 || 10iter: 26.3172 sec\n",
      "イテレーション7970 || Loss: 0.2368 || 10iter: 26.3960 sec\n",
      "イテレーション7980 || Loss: 0.2643 || 10iter: 26.3905 sec\n",
      "イテレーション7990 || Loss: 0.2150 || 10iter: 26.3753 sec\n",
      "イテレーション8000 || Loss: 0.1301 || 10iter: 26.3594 sec\n",
      "イテレーション8010 || Loss: 0.2203 || 10iter: 26.3545 sec\n",
      "イテレーション8020 || Loss: 0.2354 || 10iter: 26.3913 sec\n",
      "イテレーション8030 || Loss: 0.1450 || 10iter: 26.3420 sec\n",
      "イテレーション8040 || Loss: 0.5642 || 10iter: 26.3193 sec\n",
      "イテレーション8050 || Loss: 0.1730 || 10iter: 26.2597 sec\n",
      "------------\n",
      "epoch 22 || Epoch_TRAIN_Loss:0.3022 || Epoch_VAL_Loss: 0.3053\n",
      "timer: 1091.0586 sec\n",
      "-------------\n",
      "Epoch 23/30\n",
      "-------------\n",
      "(train)\n",
      "イテレーション8060 || Loss: 0.1905 || 10iter: 20.4386 sec\n",
      "イテレーション8070 || Loss: 0.1673 || 10iter: 26.1479 sec\n",
      "イテレーション8080 || Loss: 0.3085 || 10iter: 26.1648 sec\n",
      "イテレーション8090 || Loss: 0.1989 || 10iter: 26.1317 sec\n",
      "イテレーション8100 || Loss: 0.2638 || 10iter: 26.2023 sec\n",
      "イテレーション8110 || Loss: 0.1919 || 10iter: 26.2585 sec\n",
      "イテレーション8120 || Loss: 0.4231 || 10iter: 26.2422 sec\n",
      "イテレーション8130 || Loss: 0.3198 || 10iter: 26.2663 sec\n",
      "イテレーション8140 || Loss: 0.2149 || 10iter: 26.2393 sec\n",
      "イテレーション8150 || Loss: 0.4573 || 10iter: 26.2054 sec\n",
      "イテレーション8160 || Loss: 0.4140 || 10iter: 26.2724 sec\n",
      "イテレーション8170 || Loss: 0.3028 || 10iter: 26.1780 sec\n",
      "イテレーション8180 || Loss: 0.2078 || 10iter: 26.2629 sec\n",
      "イテレーション8190 || Loss: 0.1493 || 10iter: 26.1690 sec\n",
      "イテレーション8200 || Loss: 0.4042 || 10iter: 26.2127 sec\n",
      "イテレーション8210 || Loss: 0.3284 || 10iter: 26.2023 sec\n",
      "イテレーション8220 || Loss: 0.3290 || 10iter: 26.2968 sec\n",
      "イテレーション8230 || Loss: 0.5783 || 10iter: 26.2900 sec\n",
      "イテレーション8240 || Loss: 0.1341 || 10iter: 26.2681 sec\n",
      "イテレーション8250 || Loss: 0.4149 || 10iter: 26.1941 sec\n",
      "イテレーション8260 || Loss: 0.3288 || 10iter: 26.1753 sec\n",
      "イテレーション8270 || Loss: 0.3576 || 10iter: 26.2594 sec\n",
      "イテレーション8280 || Loss: 0.3114 || 10iter: 26.2698 sec\n",
      "イテレーション8290 || Loss: 0.3521 || 10iter: 26.2793 sec\n",
      "イテレーション8300 || Loss: 0.3799 || 10iter: 26.2938 sec\n",
      "イテレーション8310 || Loss: 0.2866 || 10iter: 26.2742 sec\n",
      "イテレーション8320 || Loss: 0.2101 || 10iter: 26.2119 sec\n",
      "イテレーション8330 || Loss: 0.3687 || 10iter: 26.2238 sec\n",
      "イテレーション8340 || Loss: 0.2069 || 10iter: 26.1994 sec\n",
      "イテレーション8350 || Loss: 0.3136 || 10iter: 26.2075 sec\n",
      "イテレーション8360 || Loss: 0.3655 || 10iter: 26.1746 sec\n",
      "イテレーション8370 || Loss: 0.4962 || 10iter: 26.2136 sec\n",
      "イテレーション8380 || Loss: 0.3888 || 10iter: 26.2116 sec\n",
      "イテレーション8390 || Loss: 0.1714 || 10iter: 26.1998 sec\n",
      "イテレーション8400 || Loss: 0.4132 || 10iter: 26.2593 sec\n",
      "イテレーション8410 || Loss: 0.3719 || 10iter: 26.2242 sec\n",
      "------------\n",
      "epoch 23 || Epoch_TRAIN_Loss:0.3005 || Epoch_VAL_Loss: 0.3036\n",
      "timer: 1060.5344 sec\n",
      "-------------\n",
      "Epoch 24/30\n",
      "-------------\n",
      "(train)\n",
      "イテレーション8420 || Loss: 0.2959 || 10iter: 3.0428 sec\n",
      "イテレーション8430 || Loss: 0.2177 || 10iter: 26.2453 sec\n",
      "イテレーション8440 || Loss: 0.1973 || 10iter: 26.2784 sec\n",
      "イテレーション8450 || Loss: 0.3366 || 10iter: 26.1587 sec\n",
      "イテレーション8460 || Loss: 0.3416 || 10iter: 26.1668 sec\n",
      "イテレーション8470 || Loss: 0.3016 || 10iter: 26.2261 sec\n",
      "イテレーション8480 || Loss: 0.3181 || 10iter: 26.2368 sec\n",
      "イテレーション8490 || Loss: 0.3107 || 10iter: 26.1712 sec\n",
      "イテレーション8500 || Loss: 0.2274 || 10iter: 26.1257 sec\n",
      "イテレーション8510 || Loss: 0.3879 || 10iter: 26.1816 sec\n",
      "イテレーション8520 || Loss: 0.4106 || 10iter: 26.2471 sec\n",
      "イテレーション8530 || Loss: 0.4185 || 10iter: 26.2029 sec\n",
      "イテレーション8540 || Loss: 0.1821 || 10iter: 26.1375 sec\n",
      "イテレーション8550 || Loss: 0.2636 || 10iter: 26.2597 sec\n",
      "イテレーション8560 || Loss: 0.4092 || 10iter: 26.2549 sec\n",
      "イテレーション8570 || Loss: 0.2023 || 10iter: 26.2184 sec\n",
      "イテレーション8580 || Loss: 0.2146 || 10iter: 26.2479 sec\n",
      "イテレーション8590 || Loss: 0.1485 || 10iter: 26.2901 sec\n",
      "イテレーション8600 || Loss: 0.3363 || 10iter: 26.2703 sec\n",
      "イテレーション8610 || Loss: 0.1463 || 10iter: 26.3252 sec\n",
      "イテレーション8620 || Loss: 0.3032 || 10iter: 26.3610 sec\n",
      "イテレーション8630 || Loss: 0.1860 || 10iter: 26.3635 sec\n",
      "イテレーション8640 || Loss: 0.3546 || 10iter: 26.3553 sec\n",
      "イテレーション8650 || Loss: 0.1480 || 10iter: 26.8776 sec\n",
      "イテレーション8660 || Loss: 0.3198 || 10iter: 26.2500 sec\n",
      "イテレーション8670 || Loss: 0.2152 || 10iter: 26.2115 sec\n",
      "イテレーション8680 || Loss: 0.3125 || 10iter: 26.9289 sec\n",
      "イテレーション8690 || Loss: 0.3838 || 10iter: 28.6680 sec\n",
      "イテレーション8700 || Loss: 0.2887 || 10iter: 28.2427 sec\n",
      "イテレーション8710 || Loss: 0.2586 || 10iter: 28.3332 sec\n",
      "イテレーション8720 || Loss: 0.4624 || 10iter: 27.2067 sec\n",
      "イテレーション8730 || Loss: 0.3375 || 10iter: 26.2354 sec\n",
      "イテレーション8740 || Loss: 0.2676 || 10iter: 26.2289 sec\n",
      "イテレーション8750 || Loss: 0.3875 || 10iter: 26.2491 sec\n",
      "イテレーション8760 || Loss: 0.3468 || 10iter: 26.3033 sec\n",
      "イテレーション8770 || Loss: 0.2960 || 10iter: 26.2290 sec\n",
      "イテレーション8780 || Loss: 0.3869 || 10iter: 26.2038 sec\n",
      "------------\n",
      "epoch 24 || Epoch_TRAIN_Loss:0.2952 || Epoch_VAL_Loss: 0.2983\n",
      "timer: 1070.9216 sec\n",
      "-------------\n",
      "Epoch 25/30\n",
      "-------------\n",
      "(train)\n",
      "イテレーション8790 || Loss: 0.4029 || 10iter: 14.6921 sec\n",
      "イテレーション8800 || Loss: 0.2776 || 10iter: 26.3199 sec\n",
      "イテレーション8810 || Loss: 0.1050 || 10iter: 26.3518 sec\n",
      "イテレーション8820 || Loss: 0.1764 || 10iter: 26.3486 sec\n",
      "イテレーション8830 || Loss: 0.3805 || 10iter: 26.3652 sec\n",
      "イテレーション8840 || Loss: 0.2354 || 10iter: 26.3895 sec\n",
      "イテレーション8850 || Loss: 0.2149 || 10iter: 26.3269 sec\n",
      "イテレーション8860 || Loss: 0.4689 || 10iter: 26.4189 sec\n",
      "イテレーション8870 || Loss: 0.3282 || 10iter: 26.3172 sec\n",
      "イテレーション8880 || Loss: 0.2242 || 10iter: 26.3646 sec\n",
      "イテレーション8890 || Loss: 0.2436 || 10iter: 26.8407 sec\n",
      "イテレーション8900 || Loss: 0.4000 || 10iter: 26.2530 sec\n",
      "イテレーション8910 || Loss: 0.3017 || 10iter: 26.2183 sec\n",
      "イテレーション8920 || Loss: 0.2155 || 10iter: 26.2651 sec\n",
      "イテレーション8930 || Loss: 0.2546 || 10iter: 29.1210 sec\n",
      "イテレーション8940 || Loss: 0.4274 || 10iter: 29.0818 sec\n",
      "イテレーション8950 || Loss: 0.2903 || 10iter: 28.7412 sec\n",
      "イテレーション8960 || Loss: 0.1330 || 10iter: 28.3090 sec\n",
      "イテレーション8970 || Loss: 0.3791 || 10iter: 26.2959 sec\n",
      "イテレーション8980 || Loss: 0.4633 || 10iter: 26.3073 sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "イテレーション8990 || Loss: 0.2323 || 10iter: 26.3093 sec\n",
      "イテレーション9000 || Loss: 0.2730 || 10iter: 26.3951 sec\n",
      "イテレーション9010 || Loss: 0.3219 || 10iter: 26.3560 sec\n",
      "イテレーション9020 || Loss: 0.2773 || 10iter: 26.3369 sec\n",
      "イテレーション9030 || Loss: 0.2474 || 10iter: 26.3014 sec\n",
      "イテレーション9040 || Loss: 0.3037 || 10iter: 26.2986 sec\n",
      "イテレーション9050 || Loss: 0.2639 || 10iter: 26.3459 sec\n",
      "イテレーション9060 || Loss: 0.1589 || 10iter: 26.3474 sec\n",
      "イテレーション9070 || Loss: 0.1389 || 10iter: 26.4084 sec\n",
      "イテレーション9080 || Loss: 0.1853 || 10iter: 26.3578 sec\n",
      "イテレーション9090 || Loss: 0.3094 || 10iter: 26.2781 sec\n",
      "イテレーション9100 || Loss: 0.2710 || 10iter: 26.3063 sec\n",
      "イテレーション9110 || Loss: 0.3332 || 10iter: 26.3383 sec\n",
      "イテレーション9120 || Loss: 0.1263 || 10iter: 26.3630 sec\n",
      "イテレーション9130 || Loss: 0.5653 || 10iter: 26.3345 sec\n",
      "イテレーション9140 || Loss: 0.1869 || 10iter: 26.3726 sec\n",
      "イテレーション9150 || Loss: 0.3300 || 10iter: 26.2849 sec\n",
      "-------------\n",
      "(val)\n",
      "------------\n",
      "epoch 25 || Epoch_TRAIN_Loss:0.2898 || Epoch_VAL_Loss: 0.6583\n",
      "timer: 1456.7053 sec\n",
      "-------------\n",
      "Epoch 26/30\n",
      "-------------\n",
      "(train)\n",
      "イテレーション9160 || Loss: 0.2229 || 10iter: 26.4088 sec\n",
      "イテレーション9170 || Loss: 0.2508 || 10iter: 26.8558 sec\n",
      "イテレーション9180 || Loss: 0.2754 || 10iter: 26.3735 sec\n",
      "イテレーション9190 || Loss: 0.4736 || 10iter: 26.3422 sec\n",
      "イテレーション9200 || Loss: 0.2108 || 10iter: 26.6514 sec\n",
      "イテレーション9210 || Loss: 0.2889 || 10iter: 29.2554 sec\n",
      "イテレーション9220 || Loss: 0.4011 || 10iter: 29.0505 sec\n",
      "イテレーション9230 || Loss: 0.2952 || 10iter: 32.3758 sec\n",
      "イテレーション9240 || Loss: 0.3124 || 10iter: 33.5032 sec\n",
      "イテレーション9250 || Loss: 0.3376 || 10iter: 29.2239 sec\n",
      "イテレーション9260 || Loss: 0.2573 || 10iter: 29.3738 sec\n",
      "イテレーション9270 || Loss: 0.4798 || 10iter: 26.5899 sec\n",
      "イテレーション9280 || Loss: 0.2403 || 10iter: 26.4435 sec\n",
      "イテレーション9290 || Loss: 0.4506 || 10iter: 26.4295 sec\n",
      "イテレーション9300 || Loss: 0.2179 || 10iter: 26.4678 sec\n",
      "イテレーション9310 || Loss: 0.1122 || 10iter: 26.3917 sec\n",
      "イテレーション9320 || Loss: 0.2646 || 10iter: 26.2873 sec\n",
      "イテレーション9330 || Loss: 0.1871 || 10iter: 26.3027 sec\n",
      "イテレーション9340 || Loss: 0.2021 || 10iter: 26.2749 sec\n",
      "イテレーション9350 || Loss: 0.2221 || 10iter: 26.3199 sec\n",
      "イテレーション9360 || Loss: 0.2731 || 10iter: 26.2736 sec\n",
      "イテレーション9370 || Loss: 0.1871 || 10iter: 26.2463 sec\n",
      "イテレーション9380 || Loss: 0.3931 || 10iter: 26.2742 sec\n",
      "イテレーション9390 || Loss: 0.1743 || 10iter: 26.4278 sec\n",
      "イテレーション9400 || Loss: 0.3282 || 10iter: 26.3221 sec\n",
      "イテレーション9410 || Loss: 0.3067 || 10iter: 26.3097 sec\n",
      "イテレーション9420 || Loss: 0.2542 || 10iter: 26.2760 sec\n",
      "イテレーション9430 || Loss: 0.2398 || 10iter: 26.3672 sec\n",
      "イテレーション9440 || Loss: 0.3879 || 10iter: 26.2726 sec\n",
      "イテレーション9450 || Loss: 0.1840 || 10iter: 26.2664 sec\n",
      "イテレーション9460 || Loss: 0.2927 || 10iter: 26.3108 sec\n",
      "イテレーション9470 || Loss: 0.3360 || 10iter: 26.3175 sec\n",
      "イテレーション9480 || Loss: 0.4256 || 10iter: 26.2968 sec\n",
      "イテレーション9490 || Loss: 0.4532 || 10iter: 26.3995 sec\n",
      "イテレーション9500 || Loss: 0.5125 || 10iter: 26.3670 sec\n",
      "イテレーション9510 || Loss: 0.1729 || 10iter: 26.3771 sec\n",
      "------------\n",
      "epoch 26 || Epoch_TRAIN_Loss:0.2850 || Epoch_VAL_Loss: 0.2880\n",
      "timer: 1094.9511 sec\n",
      "-------------\n",
      "Epoch 27/30\n",
      "-------------\n",
      "(train)\n",
      "イテレーション9520 || Loss: 0.3113 || 10iter: 8.8785 sec\n",
      "イテレーション9530 || Loss: 0.2417 || 10iter: 26.3351 sec\n",
      "イテレーション9540 || Loss: 0.4741 || 10iter: 26.2606 sec\n",
      "イテレーション9550 || Loss: 0.1276 || 10iter: 26.3232 sec\n",
      "イテレーション9560 || Loss: 0.2835 || 10iter: 26.3099 sec\n",
      "イテレーション9570 || Loss: 0.1420 || 10iter: 26.2702 sec\n",
      "イテレーション9580 || Loss: 0.3850 || 10iter: 26.2741 sec\n",
      "イテレーション9590 || Loss: 0.4560 || 10iter: 26.2812 sec\n",
      "イテレーション9600 || Loss: 0.2053 || 10iter: 26.4151 sec\n",
      "イテレーション9610 || Loss: 0.5201 || 10iter: 26.4086 sec\n",
      "イテレーション9620 || Loss: 0.1860 || 10iter: 26.3524 sec\n",
      "イテレーション9630 || Loss: 0.3253 || 10iter: 26.3649 sec\n",
      "イテレーション9640 || Loss: 0.1479 || 10iter: 26.3716 sec\n",
      "イテレーション9650 || Loss: 0.3093 || 10iter: 26.5423 sec\n",
      "イテレーション9660 || Loss: 0.1198 || 10iter: 26.4114 sec\n",
      "イテレーション9670 || Loss: 0.3044 || 10iter: 27.1209 sec\n",
      "イテレーション9680 || Loss: 0.3356 || 10iter: 26.3597 sec\n",
      "イテレーション9690 || Loss: 0.4670 || 10iter: 26.3628 sec\n",
      "イテレーション9700 || Loss: 0.4270 || 10iter: 27.7158 sec\n",
      "イテレーション9710 || Loss: 0.2665 || 10iter: 29.1721 sec\n",
      "イテレーション9720 || Loss: 0.3641 || 10iter: 29.0363 sec\n",
      "イテレーション9730 || Loss: 0.2620 || 10iter: 29.0900 sec\n",
      "イテレーション9740 || Loss: 0.3253 || 10iter: 26.5549 sec\n",
      "イテレーション9750 || Loss: 0.4357 || 10iter: 26.2515 sec\n",
      "イテレーション9760 || Loss: 0.2918 || 10iter: 26.3588 sec\n",
      "イテレーション9770 || Loss: 0.2607 || 10iter: 26.3457 sec\n",
      "イテレーション9780 || Loss: 0.1609 || 10iter: 26.2966 sec\n",
      "イテレーション9790 || Loss: 0.1863 || 10iter: 26.2792 sec\n",
      "イテレーション9800 || Loss: 0.3181 || 10iter: 26.3335 sec\n",
      "イテレーション9810 || Loss: 0.2977 || 10iter: 26.2426 sec\n",
      "イテレーション9820 || Loss: 0.2498 || 10iter: 26.2636 sec\n",
      "イテレーション9830 || Loss: 0.1112 || 10iter: 26.2153 sec\n",
      "イテレーション9840 || Loss: 0.3669 || 10iter: 26.2672 sec\n",
      "イテレーション9850 || Loss: 0.2495 || 10iter: 26.3176 sec\n",
      "イテレーション9860 || Loss: 0.2518 || 10iter: 26.2742 sec\n",
      "イテレーション9870 || Loss: 0.3273 || 10iter: 26.2700 sec\n",
      "イテレーション9880 || Loss: 0.3403 || 10iter: 26.2213 sec\n",
      "------------\n",
      "epoch 27 || Epoch_TRAIN_Loss:0.2879 || Epoch_VAL_Loss: 0.2908\n",
      "timer: 1076.5263 sec\n",
      "-------------\n",
      "Epoch 28/30\n",
      "-------------\n",
      "(train)\n",
      "イテレーション9890 || Loss: 0.1985 || 10iter: 20.4733 sec\n",
      "イテレーション9900 || Loss: 0.1687 || 10iter: 26.2351 sec\n",
      "イテレーション9910 || Loss: 0.4895 || 10iter: 26.1768 sec\n",
      "イテレーション9920 || Loss: 0.3718 || 10iter: 26.2374 sec\n",
      "イテレーション9930 || Loss: 0.2350 || 10iter: 26.2576 sec\n",
      "イテレーション9940 || Loss: 0.2381 || 10iter: 26.2104 sec\n",
      "イテレーション9950 || Loss: 0.1870 || 10iter: 26.2055 sec\n",
      "イテレーション9960 || Loss: 0.1837 || 10iter: 26.2324 sec\n",
      "イテレーション9970 || Loss: 0.3625 || 10iter: 26.3330 sec\n",
      "イテレーション9980 || Loss: 0.4148 || 10iter: 26.2955 sec\n",
      "イテレーション9990 || Loss: 0.3255 || 10iter: 26.1934 sec\n",
      "イテレーション10000 || Loss: 0.3976 || 10iter: 26.1998 sec\n",
      "イテレーション10010 || Loss: 0.1682 || 10iter: 26.2349 sec\n",
      "イテレーション10020 || Loss: 0.4998 || 10iter: 26.2747 sec\n",
      "イテレーション10030 || Loss: 0.6384 || 10iter: 26.3125 sec\n",
      "イテレーション10040 || Loss: 0.2616 || 10iter: 26.3578 sec\n",
      "イテレーション10050 || Loss: 0.2248 || 10iter: 26.3420 sec\n",
      "イテレーション10060 || Loss: 0.2320 || 10iter: 26.2621 sec\n",
      "イテレーション10070 || Loss: 0.3437 || 10iter: 26.8090 sec\n",
      "イテレーション10080 || Loss: 0.1866 || 10iter: 26.2902 sec\n",
      "イテレーション10090 || Loss: 0.1373 || 10iter: 26.2111 sec\n",
      "イテレーション10100 || Loss: 0.2500 || 10iter: 26.3839 sec\n",
      "イテレーション10110 || Loss: 0.2900 || 10iter: 28.9206 sec\n",
      "イテレーション10120 || Loss: 0.2014 || 10iter: 28.8764 sec\n",
      "イテレーション10130 || Loss: 0.2375 || 10iter: 28.9145 sec\n",
      "イテレーション10140 || Loss: 0.1076 || 10iter: 28.0615 sec\n",
      "イテレーション10150 || Loss: 0.3514 || 10iter: 26.2357 sec\n",
      "イテレーション10160 || Loss: 0.3162 || 10iter: 26.2523 sec\n",
      "イテレーション10170 || Loss: 0.1416 || 10iter: 26.2845 sec\n",
      "イテレーション10180 || Loss: 0.2233 || 10iter: 26.2743 sec\n",
      "イテレーション10190 || Loss: 0.1943 || 10iter: 26.2058 sec\n",
      "イテレーション10200 || Loss: 0.1832 || 10iter: 26.1776 sec\n",
      "イテレーション10210 || Loss: 0.3605 || 10iter: 26.2068 sec\n",
      "イテレーション10220 || Loss: 0.5200 || 10iter: 26.2158 sec\n",
      "イテレーション10230 || Loss: 0.2326 || 10iter: 26.1986 sec\n",
      "イテレーション10240 || Loss: 0.1387 || 10iter: 26.2022 sec\n",
      "------------\n",
      "epoch 28 || Epoch_TRAIN_Loss:0.2903 || Epoch_VAL_Loss: 0.2933\n",
      "timer: 1072.8027 sec\n",
      "-------------\n",
      "Epoch 29/30\n",
      "-------------\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'values' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-f70c6e4780af>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# batch_size 8 だと　cuda out of memoryになってしまったので 4 にしてみた\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mnum_epochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m30\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloaders_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-12-19ce109ed018>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(net, detaloaders_dict, criterion, scheduler, optimizer, num_epoch)\u001b[0m\n\u001b[1;32m     38\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mphase\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'train'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m                 \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m                 \u001b[0mscheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# 最適化schedulerの更新\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m                 \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'(train)'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/optim/lr_scheduler.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, epoch)\u001b[0m\n\u001b[1;32m    148\u001b[0m                     \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_lr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mparam_group\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_groups\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m             \u001b[0mparam_group\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lr'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'values' referenced before assignment"
     ]
    }
   ],
   "source": [
    "# 学習・検証の実施\n",
    "# batch_size 8 だと　cuda out of memoryになってしまったので 4 にしてみた\n",
    "num_epochs = 30\n",
    "train_model(net, dataloaders_dict, criterion, scheduler, optimizer, num_epoch=num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
