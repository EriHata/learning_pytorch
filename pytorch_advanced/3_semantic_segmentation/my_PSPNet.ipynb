{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ネットワークモデルの作成"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.3 学習目標\n",
    "\n",
    "1.\tPSPNetのネットワーク構造をモジュール単位で理解する\n",
    "2.\tPSPNetを構成する各モジュールの役割を理解する\n",
    "3.\tPSPNetのネットワーククラスの実装を理解する\n",
    "\n",
    "# 3.4 学習目標\n",
    "\n",
    "1.\tFeatureモジュールのサブネットワーク構成を理解する\n",
    "2.\tサブネットワークFeatureMap_convolution を実装できるようになる\n",
    "3.\tResidual Blockを理解する\n",
    "4.\tDilated Convolutionを理解する\n",
    "5.\tサブネットワークbottleNeckPSPとbottleNeckIdentifyPSPを実装できるようになる\n",
    "6.\tFeatureモジュールを実装できるようになる\n",
    "\n",
    "# 3.5 学習目標\n",
    "\n",
    "1.\tPyramid Poolingモジュールのサブネットワーク構成を理解する\n",
    "2.\tPyramid Poolingモジュールのマルチスケール処理の実現方法を理解する\n",
    "3.\tPyramid Poolingモジュールを実装できるようになる\n",
    "\n",
    "# 3.6 学習目標\n",
    "\n",
    "1.\tDecoderモジュールのサブネットワーク構成を理解する\n",
    "2.\tDecoder モジュールを実装できるようになる\n",
    "3.\tAuxLossモジュールのサブネットワーク構成を理解する\n",
    "4.\tAuxLossモジュールを実装できるようになる"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PSPNetのネットワーク構造\n",
    "__init__とforwardを実装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PSPNet(nn.Module):\n",
    "    def __init__(self, n_classes):\n",
    "        # コンストラクタを実行\n",
    "        super(PSPNet, self).__init__()\n",
    "        \n",
    "        # パラメータ設定\n",
    "        block_config = [3, 4, 6, 3]  # resnet50\n",
    "        img_size = 475\n",
    "        img_size_8 = 60  # img_sizeの1/8にする\n",
    "        \n",
    "        # 4つのモジュールを構成するサブネットワークの用意\n",
    "        # モデルの__init__の部分でサブネットワークを作ってしまうことが多い\n",
    "        # forwardでは純粋に順伝播実行するだけ\n",
    "        self.feature_conv = FeatureMap_convolution()\n",
    "        self.feature_res_1 = ResidualBlockPSP(n_blocks=block_config[0], in_channels=128, mid_channels=64, out_channels=256, stride=1, dilation=1)\n",
    "        self.feature_res_2 = ResidualBlockPSP(n_blocks=block_config[1], in_channels=256, mid_channels=128, out_channels=512, stride=2, dilation=1)\n",
    "        self.feature_dilated_res_1 = ResidualBlockPSP(n_blocks=block_config[2], in_channels=512, mid_channels=256, out_channels=1024, stride=1, dilation=2)\n",
    "        self.feature_dilated_res_2 = ResidualBlockPSP(n_blocks=block_config[3], in_channels=1024, mid_channels=512, out_channels=2048, stride=1, dilation=4)\n",
    "        \n",
    "        self.pyramid_pooling = PyramidPooling(in_channels=2048, pool_sizes=[6, 3, 2, 1], height=img_size_8, width=img_size_8)\n",
    "        \n",
    "        self.decode_feature = DecodePSPFeature(height=img_size, width=img_size, n_classes=n_classes)\n",
    "        \n",
    "        self.aux = AuxiliaryPSPlayers(in_channels=1024, height=img_size, width=img_size, n_classes=n_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.feature_conv(x)\n",
    "        x = self.feature_res_1(x)\n",
    "        x = self.feature_res_2(x)\n",
    "        x = self.feature_dilated_res_1(x)\n",
    "        \n",
    "        output_aux = self.aux(x)   # featureモジュールの途中をAuxモジュールへ\n",
    "        # この出力tensorを使用してピクセルごとのクラス分類を行い、損失値を前半4つのサブネットワークの学習に使用する\n",
    "        # そうしないとinputに近い層の学習が効率的に行われないから？\n",
    "        \n",
    "        x = self.feature_dilated_res_2(x)\n",
    "        \n",
    "        x = self.pyramid_pooling(x)\n",
    "        output = self.decode_feature(x)\n",
    "        \n",
    "        return (output, output_aux)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Featureモジュール"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "class conv2DBatchNormReLU(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride, padding, dilation, bias):\n",
    "        # コンストラクタ実行\n",
    "        super(conv2DBatchNormReLU, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding, dilation, bias=bias)\n",
    "        self.batchnorm = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)   # inplace=Trueでメモリ削減\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.batchnorm(x)\n",
    "        outputs = self.relu(x)\n",
    "        \n",
    "        return outputs\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 最初1回使われる\n",
    "class FeatureMap_convolution(nn.Module):\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        構成するサブネットワークを用意\n",
    "        conv2DBatchNormReLUを3回とmaxpooling1回\n",
    "        \"\"\"\n",
    "        super(FeatureMap_convolution, self).__init__()\n",
    "        \n",
    "        # 畳み込み1\n",
    "        in_channels, out_channels, kernel_size, stride, padding, dilation, bias = 3, 64, 3, 2, 1, 1, False\n",
    "        self.cbnr_1 = conv2DBatchNormReLU(in_channels, out_channels, kernel_size, stride, padding, dilation, bias)\n",
    "        \n",
    "        # 畳み込み2\n",
    "        in_channels, out_channels, kernel_size, stride, padding, dilation, bias = 64, 64, 3, 1, 1, 1, False\n",
    "        self.cbnr_2 = conv2DBatchNormReLU(in_channels, out_channels, kernel_size, stride, padding, dilation, bias)\n",
    "        \n",
    "        # 畳み込み3\n",
    "        in_channels, out_channels, kernel_size, stride, padding, dilation, bias = 64, 128, 3, 1, 1, 1, False\n",
    "        self.cbnr_3 = conv2DBatchNormReLU(in_channels, out_channels, kernel_size, stride, padding, dilation, bias)\n",
    "        \n",
    "        # MaxPooling\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.cbnr_1(x)\n",
    "        x = self.cbnr_2(x)\n",
    "        x = self.cbnr_3(x)\n",
    "        outputs = self.maxpool(x)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "# featuremapのあとに4回使われる\n",
    "# ResNetと同じResidualBlockという構造を利用している\n",
    "# bottleNeckPSP1回とbottleNeckIdntifyPSP複数回繰り返して作られるblock\n",
    "\n",
    "class ResidualBlockPSP(nn.Sequential):\n",
    "    def __init__(self, n_blocks, in_channels, mid_channels, out_channels, stride, dilation):\n",
    "        super(ResidualBlockPSP, self).__init__()\n",
    "        \n",
    "        # bottleNeckPSPの用意\n",
    "        self.add_module('block1', bottleNeckPSP(in_channels, mid_channels, out_channels, stride, dilation))\n",
    "        \n",
    "        # bottleNeckIdentifyPSPの繰り返しの用意\n",
    "        for i in range(n_blocks-1):\n",
    "            self.add_module('block'+str(i+2), bottleNeckIdentifyPSP(out_channels, mid_channels, stride, dilation))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "class conv2DBatchNorm(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride, padding, dilation, bias):\n",
    "        super(conv2DBatchNorm, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding, dilation, bias=bias)\n",
    "        self.batchnorm = nn.BatchNorm2d(out_channels)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        outputs = self.batchnorm(x)\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "class bottleNeckPSP(nn.Module):\n",
    "    def __init__(self, in_channels, mid_channels, out_channels, stride, dilation):\n",
    "        super(bottleNeckPSP, self).__init__()\n",
    "        self.cbr_1 = conv2DBatchNormReLU(in_channels, mid_channels, kernel_size=1, stride=1, padding=0, dilation=1, bias=False)\n",
    "        self.cbr_2 = conv2DBatchNormReLU(mid_channels, mid_channels, kernel_size=3, stride=stride, padding=dilation, dilation=dilation, bias=False)\n",
    "        self.cb_3 = conv2DBatchNorm(mid_channels, out_channels, kernel_size=1, stride=1, padding=0, dilation=1, bias=False)\n",
    "        \n",
    "        #  スッキプ結合  こうやるのか\n",
    "        self.cb_residual = conv2DBatchNorm(in_channels, out_channels, kernel_size=1, stride=stride, padding=0, dilation=1, bias=False)\n",
    "        self.relu = nn.ReLU(inplace=True)  # メモリ節約\n",
    "        \n",
    "    def forward(self, x):\n",
    "        conv = self.cb_3(self.cbr_2(self.cbr_1(x)))\n",
    "        residual = self.cb_residual(x)\n",
    "        \n",
    "        # スキップしたものと全部convしたものを加算してreluに入れて返す\n",
    "#         print(conv.size())\n",
    "#         print(residual.size())\n",
    "#         print((conv+residual).size())\n",
    "        out = conv + residual\n",
    "        return self.relu(out)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "class bottleNeckIdentifyPSP(nn.Module):\n",
    "    def __init__(self, in_channels, mid_channels, stride, dilation):\n",
    "        super(bottleNeckIdentifyPSP, self).__init__()\n",
    "        self.cbr_1 = conv2DBatchNormReLU(in_channels, mid_channels, kernel_size=1, stride=1, padding=0, dilation=1, bias=False)\n",
    "        self.cbr_2 = conv2DBatchNormReLU(mid_channels, mid_channels, kernel_size=3, stride=1, padding=dilation, dilation=dilation, bias=False)\n",
    "        self.cb_3 = conv2DBatchNorm(mid_channels, in_channels, kernel_size=1, stride=1, padding=0, dilation=1, bias=False)  # 最後はReLUいらない\n",
    "        self.relu = nn.ReLU(inplace=True)  # メモリ節約\n",
    "        \n",
    "    def forward(self, x):\n",
    "        conv = self.cb_3(self.cbr_2(self.cbr_1(x)))\n",
    "        residual = x\n",
    "        # inputそのままとconvしたものを加算してreluに入れて返す\n",
    "        return self.relu(conv+residual)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pyramid Poolingモジュール"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 結構お手本と違う形にしたがうまくいくか\n",
    "# うまくいかなかったらお手本に戻す\n",
    "class PyramidPooling(nn.Module):\n",
    "    def __init__(self, in_channels, pool_sizes, height, width):\n",
    "        super(PyramidPooling, self).__init__()\n",
    "        \n",
    "        # forwardで使用する画像サイズとpoolsizes, in_channels\n",
    "        self.height = height\n",
    "        self.width = width\n",
    "        self.pool_sizes = pool_sizes\n",
    "        self.in_channels = in_channels\n",
    "        \n",
    "        # 各畳み込み層の出力チャネル数\n",
    "        self.out_channels = int(in_channels / len(pool_sizes))  # 2048/4 = 512\n",
    "        \n",
    "        \n",
    "    # 各層の作成はforwardでやる\n",
    "    def forward(self, x):\n",
    "        outputs = [x]\n",
    "        \n",
    "        for i in range(len(self.pool_sizes)):\n",
    "            x_ = nn.AdaptiveAvgPool2d(output_size=self.pool_sizes[i])(x)\n",
    "            x_ = conv2DBatchNormReLU(self.in_channels, self.out_channels, kernel_size=1, stride=1, padding=0, dilation=1, bias=False)(x_)\n",
    "            x_ = F.interpolate(x_, size=(self.height, self.width), mode='bilinear', align_corners=True)\n",
    "            outputs.append(x_)\n",
    "            \n",
    "        # マルチスケールで特徴抽出したtensorを最終的に結合させる\n",
    "        output = torch.cat(outputs, dim=1)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "# self.~~で層を保持していくには名前ベタが期していくしかないんだあろうか"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add_moduleつかったらどうだろう\n",
    "class PyramidPooling(nn.Module):\n",
    "    def __init__(self, in_channels, pool_sizes, height, width):\n",
    "        super(PyramidPooling, self).__init__()\n",
    "        \n",
    "        # forwardで使用する画像サイズとpoolsizes, in_channels\n",
    "        self.height = height\n",
    "        self.width = width\n",
    "        self.pool_sizes = pool_sizes\n",
    "        self.in_channels = in_channels\n",
    "        \n",
    "        # 各畳み込み層の出力チャネル数\n",
    "        self.out_channels = int(in_channels / len(pool_sizes))  # 2048/4 = 512\n",
    "        \n",
    "        \n",
    "    # 各層の作成はforwardでやる\n",
    "    def forward(self, x):\n",
    "        outputs = [x]\n",
    "        \n",
    "        for i in range(len(self.pool_sizes)):\n",
    "            self.add_module('avpool_'+str(i+1), nn.AdaptiveAvgPool2d(output_size=self.pool_sizes[i]))\n",
    "            self.add_module('cbr_'+str(i+1), conv2DBatchNormReLU(self.in_channels, self.out_channels, kernel_size=1, stride=1, padding=0, dilation=1, bias=False))\n",
    "            x_ = self.parameters()[i*2](x)   # 破綻した？\n",
    "            x_ = conv2DBatchNormReLU(self.in_channels, self.out_channels, kernel_size=1, stride=1, padding=0, dilation=1, bias=False)(x_)\n",
    "            x_ = F.interpolate(x_, size=(self.height, self.width), mode='bilinear', align_corners=True)\n",
    "            outputs.apped(x_)\n",
    "            \n",
    "        # マルチスケールで特徴抽出したtensorを最終的に結合させる\n",
    "        output = torch.cat(outputs, dim=1)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoderモジュール"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecodePSPFeature(nn.Module):\n",
    "    def __init__(self, height, width, n_classes):\n",
    "        super(DecodePSPFeature, self).__init__()\n",
    "        \n",
    "        self.height = height\n",
    "        self.width = width\n",
    "        \n",
    "        # cbr : conv, batchnorm, relu\n",
    "        self.cbr = conv2DBatchNormReLU(in_channels=4096, out_channels=512, kernel_size=3, stride=1, padding=1, dilation=1, bias=False)\n",
    "        self.dropout = nn.Dropout2d(p=0.1)\n",
    "        self.classification = nn.Conv2d(in_channels=512, out_channels=n_classes, kernel_size=1, stride=1, padding=0)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.cbr(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.classification(x)\n",
    "        output = F.interpolate(x, size=(self.height, self.width), mode='bilinear', align_corners=True)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AuxLossモジュール"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AuxiliaryPSPlayers(nn.Module):\n",
    "    def __init__(self, in_channels, height, width, n_classes):\n",
    "        super(AuxiliaryPSPlayers, self).__init__()\n",
    "        \n",
    "        self.height = height\n",
    "        self.width = width\n",
    "        \n",
    "        # cbr : conv, batchnorm, relu\n",
    "        self.cbr = conv2DBatchNormReLU(in_channels=in_channels, out_channels=256, kernel_size=3, stride=1, padding=1, dilation=1, bias=False)\n",
    "        self.dropout = nn.Dropout2d(p=0.1)\n",
    "        self.classification = nn.Conv2d(in_channels=256, out_channels=n_classes, kernel_size=1, stride=1, padding=0)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.cbr(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.classification(x)\n",
    "        output = F.interpolate(x, size=(self.height, self.width), mode='bilinear', align_corners=True)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PSPNet(\n",
       "  (feature_conv): FeatureMap_convolution(\n",
       "    (cbnr_1): conv2DBatchNormReLU(\n",
       "      (conv): Conv2d(3, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (batchnorm): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (cbnr_2): conv2DBatchNormReLU(\n",
       "      (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (batchnorm): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (cbnr_3): conv2DBatchNormReLU(\n",
       "      (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (batchnorm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (feature_res_1): ResidualBlockPSP(\n",
       "    (block1): bottleNeckPSP(\n",
       "      (cbr_1): conv2DBatchNormReLU(\n",
       "        (conv): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (batchnorm): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (cbr_2): conv2DBatchNormReLU(\n",
       "        (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (batchnorm): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (cb_3): conv2DBatchNorm(\n",
       "        (conv): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (batchnorm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (cb_residual): conv2DBatchNorm(\n",
       "        (conv): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (batchnorm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (block2): bottleNeckIdentifyPSP(\n",
       "      (cbr_1): conv2DBatchNormReLU(\n",
       "        (conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (batchnorm): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (cbr_2): conv2DBatchNormReLU(\n",
       "        (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (batchnorm): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (cb_3): conv2DBatchNorm(\n",
       "        (conv): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (batchnorm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (block3): bottleNeckIdentifyPSP(\n",
       "      (cbr_1): conv2DBatchNormReLU(\n",
       "        (conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (batchnorm): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (cbr_2): conv2DBatchNormReLU(\n",
       "        (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (batchnorm): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (cb_3): conv2DBatchNorm(\n",
       "        (conv): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (batchnorm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (feature_res_2): ResidualBlockPSP(\n",
       "    (block1): bottleNeckPSP(\n",
       "      (cbr_1): conv2DBatchNormReLU(\n",
       "        (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (batchnorm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (cbr_2): conv2DBatchNormReLU(\n",
       "        (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (batchnorm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (cb_3): conv2DBatchNorm(\n",
       "        (conv): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (batchnorm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (cb_residual): conv2DBatchNorm(\n",
       "        (conv): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (batchnorm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (block2): bottleNeckIdentifyPSP(\n",
       "      (cbr_1): conv2DBatchNormReLU(\n",
       "        (conv): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (batchnorm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (cbr_2): conv2DBatchNormReLU(\n",
       "        (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (batchnorm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (cb_3): conv2DBatchNorm(\n",
       "        (conv): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (batchnorm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (block3): bottleNeckIdentifyPSP(\n",
       "      (cbr_1): conv2DBatchNormReLU(\n",
       "        (conv): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (batchnorm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (cbr_2): conv2DBatchNormReLU(\n",
       "        (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (batchnorm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (cb_3): conv2DBatchNorm(\n",
       "        (conv): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (batchnorm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (block4): bottleNeckIdentifyPSP(\n",
       "      (cbr_1): conv2DBatchNormReLU(\n",
       "        (conv): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (batchnorm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (cbr_2): conv2DBatchNormReLU(\n",
       "        (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (batchnorm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (cb_3): conv2DBatchNorm(\n",
       "        (conv): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (batchnorm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (feature_dilated_res_1): ResidualBlockPSP(\n",
       "    (block1): bottleNeckPSP(\n",
       "      (cbr_1): conv2DBatchNormReLU(\n",
       "        (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (batchnorm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (cbr_2): conv2DBatchNormReLU(\n",
       "        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
       "        (batchnorm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (cb_3): conv2DBatchNorm(\n",
       "        (conv): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (batchnorm): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (cb_residual): conv2DBatchNorm(\n",
       "        (conv): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (batchnorm): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (block2): bottleNeckIdentifyPSP(\n",
       "      (cbr_1): conv2DBatchNormReLU(\n",
       "        (conv): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (batchnorm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (cbr_2): conv2DBatchNormReLU(\n",
       "        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
       "        (batchnorm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (cb_3): conv2DBatchNorm(\n",
       "        (conv): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (batchnorm): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (block3): bottleNeckIdentifyPSP(\n",
       "      (cbr_1): conv2DBatchNormReLU(\n",
       "        (conv): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (batchnorm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (cbr_2): conv2DBatchNormReLU(\n",
       "        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
       "        (batchnorm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (cb_3): conv2DBatchNorm(\n",
       "        (conv): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (batchnorm): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (block4): bottleNeckIdentifyPSP(\n",
       "      (cbr_1): conv2DBatchNormReLU(\n",
       "        (conv): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (batchnorm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (cbr_2): conv2DBatchNormReLU(\n",
       "        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
       "        (batchnorm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (cb_3): conv2DBatchNorm(\n",
       "        (conv): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (batchnorm): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (block5): bottleNeckIdentifyPSP(\n",
       "      (cbr_1): conv2DBatchNormReLU(\n",
       "        (conv): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (batchnorm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (cbr_2): conv2DBatchNormReLU(\n",
       "        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
       "        (batchnorm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (cb_3): conv2DBatchNorm(\n",
       "        (conv): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (batchnorm): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (block6): bottleNeckIdentifyPSP(\n",
       "      (cbr_1): conv2DBatchNormReLU(\n",
       "        (conv): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (batchnorm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (cbr_2): conv2DBatchNormReLU(\n",
       "        (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
       "        (batchnorm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (cb_3): conv2DBatchNorm(\n",
       "        (conv): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (batchnorm): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (feature_dilated_res_2): ResidualBlockPSP(\n",
       "    (block1): bottleNeckPSP(\n",
       "      (cbr_1): conv2DBatchNormReLU(\n",
       "        (conv): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (batchnorm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (cbr_2): conv2DBatchNormReLU(\n",
       "        (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(4, 4), dilation=(4, 4), bias=False)\n",
       "        (batchnorm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (cb_3): conv2DBatchNorm(\n",
       "        (conv): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (batchnorm): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (cb_residual): conv2DBatchNorm(\n",
       "        (conv): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (batchnorm): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (block2): bottleNeckIdentifyPSP(\n",
       "      (cbr_1): conv2DBatchNormReLU(\n",
       "        (conv): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (batchnorm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (cbr_2): conv2DBatchNormReLU(\n",
       "        (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(4, 4), dilation=(4, 4), bias=False)\n",
       "        (batchnorm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (cb_3): conv2DBatchNorm(\n",
       "        (conv): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (batchnorm): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (block3): bottleNeckIdentifyPSP(\n",
       "      (cbr_1): conv2DBatchNormReLU(\n",
       "        (conv): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (batchnorm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (cbr_2): conv2DBatchNormReLU(\n",
       "        (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(4, 4), dilation=(4, 4), bias=False)\n",
       "        (batchnorm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (cb_3): conv2DBatchNorm(\n",
       "        (conv): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (batchnorm): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (pyramid_pooling): PyramidPooling()\n",
       "  (decode_feature): DecodePSPFeature(\n",
       "    (cbr): conv2DBatchNormReLU(\n",
       "      (conv): Conv2d(4096, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (batchnorm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (dropout): Dropout2d(p=0.1, inplace=False)\n",
       "    (classification): Conv2d(512, 21, kernel_size=(1, 1), stride=(1, 1))\n",
       "  )\n",
       "  (aux): AuxiliaryPSPlayers(\n",
       "    (cbr): conv2DBatchNormReLU(\n",
       "      (conv): Conv2d(1024, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (batchnorm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (dropout): Dropout2d(p=0.1, inplace=False)\n",
       "    (classification): Conv2d(256, 21, kernel_size=(1, 1), stride=(1, 1))\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 動作確認\n",
    "net = PSPNet(n_classes=21)\n",
    "net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyramidpoolingができていなかった\n",
    "# というかself.~~で保持していないだけのような気がする\n",
    "# bottleNeckPSPのcbrが全て１になっていた\n",
    "# pyramidpoolingをselfで保持しながらfor文で効率的に回すのってどうやるんだろう？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[[[-0.0998, -0.0743, -0.0488,  ...,  0.2021,  0.1882,  0.1743],\n",
      "          [-0.1113, -0.0924, -0.0735,  ...,  0.1828,  0.1665,  0.1502],\n",
      "          [-0.1228, -0.1105, -0.0982,  ...,  0.1634,  0.1448,  0.1261],\n",
      "          ...,\n",
      "          [ 0.1167,  0.1208,  0.1249,  ..., -0.1208, -0.1501, -0.1794],\n",
      "          [ 0.0936,  0.1007,  0.1078,  ..., -0.1066, -0.1262, -0.1457],\n",
      "          [ 0.0704,  0.0806,  0.0907,  ..., -0.0924, -0.1022, -0.1120]],\n",
      "\n",
      "         [[-0.2531, -0.2540, -0.2548,  ...,  0.0965,  0.0247, -0.0471],\n",
      "          [-0.2291, -0.2282, -0.2273,  ...,  0.1204,  0.0537, -0.0131],\n",
      "          [-0.2051, -0.2025, -0.1998,  ...,  0.1444,  0.0827,  0.0210],\n",
      "          ...,\n",
      "          [-0.1074, -0.1194, -0.1314,  ...,  0.1365,  0.1456,  0.1548],\n",
      "          [-0.1203, -0.1450, -0.1697,  ...,  0.1627,  0.1771,  0.1916],\n",
      "          [-0.1332, -0.1706, -0.2080,  ...,  0.1888,  0.2086,  0.2284]],\n",
      "\n",
      "         [[-0.0372, -0.0161,  0.0049,  ..., -0.5846, -0.5849, -0.5852],\n",
      "          [-0.0632, -0.0369, -0.0106,  ..., -0.5591, -0.5563, -0.5535],\n",
      "          [-0.0892, -0.0576, -0.0261,  ..., -0.5336, -0.5278, -0.5219],\n",
      "          ...,\n",
      "          [-0.3145, -0.2857, -0.2570,  ..., -0.3479, -0.3902, -0.4325],\n",
      "          [-0.3032, -0.2723, -0.2413,  ..., -0.4009, -0.4449, -0.4890],\n",
      "          [-0.2919, -0.2588, -0.2256,  ..., -0.4538, -0.4996, -0.5454]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.2532, -0.3028, -0.3524,  ..., -0.3993, -0.4344, -0.4695],\n",
      "          [-0.2403, -0.2923, -0.3444,  ..., -0.3714, -0.4006, -0.4298],\n",
      "          [-0.2273, -0.2818, -0.3363,  ..., -0.3435, -0.3668, -0.3902],\n",
      "          ...,\n",
      "          [-0.8208, -0.7464, -0.6720,  ..., -0.4965, -0.5250, -0.5534],\n",
      "          [-0.8095, -0.7423, -0.6751,  ..., -0.5309, -0.5696, -0.6083],\n",
      "          [-0.7982, -0.7381, -0.6781,  ..., -0.5652, -0.6142, -0.6633]],\n",
      "\n",
      "         [[-0.4615, -0.4347, -0.4079,  ..., -0.2855, -0.2377, -0.1898],\n",
      "          [-0.4021, -0.3814, -0.3608,  ..., -0.2937, -0.2505, -0.2074],\n",
      "          [-0.3427, -0.3281, -0.3136,  ..., -0.3018, -0.2634, -0.2250],\n",
      "          ...,\n",
      "          [ 0.2178,  0.2183,  0.2189,  ..., -0.0203, -0.0217, -0.0231],\n",
      "          [ 0.2008,  0.2081,  0.2153,  ..., -0.0532, -0.0484, -0.0436],\n",
      "          [ 0.1839,  0.1978,  0.2117,  ..., -0.0861, -0.0751, -0.0641]],\n",
      "\n",
      "         [[ 0.1127,  0.0986,  0.0845,  ...,  0.5638,  0.6273,  0.6908],\n",
      "          [ 0.0981,  0.0849,  0.0718,  ...,  0.5120,  0.5680,  0.6241],\n",
      "          [ 0.0835,  0.0713,  0.0591,  ...,  0.4602,  0.5088,  0.5574],\n",
      "          ...,\n",
      "          [ 0.8535,  0.8121,  0.7707,  ...,  0.2371,  0.2062,  0.1753],\n",
      "          [ 0.8275,  0.7978,  0.7680,  ...,  0.2667,  0.2377,  0.2086],\n",
      "          [ 0.8015,  0.7834,  0.7653,  ...,  0.2963,  0.2691,  0.2420]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0821,  0.0719,  0.0617,  ...,  0.2357,  0.2669,  0.2981],\n",
      "          [ 0.0582,  0.0480,  0.0377,  ...,  0.2453,  0.2721,  0.2990],\n",
      "          [ 0.0343,  0.0241,  0.0138,  ...,  0.2549,  0.2774,  0.2998],\n",
      "          ...,\n",
      "          [ 0.0089,  0.0186,  0.0283,  ...,  0.1504,  0.1406,  0.1308],\n",
      "          [ 0.0595,  0.0682,  0.0769,  ...,  0.1677,  0.1568,  0.1458],\n",
      "          [ 0.1102,  0.1178,  0.1254,  ...,  0.1850,  0.1729,  0.1608]],\n",
      "\n",
      "         [[-0.1429, -0.1564, -0.1700,  ...,  0.1470,  0.1814,  0.2159],\n",
      "          [-0.1130, -0.1312, -0.1493,  ...,  0.1554,  0.1813,  0.2072],\n",
      "          [-0.0832, -0.1059, -0.1287,  ...,  0.1638,  0.1812,  0.1985],\n",
      "          ...,\n",
      "          [ 0.2771,  0.2817,  0.2863,  ...,  0.1350,  0.1500,  0.1650],\n",
      "          [ 0.2756,  0.2889,  0.3022,  ...,  0.1444,  0.1527,  0.1610],\n",
      "          [ 0.2741,  0.2961,  0.3181,  ...,  0.1538,  0.1554,  0.1570]],\n",
      "\n",
      "         [[-0.0337, -0.0410, -0.0483,  ..., -0.4375, -0.4001, -0.3627],\n",
      "          [-0.0301, -0.0404, -0.0507,  ..., -0.4079, -0.3707, -0.3335],\n",
      "          [-0.0265, -0.0398, -0.0532,  ..., -0.3783, -0.3412, -0.3042],\n",
      "          ...,\n",
      "          [-0.0594, -0.0407, -0.0220,  ..., -0.0988, -0.1831, -0.2674],\n",
      "          [-0.0174,  0.0024,  0.0221,  ..., -0.1032, -0.1905, -0.2778],\n",
      "          [ 0.0245,  0.0454,  0.0663,  ..., -0.1076, -0.1979, -0.2882]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.4908, -0.4452, -0.3996,  ...,  0.2000,  0.1680,  0.1361],\n",
      "          [-0.4516, -0.4079, -0.3641,  ...,  0.1682,  0.1359,  0.1035],\n",
      "          [-0.4124, -0.3705, -0.3286,  ...,  0.1365,  0.1037,  0.0710],\n",
      "          ...,\n",
      "          [-0.1417, -0.1512, -0.1607,  ..., -0.0657, -0.0361, -0.0064],\n",
      "          [-0.1577, -0.1693, -0.1808,  ..., -0.0642, -0.0288,  0.0067],\n",
      "          [-0.1738, -0.1873, -0.2009,  ..., -0.0628, -0.0215,  0.0198]],\n",
      "\n",
      "         [[ 0.1831,  0.1480,  0.1129,  ..., -0.4866, -0.4982, -0.5097],\n",
      "          [ 0.1802,  0.1480,  0.1157,  ..., -0.4458, -0.4557, -0.4656],\n",
      "          [ 0.1773,  0.1479,  0.1186,  ..., -0.4050, -0.4132, -0.4214],\n",
      "          ...,\n",
      "          [ 0.3662,  0.3316,  0.2970,  ...,  0.1106,  0.0617,  0.0129],\n",
      "          [ 0.4103,  0.3751,  0.3400,  ...,  0.1208,  0.0707,  0.0207],\n",
      "          [ 0.4543,  0.4187,  0.3830,  ...,  0.1310,  0.0798,  0.0285]],\n",
      "\n",
      "         [[ 0.3405,  0.3850,  0.4296,  ...,  0.0824,  0.0394, -0.0035],\n",
      "          [ 0.2930,  0.3310,  0.3691,  ...,  0.0969,  0.0642,  0.0315],\n",
      "          [ 0.2455,  0.2771,  0.3086,  ...,  0.1115,  0.0890,  0.0666],\n",
      "          ...,\n",
      "          [ 0.7506,  0.7010,  0.6514,  ...,  0.1531,  0.1685,  0.1839],\n",
      "          [ 0.7825,  0.7238,  0.6651,  ...,  0.1427,  0.1670,  0.1912],\n",
      "          [ 0.8143,  0.7466,  0.6789,  ...,  0.1324,  0.1654,  0.1985]]]],\n",
      "       grad_fn=<UpsampleBilinear2DBackward>), tensor([[[[ 3.5400e-02,  6.6177e-02,  9.6955e-02,  ...,  1.7992e-01,\n",
      "            1.2452e-01,  6.9125e-02],\n",
      "          [ 8.9395e-02,  1.0700e-01,  1.2460e-01,  ...,  1.3982e-01,\n",
      "            8.6517e-02,  3.3211e-02],\n",
      "          [ 1.4339e-01,  1.4782e-01,  1.5224e-01,  ...,  9.9723e-02,\n",
      "            4.8510e-02, -2.7030e-03],\n",
      "          ...,\n",
      "          [ 1.8055e-01,  1.7416e-01,  1.6777e-01,  ..., -1.1293e-01,\n",
      "           -1.1091e-01, -1.0890e-01],\n",
      "          [ 1.7466e-01,  1.6723e-01,  1.5979e-01,  ..., -1.6651e-01,\n",
      "           -1.6650e-01, -1.6648e-01],\n",
      "          [ 1.6878e-01,  1.6030e-01,  1.5182e-01,  ..., -2.2009e-01,\n",
      "           -2.2208e-01, -2.2406e-01]],\n",
      "\n",
      "         [[-4.5831e-01, -4.3465e-01, -4.1098e-01,  ..., -2.9591e-01,\n",
      "           -3.2100e-01, -3.4610e-01],\n",
      "          [-4.5594e-01, -4.4026e-01, -4.2458e-01,  ..., -2.8289e-01,\n",
      "           -3.0517e-01, -3.2745e-01],\n",
      "          [-4.5357e-01, -4.4588e-01, -4.3818e-01,  ..., -2.6988e-01,\n",
      "           -2.8934e-01, -3.0879e-01],\n",
      "          ...,\n",
      "          [-6.3168e-01, -6.1035e-01, -5.8902e-01,  ..., -8.6594e-02,\n",
      "            1.0764e-05,  8.6616e-02],\n",
      "          [-6.3262e-01, -6.1119e-01, -5.8976e-01,  ..., -9.7462e-02,\n",
      "           -9.9225e-03,  7.7617e-02],\n",
      "          [-6.3357e-01, -6.1204e-01, -5.9051e-01,  ..., -1.0833e-01,\n",
      "           -1.9856e-02,  6.8619e-02]],\n",
      "\n",
      "         [[ 5.9014e-01,  5.6724e-01,  5.4434e-01,  ...,  5.9764e-02,\n",
      "           -4.1599e-02, -1.4296e-01],\n",
      "          [ 5.9180e-01,  5.7435e-01,  5.5691e-01,  ...,  9.2687e-02,\n",
      "            3.8436e-04, -9.1918e-02],\n",
      "          [ 5.9346e-01,  5.8147e-01,  5.6947e-01,  ...,  1.2561e-01,\n",
      "            4.2367e-02, -4.0875e-02],\n",
      "          ...,\n",
      "          [ 9.4648e-01,  8.8143e-01,  8.1638e-01,  ...,  1.5693e-01,\n",
      "            9.7582e-02,  3.8230e-02],\n",
      "          [ 9.5651e-01,  8.9447e-01,  8.3243e-01,  ...,  1.1383e-01,\n",
      "            5.5801e-02, -2.2281e-03],\n",
      "          [ 9.6653e-01,  9.0750e-01,  8.4847e-01,  ...,  7.0725e-02,\n",
      "            1.4019e-02, -4.2686e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-3.3222e-01, -2.9657e-01, -2.6092e-01,  ..., -4.7343e-02,\n",
      "            7.2567e-03,  6.1857e-02],\n",
      "          [-2.6052e-01, -2.3546e-01, -2.1039e-01,  ..., -3.9064e-02,\n",
      "            4.0406e-03,  4.7145e-02],\n",
      "          [-1.8882e-01, -1.7434e-01, -1.5987e-01,  ..., -3.0785e-02,\n",
      "            8.2445e-04,  3.2434e-02],\n",
      "          ...,\n",
      "          [-2.4756e-01, -2.2544e-01, -2.0332e-01,  ...,  8.4933e-03,\n",
      "            2.5586e-03, -3.3761e-03],\n",
      "          [-2.7625e-01, -2.5016e-01, -2.2407e-01,  ...,  3.0905e-03,\n",
      "            2.2403e-04, -2.6425e-03],\n",
      "          [-3.0494e-01, -2.7488e-01, -2.4482e-01,  ..., -2.3123e-03,\n",
      "           -2.1106e-03, -1.9088e-03]],\n",
      "\n",
      "         [[ 4.1345e-01,  3.8060e-01,  3.4775e-01,  ...,  1.7088e-01,\n",
      "            1.9473e-01,  2.1858e-01],\n",
      "          [ 3.5066e-01,  3.2127e-01,  2.9189e-01,  ...,  1.1579e-01,\n",
      "            1.3697e-01,  1.5815e-01],\n",
      "          [ 2.8786e-01,  2.6195e-01,  2.3603e-01,  ...,  6.0708e-02,\n",
      "            7.9218e-02,  9.7728e-02],\n",
      "          ...,\n",
      "          [ 5.5240e-02,  2.9432e-02,  3.6247e-03,  ..., -1.3397e-01,\n",
      "           -1.5835e-01, -1.8273e-01],\n",
      "          [ 7.0335e-02,  4.0931e-02,  1.1527e-02,  ..., -1.3178e-01,\n",
      "           -1.5593e-01, -1.8008e-01],\n",
      "          [ 8.5430e-02,  5.2430e-02,  1.9429e-02,  ..., -1.2960e-01,\n",
      "           -1.5352e-01, -1.7743e-01]],\n",
      "\n",
      "         [[ 3.8971e-01,  4.3007e-01,  4.7042e-01,  ...,  3.0795e-01,\n",
      "            2.1727e-01,  1.2659e-01],\n",
      "          [ 4.3330e-01,  4.7264e-01,  5.1198e-01,  ...,  2.9180e-01,\n",
      "            2.0217e-01,  1.1255e-01],\n",
      "          [ 4.7688e-01,  5.1521e-01,  5.5355e-01,  ...,  2.7564e-01,\n",
      "            1.8708e-01,  9.8508e-02],\n",
      "          ...,\n",
      "          [ 2.7410e-01,  2.4703e-01,  2.1997e-01,  ...,  2.3393e-01,\n",
      "            2.3181e-01,  2.2969e-01],\n",
      "          [ 2.7174e-01,  2.4734e-01,  2.2295e-01,  ...,  2.2948e-01,\n",
      "            2.2327e-01,  2.1706e-01],\n",
      "          [ 2.6938e-01,  2.4765e-01,  2.2593e-01,  ...,  2.2503e-01,\n",
      "            2.1473e-01,  2.0443e-01]]],\n",
      "\n",
      "\n",
      "        [[[ 4.1424e-01,  4.5633e-01,  4.9842e-01,  ...,  1.4636e-01,\n",
      "            1.2907e-01,  1.1179e-01],\n",
      "          [ 3.3730e-01,  3.7915e-01,  4.2100e-01,  ...,  1.4696e-01,\n",
      "            1.3559e-01,  1.2422e-01],\n",
      "          [ 2.6035e-01,  3.0197e-01,  3.4358e-01,  ...,  1.4756e-01,\n",
      "            1.4210e-01,  1.3665e-01],\n",
      "          ...,\n",
      "          [-1.4844e-01, -1.1322e-01, -7.7989e-02,  ..., -7.9900e-02,\n",
      "           -1.5259e-01, -2.2528e-01],\n",
      "          [-1.7669e-01, -1.4159e-01, -1.0650e-01,  ..., -1.1370e-01,\n",
      "           -1.8657e-01, -2.5944e-01],\n",
      "          [-2.0493e-01, -1.6997e-01, -1.3501e-01,  ..., -1.4749e-01,\n",
      "           -2.2054e-01, -2.9359e-01]],\n",
      "\n",
      "         [[-3.5797e-01, -3.7961e-01, -4.0126e-01,  ..., -1.1099e-01,\n",
      "           -6.5186e-02, -1.9384e-02],\n",
      "          [-3.8416e-01, -4.1628e-01, -4.4840e-01,  ..., -1.5788e-01,\n",
      "           -1.1296e-01, -6.8041e-02],\n",
      "          [-4.1035e-01, -4.5295e-01, -4.9555e-01,  ..., -2.0477e-01,\n",
      "           -1.6073e-01, -1.1670e-01],\n",
      "          ...,\n",
      "          [-5.9500e-01, -5.5008e-01, -5.0517e-01,  ...,  5.2300e-02,\n",
      "            6.6005e-02,  7.9711e-02],\n",
      "          [-5.6820e-01, -5.2365e-01, -4.7910e-01,  ...,  8.5123e-02,\n",
      "            9.8141e-02,  1.1116e-01],\n",
      "          [-5.4140e-01, -4.9722e-01, -4.5304e-01,  ...,  1.1795e-01,\n",
      "            1.3028e-01,  1.4261e-01]],\n",
      "\n",
      "         [[ 3.7166e-01,  3.6291e-01,  3.5416e-01,  ...,  2.5491e-01,\n",
      "            2.1863e-01,  1.8235e-01],\n",
      "          [ 4.1327e-01,  3.9583e-01,  3.7839e-01,  ...,  2.3279e-01,\n",
      "            2.0066e-01,  1.6853e-01],\n",
      "          [ 4.5487e-01,  4.2875e-01,  4.0263e-01,  ...,  2.1068e-01,\n",
      "            1.8270e-01,  1.5472e-01],\n",
      "          ...,\n",
      "          [ 3.9971e-01,  3.6977e-01,  3.3982e-01,  ...,  3.3338e-01,\n",
      "            2.7240e-01,  2.1143e-01],\n",
      "          [ 3.8697e-01,  3.6529e-01,  3.4362e-01,  ...,  3.4438e-01,\n",
      "            2.8702e-01,  2.2966e-01],\n",
      "          [ 3.7423e-01,  3.6082e-01,  3.4742e-01,  ...,  3.5538e-01,\n",
      "            3.0163e-01,  2.4789e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 3.4928e-01,  3.3157e-01,  3.1385e-01,  ...,  1.9251e-01,\n",
      "            2.5877e-01,  3.2504e-01],\n",
      "          [ 2.9760e-01,  2.9003e-01,  2.8246e-01,  ...,  2.3491e-01,\n",
      "            2.9792e-01,  3.6092e-01],\n",
      "          [ 2.4592e-01,  2.4849e-01,  2.5107e-01,  ...,  2.7732e-01,\n",
      "            3.3706e-01,  3.9680e-01],\n",
      "          ...,\n",
      "          [ 1.8879e-01,  1.6777e-01,  1.4674e-01,  ...,  2.5082e-01,\n",
      "            2.2860e-01,  2.0639e-01],\n",
      "          [ 2.8250e-01,  2.5514e-01,  2.2778e-01,  ...,  2.2534e-01,\n",
      "            1.9481e-01,  1.6429e-01],\n",
      "          [ 3.7622e-01,  3.4252e-01,  3.0881e-01,  ...,  1.9985e-01,\n",
      "            1.6102e-01,  1.2218e-01]],\n",
      "\n",
      "         [[ 5.4260e-01,  4.8226e-01,  4.2191e-01,  ...,  4.2235e-01,\n",
      "            4.0280e-01,  3.8324e-01],\n",
      "          [ 5.3297e-01,  4.7276e-01,  4.1255e-01,  ...,  3.9539e-01,\n",
      "            3.7384e-01,  3.5230e-01],\n",
      "          [ 5.2335e-01,  4.6326e-01,  4.0318e-01,  ...,  3.6843e-01,\n",
      "            3.4489e-01,  3.2136e-01],\n",
      "          ...,\n",
      "          [ 5.1034e-01,  4.5136e-01,  3.9238e-01,  ...,  2.2542e-01,\n",
      "            2.2356e-01,  2.2169e-01],\n",
      "          [ 5.1462e-01,  4.6454e-01,  4.1446e-01,  ...,  2.1227e-01,\n",
      "            2.0583e-01,  1.9939e-01],\n",
      "          [ 5.1889e-01,  4.7772e-01,  4.3654e-01,  ...,  1.9913e-01,\n",
      "            1.8811e-01,  1.7709e-01]],\n",
      "\n",
      "         [[ 2.2265e-01,  2.2519e-01,  2.2773e-01,  ...,  5.5418e-02,\n",
      "           -5.3822e-03, -6.6182e-02],\n",
      "          [ 3.0150e-01,  2.8993e-01,  2.7836e-01,  ...,  6.6910e-02,\n",
      "            7.9496e-03, -5.1010e-02],\n",
      "          [ 3.8035e-01,  3.5467e-01,  3.2899e-01,  ...,  7.8401e-02,\n",
      "            2.1281e-02, -3.5838e-02],\n",
      "          ...,\n",
      "          [ 3.1547e-01,  3.0073e-01,  2.8600e-01,  ...,  2.2172e-01,\n",
      "            1.9909e-01,  1.7646e-01],\n",
      "          [ 2.9467e-01,  2.7679e-01,  2.5891e-01,  ...,  2.5445e-01,\n",
      "            2.3782e-01,  2.2120e-01],\n",
      "          [ 2.7387e-01,  2.5285e-01,  2.3183e-01,  ...,  2.8717e-01,\n",
      "            2.7655e-01,  2.6593e-01]]]], grad_fn=<UpsampleBilinear2DBackward>))\n"
     ]
    }
   ],
   "source": [
    "batch_size = 2\n",
    "dummy_img = torch.rand(batch_size, 3, 475, 475)\n",
    "\n",
    "# 計算\n",
    "outputs = net(dummy_img)\n",
    "print(outputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# サイズは合っているはずなのになんでエラーになるのか？\n",
    "# 見ている箇所が間違っていた"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
