{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# セマンティックセグメンテーション用のDatasetとDataLoaderを作成"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Library "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path as osp\n",
    "from PIL import Image\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.utils.data as data\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## データのパスリストを作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# これは関数で良い\n",
    "def make_data_path(root_path):\n",
    "    \"\"\"\n",
    "    学習用、検証用の画像データとアノテーションデータへのファイルパスリストを作成する\n",
    "    \n",
    "    Parameters\n",
    "    -----------------\n",
    "    root_path : str\n",
    "        データディレクトリへのパス\n",
    "    \n",
    "    Returns\n",
    "    -----------------\n",
    "    train_img_list : list\n",
    "    train_anno_list : \n",
    "    val_img_list : list\n",
    "    val_anno_list : \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # 画像ファイルとアノテーションファイルへのパスのテンプレートを作成\n",
    "    # formatできる形式にしておく\n",
    "    # アノテーションデータはpng画像\n",
    "    imgpath_tmp = osp.join(root_path, 'JPEGImages', '%s.jpg')\n",
    "    annopath_tmp = osp.join(root_path, 'SegmentationClass', '%s.png')\n",
    "    \n",
    "    # 訓練と検証それぞれのファイルIDを取得\n",
    "    train_id_names = osp.join(root_path + 'ImageSets/Segmentation/train.txt')\n",
    "    val_id_names = osp.join(root_path + 'ImageSets/Segmentation/val.txt')\n",
    "    \n",
    "    # 訓練データの画像ファイルとアノテーションファイルへのパスのリストを作成\n",
    "    train_img_list = []\n",
    "    train_anno_list = []\n",
    "    for line in open(train_id_names):\n",
    "        file_id = line.strip()  # 空白と改行を削除\n",
    "        img_path = (imgpath_tmp % file_id)  # 雛形にfile_idを入れる   画像のパス\n",
    "        anno_path = (annopath_tmp % file_id)   # 雛形にfile_idを入れる　　アノテーションのパス\n",
    "        train_img_list.append(img_path)\n",
    "        train_anno_list.append(anno_path)      \n",
    "    \n",
    "    # 検証データの画像ファイルとアノテーションファイルへのパスのリストを作成\n",
    "    val_img_list = []\n",
    "    val_anno_list = []\n",
    "    for line in open(val_id_names):\n",
    "        file_id = line.strip()\n",
    "        img_path = (imgpath_tmp % file_id)\n",
    "        anno_path = (annopath_tmp % file_id)\n",
    "        val_img_list.append(img_path)\n",
    "        val_anno_list.append(anno_path)\n",
    "        \n",
    "    return train_img_list, train_anno_list, val_img_list, val_anno_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../2_objectdetection/data/VOCdevkit/VOC2012/JPEGImages/2007_000032.jpg\n",
      "../2_objectdetection/data/VOCdevkit/VOC2012/SegmentationClass/2007_000032.png\n"
     ]
    }
   ],
   "source": [
    "# make_data_pathの動作確認\n",
    "root_path = '../2_objectdetection/data/VOCdevkit/VOC2012/'\n",
    "train_img_list, train_anno_list, val_img_list, val_anno_list = make_data_path(root_path)\n",
    "print(train_img_list[0])\n",
    "print(train_anno_list[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataTransformを作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.data_augumentation import Compose, Scale, RandomRotation, RandomMirror, Resize, Normalize_Tensor\n",
    "\n",
    "class DataTransform():\n",
    "    \"\"\"\n",
    "    画像とアノテーションの前処理クラス\n",
    "    訓練時と検証時で異なる動作をする\n",
    "    訓練時はaugmentationする \n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, color_mean, color_std):\n",
    "        # 初期化の時点でtrainとvalのComposeの辞書作っておく\n",
    "        # その際に input_size, color_mean, color_stdが必要になってくる\n",
    "        self.data_transform = {\n",
    "            'train' : Compose([\n",
    "                Scale(scale=[0.5, 1.5]),  # 画像の拡大縮小\n",
    "                RandomRotation(angle=[-10, 10]),  # 回転\n",
    "                RandomMirror(),  # 1/2の確率で左右反転\n",
    "                Resize(input_size),\n",
    "                Normalize_Tensor(color_mean, color_std)  # 色情報の標準化とテンソル化\n",
    "            ]),\n",
    "            'val' : Compose([\n",
    "                Resize(input_size),\n",
    "                Normalize_Tensor(color_mean, color_std)\n",
    "            ])\n",
    "        }\n",
    "    \n",
    "    def __call__(self, phase, img, anno_img):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        -----------------\n",
    "        phase : str\n",
    "            'train' or 'val'\n",
    "            \n",
    "        \"\"\"\n",
    "        return self.data_transform[phase](img, anno_img)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasetの作成\n",
    "opencvではなくPillowを使う"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VOCDataset(data.Dataset):\n",
    "    \"\"\"\n",
    "    VOC2012のDatasetを作成するクラス\n",
    "    PytorchのDatasetクラスを継承\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, img_list, anno_list, phase, transform):\n",
    "        self.img_list = img_list\n",
    "        self.anno_list = anno_list\n",
    "        self.phase = phase\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        画像の枚数を返す\n",
    "        \"\"\"\n",
    "        return len(self.img_list)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        前処理をした画像のTensor形式のデータとアノテーションを取得\n",
    "        \"\"\"\n",
    "        img, anno_img = self.pull_item(index)\n",
    "        return img, anno_img\n",
    "    \n",
    "    def pull_item(self, index):\n",
    "        \"\"\"\n",
    "        画像のTensor形式のデータ、アノテーションを取得する\n",
    "        \"\"\"\n",
    "        # 画像読み込み\n",
    "        img_file_path = self.img_list[index]\n",
    "        img = Image.open(img_file_path)\n",
    "        \n",
    "        # アノテーション画像読み込み\n",
    "        anno_file_path = self.anno_list[index]    # self.img_listにしててshapeがおかしかったのを修正\n",
    "        anno_img = Image.open(anno_file_path)\n",
    "        \n",
    "        # 前処理の実施\n",
    "        # ここでDataTransformの__call__が実行される\n",
    "        img, anno_img = self.transform(self.phase, img, anno_img)\n",
    "        \n",
    "        return img, anno_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 475, 475])\n",
      "torch.Size([475, 475])\n",
      "(tensor([[[-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
      "         [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
      "         [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
      "         ...,\n",
      "         [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
      "         [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
      "         [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179]],\n",
      "\n",
      "        [[-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
      "         [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
      "         [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
      "         ...,\n",
      "         [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
      "         [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
      "         [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357]],\n",
      "\n",
      "        [[-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
      "         [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
      "         [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
      "         ...,\n",
      "         [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
      "         [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
      "         [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044]]]), tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], dtype=torch.uint8))\n"
     ]
    }
   ],
   "source": [
    "# Datasetの動作確認\n",
    "\n",
    "color_mean = (0.485, 0.456, 0.406)\n",
    "color_std = (0.229, 0.224, 0.225)\n",
    "input_size = 475\n",
    "\n",
    "# Dataset作成\n",
    "train_dataset = VOCDataset(train_img_list, train_anno_list,phase='train', transform=DataTransform(input_size, color_mean, color_std))\n",
    "val_dataset = VOCDataset(val_img_list, val_anno_list,phase='val', transform=DataTransform(input_size, color_mean, color_std))\n",
    "\n",
    "# データ取り出し例\n",
    "print(train_dataset.__getitem__(0)[0].shape)  # img\n",
    "print(train_dataset.__getitem__(0)[1].shape)  # anno_img\n",
    "print(train_dataset.__getitem__(0))                   # Sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 475, 475])\n",
      "torch.Size([475, 475])\n",
      "(tensor([[[-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
      "         [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
      "         [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
      "         ...,\n",
      "         [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
      "         [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
      "         [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179]],\n",
      "\n",
      "        [[-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
      "         [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
      "         [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
      "         ...,\n",
      "         [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
      "         [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
      "         [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357]],\n",
      "\n",
      "        [[-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
      "         [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
      "         [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
      "         ...,\n",
      "         [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
      "         [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
      "         [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044]]]), tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], dtype=torch.uint8))\n"
     ]
    }
   ],
   "source": [
    "# データ取り出し例\n",
    "# __getitem__は[添字]でアクセスできるようにするための特殊メソッド\n",
    "print(train_dataset[0][0].shape)  # img\n",
    "print(train_dataset[0][1].shape)  # anno_img\n",
    "print(train_dataset[0])                   # Sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataLoaderの作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "train_dataloader = data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_dataloader = data.DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# 辞書で管理\n",
    "dataloaders_dict = {'train' : train_dataloader, 'val' : val_dataloader}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 3, 475, 475])\n",
      "torch.Size([8, 475, 475])\n"
     ]
    }
   ],
   "source": [
    "# DataLoaderの動作確認\n",
    "batch_iterator = iter(dataloaders_dict['val'])  # イテレータに変換\n",
    "imgs, anno_imgs = next(batch_iterator)\n",
    "print(imgs.shape)\n",
    "print(anno_imgs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
