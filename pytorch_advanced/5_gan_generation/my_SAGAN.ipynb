{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Self Attention GANの実装\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# パッケージのimport\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.utils.data as data\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "np.random.seed(123)\n",
    "random.seed(123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## self attentionとは\n",
    "DCGANのGeneratorではDeconvで拡大する際どうしても局所情報の拡大になってしまい、大域的な情報が含まれない<br>\n",
    "大域的な情報を含んだ拡大を実現するのがself attentionという技術"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1x1 Convolutions (pointwise convolution)とは\n",
    "出力チャネルが1だけ -> 入力xをチャネルごとに線形和をとったもの<br>\n",
    "出力チャネルが複数 -> 異なる係数で（重み）でチャネルごとに線形和をとったものを出力チャネル分だけ用意する<br>\n",
    "\n",
    "次元圧縮とも言える<br>\n",
    "情報が欠落しにくい線形和の係数を学習してくれる\n",
    "\n",
    "次元が圧縮され、計算量が少なくなることを活かして、エッジ端末で使われるmobile netでも使用される"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spectral Normalizationとは\n",
    "ネットワークの重みの正規化<br>\n",
    "GANがうまく学習するにはDiscriminatorがリプシッツ連続星を保つ必要がある→<br>\n",
    "リプシッツ連続をDiscriminatorのネットワークが有するにはどうすればいいか→<br>\n",
    "Spectral Normalizationで重みを正規化する"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## self attentionモジュールの実装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Self_Attention(nn.Module):\n",
    "    \"\"\"\n",
    "    self attentionのlayer\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, in_dim):\n",
    "        \"\"\"\n",
    "        query:\n",
    "            元の入力xの転置に対応するもの\n",
    "        key:\n",
    "            元の入力xに対応するもの\n",
    "        value:\n",
    "            attention mapと掛け算する対象\n",
    "        \"\"\"\n",
    "        super(Self_Attention, self).__init__()\n",
    "        \n",
    "        # pointwise convolutionを用意\n",
    "        # pointwiseなのでkernel_size=1\n",
    "        self.query_conv = nn.Conv2d(\n",
    "            in_channels=in_dim, out_channels=in_dim//8, kernel_size=1)\n",
    "        self.key_conv = nn.Conv2d(\n",
    "            in_channels=in_dim, out_channels=in_dim//8, kernel_size=1)\n",
    "        self.value_conv = nn.Conv2d(\n",
    "            in_channels=in_dim, out_channels=in_dim, kernel_size=1)\n",
    "        \n",
    "        # attention map作成時の規格化のソフトマックス\n",
    "        self.softmax = nn.Softmax(dim=-2)\n",
    "        \n",
    "        # もとの入力ｘとself attention map(o)を加算するときの係数\n",
    "        # 最初はgammma=0で学習させていく\n",
    "        # nn.Parameter : pytocrhで学習可能な変数を作成する命令\n",
    "        # これはインスタンス変数で保持しておく必要があるのかな？\n",
    "        self.gamma = nn.Parameter(torch.zeros(1))  \n",
    "        \n",
    "    def forward(self, x):\n",
    "        # 入力変数  B, C, W, H\n",
    "        X = x\n",
    "        \n",
    "        # 畳み込み後にサイズを変更 B, C', W, H -> B, C', N\n",
    "        proj_query = self.query_conv(X).view(\n",
    "            X.shape[0], -1, X.shape[2]*X.shape[3])  # channelのサイズが変わるので-1\n",
    "        proj_query = proj_query.permute(0, 2, 1)  # 転置　　B, N, C'\n",
    "        proj_key = self.key_conv(X).view(\n",
    "            X.shape[0], -1, X.shape[2]*X.shape[3])  # channelのサイズが変わるので-1\n",
    "                                                                                   # B, C', N\n",
    "\n",
    "        # 掛け算\n",
    "        S = torch.bmm(proj_query, proj_key)  # bmm : バッチごとの掛け算\n",
    "        \n",
    "        # 規格化\n",
    "        attention_map_T = self.softmax(S)  # 行i方向の和を1にするsoftmax\n",
    "        attention_map = attention_map_T.permute(0, 2, 1)  # 転置\n",
    "        \n",
    "        # self attentino mapを計算\n",
    "        proj_value = self.value_conv(X).view(\n",
    "            X.shape[0], -1, X.shape[2]*X.shape[3])  # B, C, N\n",
    "        o = torch.bmm(proj_value, attention_map.permute(0, 2, 1))  # bmm : バッチごとの掛け算\n",
    "        # attention mapは転置して掛け算\n",
    "        \n",
    "        # self attentino mapであるoのテンソルサイズをXに揃えて出力にする\n",
    "        o = o.view(X.shape[0], X.shape[1], X.shape[2], X.shape[3])\n",
    "        out = x + self.gamma*o\n",
    "        \n",
    "        return out, attention_map  # attention_mapはattentionの強さを可視化するために使用"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generatorの実装\n",
    "DCGANからの変更点\n",
    "- Deconv層にSpectral Normalizationを追加する\n",
    "- self attentionモジュールを追加する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, z_dim=20, image_size=64):\n",
    "        super(Generator, self).__init__()\n",
    "        \n",
    "        self.layer1= nn.Sequential(\n",
    "            # Spectral Normalizationを追加\n",
    "            nn.utils.spectral_norm(\n",
    "                nn.ConvTranspose2d(z_dim, image_size*8, kernel_size=4, stride=1)),\n",
    "            nn.BatchNorm2d(image_size*8),\n",
    "            nn.ReLU(inplace=True))\n",
    "        \n",
    "        self.layer2 = nn.Sequential(\n",
    "            # Spectral Normalizationを追加\n",
    "            nn.utils.spectral_norm(\n",
    "                nn.ConvTranspose2d(image_size*8, image_size*4, kernel_size=4, stride=2, padding=1)),\n",
    "            nn.BatchNorm2d(image_size*4),\n",
    "            nn.ReLU(inplace=True))\n",
    "        \n",
    "        self.layer3 = nn.Sequential(\n",
    "            # Spectral Normalizationを追加\n",
    "            nn.utils.spectral_norm(\n",
    "                nn.ConvTranspose2d(image_size*4, image_size*2, kernel_size=4, stride=2, padding=1)),\n",
    "            nn.BatchNorm2d(image_size*2),\n",
    "            nn.ReLU(inplace=True))\n",
    "        \n",
    "        # self attention層を追加\n",
    "        self.self_attention1 = Self_Attention(in_dim=image_size*2)\n",
    "        \n",
    "        self.layer4 = nn.Sequential(\n",
    "            # Spectral Normalizationを追加\n",
    "            nn.utils.spectral_norm(\n",
    "                nn.ConvTranspose2d(image_size*2, image_size, kernel_size=4, stride=2, padding=1)),\n",
    "            nn.BatchNorm2d(image_size),\n",
    "            nn.ReLU(inplace=True))\n",
    "        \n",
    "        # self attention層を追加\n",
    "        self.self_attention2 = Self_Attention(in_dim=image_size)\n",
    "        \n",
    "        self.last = nn.Sequential(\n",
    "            # Spectral Normalizationはしない\n",
    "            nn.ConvTranspose2d(image_size, 1, kernel_size=4, stride=2, padding=1),\n",
    "            nn.Tanh())  # -1~1の出力になる\n",
    "        \n",
    "        \n",
    "    def forward(self, z):\n",
    "        out = self.layer1(z)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out, attention_map1 = self.self_attention1(out)\n",
    "        out = self.layer4(out)\n",
    "        out, attention_map2 = self.self_attention2(out)\n",
    "        out = self.last(out)\n",
    "        \n",
    "        return out, attention_map1, attention_map2  #  outと2つのattention_mapを返す\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 動作確認\n",
    "# どこかにミスがある前提で動作確認するほうがいい\n",
    "# 修正するための動作確認だから\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# 偽画像の生成\n",
    "G = Generator(z_dim=20, image_size=64)\n",
    "input_z = torch.randn(1, 20)\n",
    "input_z = input_z.view(input_z.size(0), input_z.size(1), 1, 1)\n",
    "fake_images, attention_map1, attention_map2 = G(input_z)\n",
    "\n",
    "# 偽画像の表示\n",
    "image_transformed = fake_images[0][0].detach().numpy()  # 64x64\n",
    "plt.imshow(image_transformed, 'gray')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discriminator\n",
    "DCGANからの変更点(Generatorと一緒)\n",
    "- Deconv層にSpectral Normalizationを追加する\n",
    "- self attentionモジュールを追加する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, z_dim, image_size=64):\n",
    "        super(Discriminator, self).__init__()\n",
    "        \n",
    "        # layerを用意しておく\n",
    "        self.layer1 = nn.Sequential(\n",
    "            # Spectral normalization\n",
    "            nn.utils.spectral_norm(nn.Conv2d(1, image_size, kernel_size=4, stride=2, padding=1)),\n",
    "            nn.LeakyReLU(0.1, inplace=True))\n",
    "        \n",
    "        self.layer2 = nn.Sequential(\n",
    "            # Spectral normalization\n",
    "            nn.utils.spectral_norm(nn.Conv2d(image_size, image_size*2, kernel_size=4, stride=2, padding=1)),\n",
    "            nn.LeakyReLU(0.1, inplace=True))\n",
    "        \n",
    "        self.layer3 = nn.Sequential(\n",
    "            # Spectral normalization\n",
    "            nn.utils.spectral_norm(nn.Conv2d(image_size*2, image_size*4, kernel_size=4, stride=2, padding=1)),\n",
    "            nn.LeakyReLU(0.1, inplace=True))\n",
    "        \n",
    "        self.self_attention1 = Self_Attention(in_dim=image_size*4)\n",
    "        \n",
    "        self.layer4 = nn.Sequential(\n",
    "            # Spectral normalization\n",
    "            nn.utils.spectral_norm(nn.Conv2d(image_size*4, image_size*8, kernel_size=4, stride=2, padding=1)),\n",
    "            nn.LeakyReLU(0.1, inplace=True))\n",
    "        \n",
    "        self.self_attention2 = Self_Attention(in_dim=image_size*8)\n",
    "        \n",
    "        self.last = nn.Conv2d(image_size*8, 1, kernel_size=4, stride=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out, attention_map1 = self.self_attention1(out)\n",
    "        out = self.layer4(out)\n",
    "        out, attention_map2 = self.self_attention2(out)\n",
    "        out = self.last(out)\n",
    "        \n",
    "        return out, attention_map1, attention_map2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[0.4955]]]], grad_fn=<SigmoidBackward>)\n"
     ]
    }
   ],
   "source": [
    "# 動作確認\n",
    "D = Discriminator(z_dim=20, image_size=64)\n",
    "\n",
    "#　偽画像生成して判定\n",
    "input_z = torch.randn(1, 20)\n",
    "input_z = input_z.view(input_z.size(0), input_z.size(1), 1, 1)\n",
    "fake_images, _, _ = G(input_z)\n",
    "d_out, attention_map1, attention_map2 = D(fake_images)\n",
    "\n",
    "print(nn.Sigmoid()(d_out))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataLoaderの作成\n",
    "変更点はなし"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_datapath_list():\n",
    "    \"\"\"\n",
    "    学習の画像データへのファイルパスリストを作成\n",
    "    \"\"\"\n",
    "    \n",
    "    train_img_list = []\n",
    "    for img_idx in range(200):\n",
    "        img_path = './data/img_78/img_7_' + str(img_idx) + '.jpg'\n",
    "        train_img_list.append(img_path)\n",
    "        \n",
    "        img_path = './data/img_78/img_8_' + str(img_idx) + '.jpg'\n",
    "        train_img_list.append(img_path)\n",
    "        \n",
    "    return train_img_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageTransform():\n",
    "    \"\"\"画像の前処理クラス\"\"\"\n",
    "    \n",
    "    def __init__(self, mean, std):\n",
    "        self.data_transform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean, std)\n",
    "        ])\n",
    "        \n",
    "    def __call__(self, img):\n",
    "        return self.data_transform(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAN_Img_Dataset(data.Dataset):\n",
    "    '''\n",
    "    画像のDatasetクラス\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, file_list, transform):\n",
    "        self.file_list = file_list\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.file_list)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        画像のTensor形式のデータを取得\n",
    "        \"\"\"\n",
    "        img_path = self.file_list[index]\n",
    "        img = Image.open(img_path)  # 高さ、幅、色【白黒】\n",
    "        \n",
    "        # 画像の前処理\n",
    "        img_transformed = self.transform(img)\n",
    "        \n",
    "        return img_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 1, 64, 64])\n"
     ]
    }
   ],
   "source": [
    "# DataLoaderの作成と動作確認\n",
    "\n",
    "# ファイルリストの作成\n",
    "train_img_list = make_datapath_list()\n",
    "\n",
    "# Datasetを作成\n",
    "mean  = (0.5,)  # カンマがないとエラーになる\n",
    "std = (0.5,)\n",
    "train_dataset = GAN_Img_Dataset(file_list=train_img_list, transform=ImageTransform(mean, std))\n",
    "\n",
    "# DataLoaderの作成\n",
    "batch_size = 64\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# 動作の確認\n",
    "batch_iterator = iter(train_dataloader)  # イテレータに変換\n",
    "images = next(batch_iterator)\n",
    "print(images.size())  # torch.Size([])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 学習\n",
    "SAGANでは損失関数に hinge version of the adversarial lossを使う<br>\n",
    "logなし、平均を取る<br>\n",
    "Discriminatorの方でReLU使ってる<br>\n",
    "\n",
    "GANの損失関数は多数提案されているが、それらは経験的にうまくいくからと言う理由で使用されていることが多い"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# モデルを学習させる関数を作成\n",
    "\n",
    "def train_model(G, D, dataloader, num_epochs):\n",
    "    # GPUが使えるかを確認\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(\"使用デバイス：\", device)\n",
    "    \n",
    "    # 最適化手法の設定\n",
    "    g_lr, d_lr = 0.0001, 0.0004\n",
    "    beta1, beta2 = 0.0, 0.9\n",
    "    g_optimizer = torch.optim.Adam(G.parameters(), g_lr, [beta1, beta2])\n",
    "    d_optimizer = torch.optim.Adam(D.parameters(), d_lr, [beta1, beta2])\n",
    "    \n",
    "    z_dim = 20\n",
    "    mini_batch_size = 64  # この一文はなんのためにあるの？\n",
    "    \n",
    "    # ハードに送って\n",
    "    G.to(device)\n",
    "    D.to(device)\n",
    "    \n",
    "    # 訓練モードに設定\n",
    "    G.train()\n",
    "    D.train()\n",
    "\n",
    "    torch.backends.cudnn.bencmark = True\n",
    "    \n",
    "    num_train_imgs = len(dataloader.dataset)\n",
    "    batch_size = dataloader.batch_size\n",
    "    \n",
    "    # イテレーションカウンタのセット\n",
    "    iteration = 1\n",
    "    logs = []\n",
    "    \n",
    "    # epochのループ\n",
    "    for epoch in range(num_epochs):\n",
    "        # 開始時刻\n",
    "        t_epoch_start = time.time()\n",
    "        epoch_loss_g = 0.0\n",
    "        epoch_loss_d = 0.0\n",
    "        \n",
    "        print('------------')\n",
    "        print('Epoch : {}/{}'.format(epoch, num_epochs))\n",
    "        print('------------')\n",
    "        print('(train)')\n",
    "        \n",
    "        # データローダからminibatcずつ取り出すループ\n",
    "        for images in dataloader:  # ここはiteratorにしなくても取り出してくれる\n",
    "            # --------------------\n",
    "            # 1. Discriminatorの学習\n",
    "            # --------------------\n",
    "            if images.size()[0] == 1:\n",
    "                continue\n",
    "            \n",
    "            # modelは○.to(device)だけでいいけど\n",
    "            # データは ○ = ○.to(device)で再度代入する必要がある\n",
    "            images = images.to(device)\n",
    "            \n",
    "            # epochの最後のイテレーションはミニバッチサイズの数が少なくなる\n",
    "            mini_batch_size = images.size()[0]\n",
    "            \n",
    "            # 真の画像を判定\n",
    "            d_out_real, _, _ = D(images)\n",
    "            \n",
    "            # 偽の画像を生成して判定\n",
    "            input_z = torch.randn(mini_batch_size, z_dim).to(device)\n",
    "            input_z = input_z.view(input_z.size(0), input_z.size(1), 1, 1)\n",
    "            fake_images, _, _ = G(input_z)\n",
    "            d_out_fake, _, _ = D(fake_images)\n",
    "            # D, Gどちらもattention mapが生成される\n",
    "            # Dのattention mapってどんなもの？\n",
    "            \n",
    "            # 誤差計算　hinge version of the adversarial lossに変更\n",
    "            d_loss_real = torch.nn.ReLU()(1.0-d_out_real).mean()\n",
    "            # d_out_realが１以上の場合誤差0になる\n",
    "            \n",
    "            d_loss_fake = torch.nn.ReLU()(1.0+d_out_fake).mean()\n",
    "            # d_loss_fakeが−１以下なら誤差0になる\n",
    "            \n",
    "            d_loss = d_loss_real + d_loss_fake\n",
    "            \n",
    "            # backpropagation\n",
    "            g_optimizer.zero_grad()\n",
    "            d_optimizer.zero_grad()\n",
    "            \n",
    "            d_loss.backward()\n",
    "            d_optimizer.step()\n",
    "            \n",
    "            # --------------------\n",
    "            # 2. Generatorの学習\n",
    "            # --------------------\n",
    "            # 偽の画像を生成して判定\n",
    "            input_z = torch.randn(mini_batch_size, z_dim).to(device)\n",
    "            input_z = input_z.view(input_z.size(0), input_z.size(1), 1, 1)\n",
    "            fake_images, _, _ = G(input_z)\n",
    "            d_out_fake, _, _ = D(fake_images)\n",
    "            \n",
    "            # 誤差計算　hinge version of the adversarial lossに変更\n",
    "            g_loss = - d_out_fake.mean()  # マイナス忘れ注意\n",
    "            \n",
    "            # backpropagation\n",
    "            g_optimizer.zero_grad()\n",
    "            d_optimizer.zero_grad()\n",
    "            \n",
    "            g_loss.backward()\n",
    "            g_optimizer.step()\n",
    "            \n",
    "            # --------------------\n",
    "            # 3. 記録\n",
    "            # --------------------\n",
    "            epoch_loss_d += d_loss.item()  # .item()忘れがち\n",
    "            epoch_loss_g += g_loss.item()\n",
    "            iteration += 1 \n",
    "            \n",
    "        # epochごとのlossと計算時間\n",
    "        t_epoch_finish = time.time()\n",
    "        print('------------')\n",
    "        print('epoch {} || Epoch_D_Loss :{:.4f} || Epoch_G_Loss :{:.4f}'.format(\n",
    "            epoch, epoch_loss_d/batch_size, epoch_loss_g/batch_size))\n",
    "        print('timer : {:.4f} sec.'.format(t_epoch_finish - t_epoch_start))\n",
    "        t_epoch_start = time.time()\n",
    "        \n",
    "    return G, D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ネットワークの初期化完了\n"
     ]
    }
   ],
   "source": [
    "# ネットワークの初期化\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__  # オブジェクトのクラス名を取得\n",
    "    if classname.find('Conv') != -1:  # 文字列中の任意の文字列の位置を取得　含まれていなければ-1\n",
    "        # Conv2dとConvTranspose2dの重みの初期化\n",
    "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "        nn.init.constant_(m.bias.data, 0)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        # BatchNorm2dの重みの初期化\n",
    "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        nn.init.constant_(m.bias.data, 0)\n",
    "        \n",
    "# 重みの初期化の実施\n",
    "G.apply(weights_init)\n",
    "D.apply(weights_init)\n",
    "\n",
    "print('ネットワークの初期化完了')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "使用デバイス： cuda:0\n",
      "------------\n",
      "Epoch : 0/300\n",
      "------------\n",
      "(train)\n",
      "------------\n",
      "epoch 0 || Epoch_D_Loss :0.1574 || Epoch_G_Loss :0.0195\n",
      "timer : 3.0071 sec.\n",
      "------------\n",
      "Epoch : 1/300\n",
      "------------\n",
      "(train)\n",
      "------------\n",
      "epoch 1 || Epoch_D_Loss :0.1736 || Epoch_G_Loss :0.1687\n",
      "timer : 2.7432 sec.\n",
      "------------\n",
      "Epoch : 2/300\n",
      "------------\n",
      "(train)\n",
      "------------\n",
      "epoch 2 || Epoch_D_Loss :0.0372 || Epoch_G_Loss :0.1106\n",
      "timer : 2.7506 sec.\n",
      "------------\n",
      "Epoch : 3/300\n",
      "------------\n",
      "(train)\n",
      "------------\n",
      "epoch 3 || Epoch_D_Loss :0.0037 || Epoch_G_Loss :0.1379\n",
      "timer : 2.7198 sec.\n",
      "------------\n",
      "Epoch : 4/300\n",
      "------------\n",
      "(train)\n",
      "------------\n",
      "epoch 4 || Epoch_D_Loss :0.0887 || Epoch_G_Loss :0.1092\n",
      "timer : 2.7510 sec.\n",
      "------------\n",
      "Epoch : 5/300\n",
      "------------\n",
      "(train)\n",
      "------------\n",
      "epoch 5 || Epoch_D_Loss :0.0053 || Epoch_G_Loss :0.1842\n",
      "timer : 2.7319 sec.\n",
      "------------\n",
      "Epoch : 6/300\n",
      "------------\n",
      "(train)\n",
      "------------\n",
      "epoch 6 || Epoch_D_Loss :0.1005 || Epoch_G_Loss :0.0997\n",
      "timer : 2.7446 sec.\n",
      "------------\n",
      "Epoch : 7/300\n",
      "------------\n",
      "(train)\n",
      "------------\n",
      "epoch 7 || Epoch_D_Loss :0.0600 || Epoch_G_Loss :0.1241\n",
      "timer : 2.7476 sec.\n",
      "------------\n",
      "Epoch : 8/300\n",
      "------------\n",
      "(train)\n",
      "------------\n",
      "epoch 8 || Epoch_D_Loss :0.0029 || Epoch_G_Loss :0.1671\n",
      "timer : 2.7316 sec.\n",
      "------------\n",
      "Epoch : 9/300\n",
      "------------\n",
      "(train)\n",
      "------------\n",
      "epoch 9 || Epoch_D_Loss :0.0851 || Epoch_G_Loss :0.1099\n",
      "timer : 2.7310 sec.\n",
      "------------\n",
      "Epoch : 10/300\n",
      "------------\n",
      "(train)\n",
      "------------\n",
      "epoch 10 || Epoch_D_Loss :0.1032 || Epoch_G_Loss :0.0690\n",
      "timer : 2.7773 sec.\n",
      "------------\n",
      "Epoch : 11/300\n",
      "------------\n",
      "(train)\n",
      "------------\n",
      "epoch 11 || Epoch_D_Loss :0.0756 || Epoch_G_Loss :0.1030\n",
      "timer : 2.7747 sec.\n",
      "------------\n",
      "Epoch : 12/300\n",
      "------------\n",
      "(train)\n",
      "------------\n",
      "epoch 12 || Epoch_D_Loss :0.0878 || Epoch_G_Loss :0.0926\n",
      "timer : 2.7560 sec.\n",
      "------------\n",
      "Epoch : 13/300\n",
      "------------\n",
      "(train)\n",
      "------------\n",
      "epoch 13 || Epoch_D_Loss :0.0573 || Epoch_G_Loss :0.1302\n",
      "timer : 2.7505 sec.\n",
      "------------\n",
      "Epoch : 14/300\n",
      "------------\n",
      "(train)\n",
      "------------\n",
      "epoch 14 || Epoch_D_Loss :0.1193 || Epoch_G_Loss :0.1232\n",
      "timer : 2.7695 sec.\n",
      "------------\n",
      "Epoch : 15/300\n",
      "------------\n",
      "(train)\n",
      "------------\n",
      "epoch 15 || Epoch_D_Loss :0.0893 || Epoch_G_Loss :0.0821\n",
      "timer : 2.7522 sec.\n",
      "------------\n",
      "Epoch : 16/300\n",
      "------------\n",
      "(train)\n",
      "------------\n",
      "epoch 16 || Epoch_D_Loss :0.0864 || Epoch_G_Loss :0.1242\n",
      "timer : 2.7654 sec.\n",
      "------------\n",
      "Epoch : 17/300\n",
      "------------\n",
      "(train)\n",
      "------------\n",
      "epoch 17 || Epoch_D_Loss :0.1098 || Epoch_G_Loss :0.0749\n",
      "timer : 2.7655 sec.\n",
      "------------\n",
      "Epoch : 18/300\n",
      "------------\n",
      "(train)\n",
      "------------\n",
      "epoch 18 || Epoch_D_Loss :0.1042 || Epoch_G_Loss :0.0921\n",
      "timer : 2.7666 sec.\n",
      "------------\n",
      "Epoch : 19/300\n",
      "------------\n",
      "(train)\n",
      "------------\n",
      "epoch 19 || Epoch_D_Loss :0.1032 || Epoch_G_Loss :0.0785\n",
      "timer : 2.7800 sec.\n",
      "------------\n",
      "Epoch : 20/300\n",
      "------------\n",
      "(train)\n",
      "------------\n",
      "epoch 20 || Epoch_D_Loss :0.1286 || Epoch_G_Loss :0.0796\n",
      "timer : 2.7769 sec.\n",
      "------------\n",
      "Epoch : 21/300\n",
      "------------\n",
      "(train)\n",
      "------------\n",
      "epoch 21 || Epoch_D_Loss :0.1190 || Epoch_G_Loss :0.0490\n",
      "timer : 2.7784 sec.\n",
      "------------\n",
      "Epoch : 22/300\n",
      "------------\n",
      "(train)\n",
      "------------\n",
      "epoch 22 || Epoch_D_Loss :0.1400 || Epoch_G_Loss :0.0889\n",
      "timer : 2.7814 sec.\n",
      "------------\n",
      "Epoch : 23/300\n",
      "------------\n",
      "(train)\n",
      "------------\n",
      "epoch 23 || Epoch_D_Loss :0.1263 || Epoch_G_Loss :0.0743\n",
      "timer : 2.7941 sec.\n",
      "------------\n",
      "Epoch : 24/300\n",
      "------------\n",
      "(train)\n",
      "------------\n",
      "epoch 24 || Epoch_D_Loss :0.1469 || Epoch_G_Loss :0.0581\n",
      "timer : 2.8040 sec.\n",
      "------------\n",
      "Epoch : 25/300\n",
      "------------\n",
      "(train)\n",
      "------------\n",
      "epoch 25 || Epoch_D_Loss :0.1590 || Epoch_G_Loss :0.0234\n",
      "timer : 2.8008 sec.\n",
      "------------\n",
      "Epoch : 26/300\n",
      "------------\n",
      "(train)\n",
      "------------\n",
      "epoch 26 || Epoch_D_Loss :0.1531 || Epoch_G_Loss :0.0470\n",
      "timer : 2.7866 sec.\n",
      "------------\n",
      "Epoch : 27/300\n",
      "------------\n",
      "(train)\n",
      "------------\n",
      "epoch 27 || Epoch_D_Loss :0.1717 || Epoch_G_Loss :0.0247\n",
      "timer : 2.7960 sec.\n",
      "------------\n",
      "Epoch : 28/300\n",
      "------------\n",
      "(train)\n",
      "------------\n",
      "epoch 28 || Epoch_D_Loss :0.1705 || Epoch_G_Loss :0.0543\n",
      "timer : 2.7969 sec.\n",
      "------------\n",
      "Epoch : 29/300\n",
      "------------\n",
      "(train)\n",
      "------------\n",
      "epoch 29 || Epoch_D_Loss :0.1405 || Epoch_G_Loss :0.0709\n",
      "timer : 2.8176 sec.\n",
      "------------\n",
      "Epoch : 30/300\n",
      "------------\n",
      "(train)\n",
      "------------\n",
      "epoch 30 || Epoch_D_Loss :0.1701 || Epoch_G_Loss :0.0271\n",
      "timer : 2.8175 sec.\n",
      "------------\n",
      "Epoch : 31/300\n",
      "------------\n",
      "(train)\n",
      "------------\n",
      "epoch 31 || Epoch_D_Loss :0.1604 || Epoch_G_Loss :0.1012\n",
      "timer : 2.8098 sec.\n",
      "------------\n",
      "Epoch : 32/300\n",
      "------------\n",
      "(train)\n",
      "------------\n",
      "epoch 32 || Epoch_D_Loss :0.1963 || Epoch_G_Loss :0.0488\n",
      "timer : 2.7969 sec.\n",
      "------------\n",
      "Epoch : 33/300\n",
      "------------\n",
      "(train)\n",
      "------------\n",
      "epoch 33 || Epoch_D_Loss :0.1674 || Epoch_G_Loss :0.0366\n",
      "timer : 2.8117 sec.\n",
      "------------\n",
      "Epoch : 34/300\n",
      "------------\n",
      "(train)\n",
      "------------\n",
      "epoch 34 || Epoch_D_Loss :0.1849 || Epoch_G_Loss :0.0223\n",
      "timer : 2.8009 sec.\n",
      "------------\n",
      "Epoch : 35/300\n",
      "------------\n",
      "(train)\n",
      "------------\n",
      "epoch 35 || Epoch_D_Loss :0.1706 || Epoch_G_Loss :0.0321\n",
      "timer : 2.8054 sec.\n",
      "------------\n",
      "Epoch : 36/300\n",
      "------------\n",
      "(train)\n",
      "------------\n",
      "epoch 36 || Epoch_D_Loss :0.1566 || Epoch_G_Loss :0.0881\n",
      "timer : 2.8149 sec.\n",
      "------------\n",
      "Epoch : 37/300\n",
      "------------\n",
      "(train)\n",
      "------------\n",
      "epoch 37 || Epoch_D_Loss :0.1946 || Epoch_G_Loss :0.0087\n",
      "timer : 2.8116 sec.\n",
      "------------\n",
      "Epoch : 38/300\n",
      "------------\n",
      "(train)\n",
      "------------\n",
      "epoch 38 || Epoch_D_Loss :0.1652 || Epoch_G_Loss :0.0461\n",
      "timer : 2.8042 sec.\n",
      "------------\n",
      "Epoch : 39/300\n",
      "------------\n",
      "(train)\n",
      "------------\n",
      "epoch 39 || Epoch_D_Loss :0.1751 || Epoch_G_Loss :0.0286\n",
      "timer : 2.8018 sec.\n",
      "------------\n",
      "Epoch : 40/300\n",
      "------------\n",
      "(train)\n",
      "------------\n",
      "epoch 40 || Epoch_D_Loss :0.1862 || Epoch_G_Loss :0.0328\n",
      "timer : 2.8073 sec.\n",
      "------------\n",
      "Epoch : 41/300\n",
      "------------\n",
      "(train)\n",
      "------------\n",
      "epoch 41 || Epoch_D_Loss :0.1680 || Epoch_G_Loss :0.0544\n",
      "timer : 2.8105 sec.\n",
      "------------\n",
      "Epoch : 42/300\n",
      "------------\n",
      "(train)\n",
      "------------\n",
      "epoch 42 || Epoch_D_Loss :0.1582 || Epoch_G_Loss :0.0448\n",
      "timer : 2.8192 sec.\n",
      "------------\n",
      "Epoch : 43/300\n",
      "------------\n",
      "(train)\n",
      "------------\n",
      "epoch 43 || Epoch_D_Loss :0.1808 || Epoch_G_Loss :0.0584\n",
      "timer : 2.8177 sec.\n",
      "------------\n",
      "Epoch : 44/300\n",
      "------------\n",
      "(train)\n",
      "------------\n",
      "epoch 44 || Epoch_D_Loss :0.1905 || Epoch_G_Loss :0.0237\n",
      "timer : 2.8088 sec.\n",
      "------------\n",
      "Epoch : 45/300\n",
      "------------\n",
      "(train)\n",
      "------------\n",
      "epoch 45 || Epoch_D_Loss :0.1713 || Epoch_G_Loss :0.0497\n",
      "timer : 2.8152 sec.\n",
      "------------\n",
      "Epoch : 46/300\n",
      "------------\n",
      "(train)\n",
      "------------\n",
      "epoch 46 || Epoch_D_Loss :0.1765 || Epoch_G_Loss :0.0657\n",
      "timer : 2.8094 sec.\n",
      "------------\n",
      "Epoch : 47/300\n",
      "------------\n",
      "(train)\n",
      "------------\n",
      "epoch 47 || Epoch_D_Loss :0.1549 || Epoch_G_Loss :0.0429\n",
      "timer : 2.8161 sec.\n",
      "------------\n",
      "Epoch : 48/300\n",
      "------------\n",
      "(train)\n",
      "------------\n",
      "epoch 48 || Epoch_D_Loss :0.1643 || Epoch_G_Loss :0.0799\n",
      "timer : 2.8138 sec.\n",
      "------------\n",
      "Epoch : 49/300\n",
      "------------\n",
      "(train)\n",
      "------------\n",
      "epoch 49 || Epoch_D_Loss :0.1736 || Epoch_G_Loss :0.0342\n",
      "timer : 2.8180 sec.\n",
      "------------\n",
      "Epoch : 50/300\n",
      "------------\n",
      "(train)\n",
      "------------\n",
      "epoch 50 || Epoch_D_Loss :0.1901 || Epoch_G_Loss :0.0810\n",
      "timer : 2.8119 sec.\n",
      "------------\n",
      "Epoch : 51/300\n",
      "------------\n",
      "(train)\n",
      "------------\n",
      "epoch 51 || Epoch_D_Loss :0.1761 || Epoch_G_Loss :0.0460\n",
      "timer : 2.8027 sec.\n",
      "------------\n",
      "Epoch : 52/300\n",
      "------------\n",
      "(train)\n",
      "------------\n",
      "epoch 52 || Epoch_D_Loss :0.1702 || Epoch_G_Loss :0.0470\n",
      "timer : 2.8063 sec.\n",
      "------------\n",
      "Epoch : 53/300\n",
      "------------\n",
      "(train)\n",
      "------------\n",
      "epoch 53 || Epoch_D_Loss :0.1736 || Epoch_G_Loss :0.0398\n",
      "timer : 2.8109 sec.\n",
      "------------\n",
      "Epoch : 54/300\n",
      "------------\n",
      "(train)\n",
      "------------\n",
      "epoch 54 || Epoch_D_Loss :0.1623 || Epoch_G_Loss :0.0272\n",
      "timer : 2.8129 sec.\n",
      "------------\n",
      "Epoch : 55/300\n",
      "------------\n",
      "(train)\n",
      "------------\n",
      "epoch 55 || Epoch_D_Loss :0.1830 || Epoch_G_Loss :0.0612\n",
      "timer : 2.8158 sec.\n",
      "------------\n",
      "Epoch : 56/300\n",
      "------------\n",
      "(train)\n",
      "------------\n",
      "epoch 56 || Epoch_D_Loss :0.1679 || Epoch_G_Loss :0.0524\n",
      "timer : 2.8066 sec.\n",
      "------------\n",
      "Epoch : 57/300\n",
      "------------\n",
      "(train)\n",
      "------------\n",
      "epoch 57 || Epoch_D_Loss :0.1701 || Epoch_G_Loss :0.0486\n",
      "timer : 2.8183 sec.\n",
      "------------\n",
      "Epoch : 58/300\n",
      "------------\n",
      "(train)\n",
      "------------\n",
      "epoch 58 || Epoch_D_Loss :0.1786 || Epoch_G_Loss :0.0409\n",
      "timer : 2.8091 sec.\n",
      "------------\n",
      "Epoch : 59/300\n",
      "------------\n",
      "(train)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------\n",
      "epoch 59 || Epoch_D_Loss :0.1612 || Epoch_G_Loss :0.0380\n",
      "timer : 2.8090 sec.\n",
      "------------\n",
      "Epoch : 60/300\n",
      "------------\n",
      "(train)\n",
      "------------\n",
      "epoch 60 || Epoch_D_Loss :0.1596 || Epoch_G_Loss :0.0692\n",
      "timer : 2.8078 sec.\n",
      "------------\n",
      "Epoch : 61/300\n",
      "------------\n",
      "(train)\n",
      "------------\n",
      "epoch 61 || Epoch_D_Loss :0.1675 || Epoch_G_Loss :0.0194\n",
      "timer : 2.8112 sec.\n",
      "------------\n",
      "Epoch : 62/300\n",
      "------------\n",
      "(train)\n",
      "------------\n",
      "epoch 62 || Epoch_D_Loss :0.1914 || Epoch_G_Loss :0.0607\n",
      "timer : 2.8068 sec.\n",
      "------------\n",
      "Epoch : 63/300\n",
      "------------\n",
      "(train)\n",
      "------------\n",
      "epoch 63 || Epoch_D_Loss :0.1619 || Epoch_G_Loss :0.0583\n",
      "timer : 2.8107 sec.\n",
      "------------\n",
      "Epoch : 64/300\n",
      "------------\n",
      "(train)\n",
      "------------\n",
      "epoch 64 || Epoch_D_Loss :0.1700 || Epoch_G_Loss :0.0493\n",
      "timer : 2.8103 sec.\n",
      "------------\n",
      "Epoch : 65/300\n",
      "------------\n",
      "(train)\n",
      "------------\n",
      "epoch 65 || Epoch_D_Loss :0.1731 || Epoch_G_Loss :0.0477\n",
      "timer : 2.8157 sec.\n",
      "------------\n",
      "Epoch : 66/300\n",
      "------------\n",
      "(train)\n",
      "------------\n",
      "epoch 66 || Epoch_D_Loss :0.1744 || Epoch_G_Loss :0.0453\n",
      "timer : 2.8183 sec.\n",
      "------------\n",
      "Epoch : 67/300\n",
      "------------\n",
      "(train)\n",
      "------------\n",
      "epoch 67 || Epoch_D_Loss :0.1513 || Epoch_G_Loss :0.0485\n",
      "timer : 2.8255 sec.\n",
      "------------\n",
      "Epoch : 68/300\n",
      "------------\n",
      "(train)\n",
      "------------\n",
      "epoch 68 || Epoch_D_Loss :0.1669 || Epoch_G_Loss :0.0548\n",
      "timer : 2.8114 sec.\n",
      "------------\n",
      "Epoch : 69/300\n",
      "------------\n",
      "(train)\n",
      "------------\n",
      "epoch 69 || Epoch_D_Loss :0.1667 || Epoch_G_Loss :0.0370\n",
      "timer : 2.8247 sec.\n",
      "------------\n",
      "Epoch : 70/300\n",
      "------------\n",
      "(train)\n",
      "------------\n",
      "epoch 70 || Epoch_D_Loss :0.1696 || Epoch_G_Loss :0.0329\n",
      "timer : 2.8143 sec.\n",
      "------------\n",
      "Epoch : 71/300\n",
      "------------\n",
      "(train)\n",
      "------------\n",
      "epoch 71 || Epoch_D_Loss :0.1631 || Epoch_G_Loss :0.0498\n",
      "timer : 2.8140 sec.\n",
      "------------\n",
      "Epoch : 72/300\n",
      "------------\n",
      "(train)\n",
      "------------\n",
      "epoch 72 || Epoch_D_Loss :0.1574 || Epoch_G_Loss :0.0531\n",
      "timer : 2.8377 sec.\n",
      "------------\n",
      "Epoch : 73/300\n",
      "------------\n",
      "(train)\n",
      "------------\n",
      "epoch 73 || Epoch_D_Loss :0.1608 || Epoch_G_Loss :0.0479\n",
      "timer : 2.8251 sec.\n",
      "------------\n",
      "Epoch : 74/300\n",
      "------------\n",
      "(train)\n",
      "------------\n",
      "epoch 74 || Epoch_D_Loss :0.1654 || Epoch_G_Loss :0.0434\n",
      "timer : 2.8203 sec.\n",
      "------------\n",
      "Epoch : 75/300\n",
      "------------\n",
      "(train)\n",
      "------------\n",
      "epoch 75 || Epoch_D_Loss :0.1601 || Epoch_G_Loss :0.0479\n",
      "timer : 2.8147 sec.\n",
      "------------\n",
      "Epoch : 76/300\n",
      "------------\n",
      "(train)\n",
      "------------\n",
      "epoch 76 || Epoch_D_Loss :0.1647 || Epoch_G_Loss :0.0426\n",
      "timer : 2.8132 sec.\n",
      "------------\n",
      "Epoch : 77/300\n",
      "------------\n",
      "(train)\n",
      "------------\n",
      "epoch 77 || Epoch_D_Loss :0.1501 || Epoch_G_Loss :0.0378\n",
      "timer : 2.8253 sec.\n",
      "------------\n",
      "Epoch : 78/300\n",
      "------------\n",
      "(train)\n",
      "------------\n",
      "epoch 78 || Epoch_D_Loss :0.1576 || Epoch_G_Loss :0.0512\n",
      "timer : 2.8185 sec.\n",
      "------------\n",
      "Epoch : 79/300\n",
      "------------\n",
      "(train)\n",
      "------------\n",
      "epoch 79 || Epoch_D_Loss :0.1409 || Epoch_G_Loss :0.0863\n",
      "timer : 2.8304 sec.\n",
      "------------\n",
      "Epoch : 80/300\n",
      "------------\n",
      "(train)\n",
      "------------\n",
      "epoch 80 || Epoch_D_Loss :0.1703 || Epoch_G_Loss :0.0344\n",
      "timer : 2.8231 sec.\n",
      "------------\n",
      "Epoch : 81/300\n",
      "------------\n",
      "(train)\n",
      "------------\n",
      "epoch 81 || Epoch_D_Loss :0.1664 || Epoch_G_Loss :0.0544\n",
      "timer : 2.8136 sec.\n",
      "------------\n",
      "Epoch : 82/300\n",
      "------------\n",
      "(train)\n",
      "------------\n",
      "epoch 82 || Epoch_D_Loss :0.1509 || Epoch_G_Loss :0.0478\n",
      "timer : 2.8161 sec.\n",
      "------------\n",
      "Epoch : 83/300\n",
      "------------\n",
      "(train)\n",
      "------------\n",
      "epoch 83 || Epoch_D_Loss :0.1569 || Epoch_G_Loss :0.0567\n",
      "timer : 2.8343 sec.\n",
      "------------\n",
      "Epoch : 84/300\n",
      "------------\n",
      "(train)\n",
      "------------\n",
      "epoch 84 || Epoch_D_Loss :0.1491 || Epoch_G_Loss :0.0407\n",
      "timer : 2.8183 sec.\n",
      "------------\n",
      "Epoch : 85/300\n",
      "------------\n",
      "(train)\n",
      "------------\n",
      "epoch 85 || Epoch_D_Loss :0.1601 || Epoch_G_Loss :0.0645\n",
      "timer : 2.8184 sec.\n",
      "------------\n",
      "Epoch : 86/300\n",
      "------------\n",
      "(train)\n",
      "------------\n",
      "epoch 86 || Epoch_D_Loss :0.1583 || Epoch_G_Loss :0.0450\n",
      "timer : 2.8159 sec.\n",
      "------------\n",
      "Epoch : 87/300\n",
      "------------\n",
      "(train)\n",
      "------------\n",
      "epoch 87 || Epoch_D_Loss :0.1451 || Epoch_G_Loss :0.0610\n",
      "timer : 2.8164 sec.\n",
      "------------\n",
      "Epoch : 88/300\n",
      "------------\n",
      "(train)\n",
      "------------\n",
      "epoch 88 || Epoch_D_Loss :0.1605 || Epoch_G_Loss :0.0932\n",
      "timer : 2.8064 sec.\n",
      "------------\n",
      "Epoch : 89/300\n",
      "------------\n",
      "(train)\n",
      "------------\n",
      "epoch 89 || Epoch_D_Loss :0.1434 || Epoch_G_Loss :0.0521\n",
      "timer : 2.8129 sec.\n",
      "------------\n",
      "Epoch : 90/300\n",
      "------------\n",
      "(train)\n",
      "------------\n",
      "epoch 90 || Epoch_D_Loss :0.1576 || Epoch_G_Loss :0.0545\n",
      "timer : 2.8203 sec.\n",
      "------------\n",
      "Epoch : 91/300\n",
      "------------\n",
      "(train)\n",
      "------------\n",
      "epoch 91 || Epoch_D_Loss :0.1445 || Epoch_G_Loss :0.0569\n",
      "timer : 2.8258 sec.\n",
      "------------\n",
      "Epoch : 92/300\n",
      "------------\n",
      "(train)\n",
      "------------\n",
      "epoch 92 || Epoch_D_Loss :0.1324 || Epoch_G_Loss :0.0791\n",
      "timer : 2.8273 sec.\n",
      "------------\n",
      "Epoch : 93/300\n",
      "------------\n",
      "(train)\n",
      "------------\n",
      "epoch 93 || Epoch_D_Loss :0.1490 || Epoch_G_Loss :0.0490\n",
      "timer : 2.8168 sec.\n",
      "------------\n",
      "Epoch : 94/300\n",
      "------------\n",
      "(train)\n",
      "------------\n",
      "epoch 94 || Epoch_D_Loss :0.1465 || Epoch_G_Loss :0.0638\n",
      "timer : 2.8177 sec.\n",
      "------------\n",
      "Epoch : 95/300\n",
      "------------\n",
      "(train)\n",
      "------------\n",
      "epoch 95 || Epoch_D_Loss :0.1460 || Epoch_G_Loss :0.0562\n",
      "timer : 2.8181 sec.\n",
      "------------\n",
      "Epoch : 96/300\n",
      "------------\n",
      "(train)\n",
      "------------\n",
      "epoch 96 || Epoch_D_Loss :0.1242 || Epoch_G_Loss :0.0942\n",
      "timer : 2.8263 sec.\n",
      "------------\n",
      "Epoch : 97/300\n",
      "------------\n",
      "(train)\n",
      "------------\n",
      "epoch 97 || Epoch_D_Loss :0.1528 || Epoch_G_Loss :0.0538\n",
      "timer : 2.8245 sec.\n",
      "------------\n",
      "Epoch : 98/300\n",
      "------------\n",
      "(train)\n",
      "------------\n",
      "epoch 98 || Epoch_D_Loss :0.1520 || Epoch_G_Loss :0.0625\n",
      "timer : 2.8198 sec.\n",
      "------------\n",
      "Epoch : 99/300\n",
      "------------\n",
      "(train)\n",
      "------------\n",
      "epoch 99 || Epoch_D_Loss :0.1374 || Epoch_G_Loss :0.0645\n",
      "timer : 2.8122 sec.\n",
      "------------\n",
      "Epoch : 100/300\n",
      "------------\n",
      "(train)\n",
      "------------\n",
      "epoch 100 || Epoch_D_Loss :0.1419 || Epoch_G_Loss :0.0708\n",
      "timer : 2.8077 sec.\n",
      "------------\n",
      "Epoch : 101/300\n",
      "------------\n",
      "(train)\n",
      "------------\n",
      "epoch 101 || Epoch_D_Loss :0.1457 || Epoch_G_Loss :0.0631\n",
      "timer : 2.8166 sec.\n",
      "------------\n",
      "Epoch : 102/300\n",
      "------------\n",
      "(train)\n",
      "------------\n",
      "epoch 102 || Epoch_D_Loss :0.1359 || Epoch_G_Loss :0.0696\n",
      "timer : 2.8150 sec.\n",
      "------------\n",
      "Epoch : 103/300\n",
      "------------\n",
      "(train)\n",
      "------------\n",
      "epoch 103 || Epoch_D_Loss :0.1347 || Epoch_G_Loss :0.0728\n",
      "timer : 2.8182 sec.\n",
      "------------\n",
      "Epoch : 104/300\n",
      "------------\n",
      "(train)\n",
      "------------\n",
      "epoch 104 || Epoch_D_Loss :0.1410 || Epoch_G_Loss :0.0651\n",
      "timer : 2.8214 sec.\n",
      "------------\n",
      "Epoch : 105/300\n",
      "------------\n",
      "(train)\n",
      "------------\n",
      "epoch 105 || Epoch_D_Loss :0.1317 || Epoch_G_Loss :0.0746\n",
      "timer : 2.8294 sec.\n",
      "------------\n",
      "Epoch : 106/300\n",
      "------------\n",
      "(train)\n",
      "------------\n",
      "epoch 106 || Epoch_D_Loss :0.1329 || Epoch_G_Loss :0.0631\n",
      "timer : 2.8239 sec.\n",
      "------------\n",
      "Epoch : 107/300\n",
      "------------\n",
      "(train)\n",
      "------------\n",
      "epoch 107 || Epoch_D_Loss :0.1329 || Epoch_G_Loss :0.0616\n",
      "timer : 2.8137 sec.\n",
      "------------\n",
      "Epoch : 108/300\n",
      "------------\n",
      "(train)\n",
      "------------\n",
      "epoch 108 || Epoch_D_Loss :0.1415 || Epoch_G_Loss :0.0690\n",
      "timer : 2.8262 sec.\n",
      "------------\n",
      "Epoch : 109/300\n",
      "------------\n",
      "(train)\n",
      "------------\n",
      "epoch 109 || Epoch_D_Loss :0.1330 || Epoch_G_Loss :0.0698\n",
      "timer : 2.8140 sec.\n",
      "------------\n",
      "Epoch : 110/300\n",
      "------------\n",
      "(train)\n",
      "------------\n",
      "epoch 110 || Epoch_D_Loss :0.1309 || Epoch_G_Loss :0.0810\n",
      "timer : 2.8263 sec.\n",
      "------------\n",
      "Epoch : 111/300\n",
      "------------\n",
      "(train)\n",
      "------------\n",
      "epoch 111 || Epoch_D_Loss :0.1345 || Epoch_G_Loss :0.0730\n",
      "timer : 2.8121 sec.\n",
      "------------\n",
      "Epoch : 112/300\n",
      "------------\n",
      "(train)\n",
      "------------\n",
      "epoch 112 || Epoch_D_Loss :0.1298 || Epoch_G_Loss :0.0596\n",
      "timer : 2.8146 sec.\n",
      "------------\n",
      "Epoch : 113/300\n",
      "------------\n",
      "(train)\n",
      "------------\n",
      "epoch 113 || Epoch_D_Loss :0.1372 || Epoch_G_Loss :0.0674\n",
      "timer : 2.8104 sec.\n",
      "------------\n",
      "Epoch : 114/300\n",
      "------------\n",
      "(train)\n",
      "------------\n",
      "epoch 114 || Epoch_D_Loss :0.1222 || Epoch_G_Loss :0.0727\n",
      "timer : 2.8267 sec.\n",
      "------------\n",
      "Epoch : 115/300\n",
      "------------\n",
      "(train)\n",
      "------------\n",
      "epoch 115 || Epoch_D_Loss :0.1359 || Epoch_G_Loss :0.0713\n",
      "timer : 2.8138 sec.\n",
      "------------\n",
      "Epoch : 116/300\n",
      "------------\n",
      "(train)\n",
      "------------\n",
      "epoch 116 || Epoch_D_Loss :0.1234 || Epoch_G_Loss :0.0687\n",
      "timer : 2.8149 sec.\n",
      "------------\n",
      "Epoch : 117/300\n",
      "------------\n",
      "(train)\n",
      "------------\n",
      "epoch 117 || Epoch_D_Loss :0.1169 || Epoch_G_Loss :0.0940\n",
      "timer : 2.8082 sec.\n",
      "------------\n",
      "Epoch : 118/300\n",
      "------------\n",
      "(train)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------\n",
      "epoch 118 || Epoch_D_Loss :0.1277 || Epoch_G_Loss :0.0478\n",
      "timer : 2.8173 sec.\n",
      "------------\n",
      "Epoch : 119/300\n",
      "------------\n",
      "(train)\n",
      "------------\n",
      "epoch 119 || Epoch_D_Loss :0.1313 || Epoch_G_Loss :0.0873\n",
      "timer : 2.8100 sec.\n",
      "------------\n",
      "Epoch : 120/300\n",
      "------------\n",
      "(train)\n",
      "------------\n",
      "epoch 120 || Epoch_D_Loss :0.1268 || Epoch_G_Loss :0.0638\n",
      "timer : 2.8084 sec.\n",
      "------------\n",
      "Epoch : 121/300\n",
      "------------\n",
      "(train)\n",
      "------------\n",
      "epoch 121 || Epoch_D_Loss :0.1221 || Epoch_G_Loss :0.0916\n",
      "timer : 2.8152 sec.\n",
      "------------\n",
      "Epoch : 122/300\n",
      "------------\n",
      "(train)\n",
      "------------\n",
      "epoch 122 || Epoch_D_Loss :0.1296 || Epoch_G_Loss :0.0572\n",
      "timer : 2.8202 sec.\n",
      "------------\n",
      "Epoch : 123/300\n",
      "------------\n",
      "(train)\n",
      "------------\n",
      "epoch 123 || Epoch_D_Loss :0.1298 || Epoch_G_Loss :0.0777\n",
      "timer : 2.8085 sec.\n",
      "------------\n",
      "Epoch : 124/300\n",
      "------------\n",
      "(train)\n",
      "------------\n",
      "epoch 124 || Epoch_D_Loss :0.1120 || Epoch_G_Loss :0.0859\n",
      "timer : 2.8084 sec.\n",
      "------------\n",
      "Epoch : 125/300\n",
      "------------\n",
      "(train)\n",
      "------------\n",
      "epoch 125 || Epoch_D_Loss :0.1212 || Epoch_G_Loss :0.0681\n",
      "timer : 2.8131 sec.\n",
      "------------\n",
      "Epoch : 126/300\n",
      "------------\n",
      "(train)\n",
      "------------\n",
      "epoch 126 || Epoch_D_Loss :0.1275 || Epoch_G_Loss :0.0922\n",
      "timer : 2.8136 sec.\n",
      "------------\n",
      "Epoch : 127/300\n",
      "------------\n",
      "(train)\n",
      "------------\n",
      "epoch 127 || Epoch_D_Loss :0.1171 || Epoch_G_Loss :0.0858\n",
      "timer : 2.8259 sec.\n",
      "------------\n",
      "Epoch : 128/300\n",
      "------------\n",
      "(train)\n",
      "------------\n",
      "epoch 128 || Epoch_D_Loss :0.1148 || Epoch_G_Loss :0.0989\n",
      "timer : 2.8333 sec.\n",
      "------------\n",
      "Epoch : 129/300\n",
      "------------\n",
      "(train)\n",
      "------------\n",
      "epoch 129 || Epoch_D_Loss :0.1215 || Epoch_G_Loss :0.0762\n",
      "timer : 2.8449 sec.\n",
      "------------\n",
      "Epoch : 130/300\n",
      "------------\n",
      "(train)\n",
      "------------\n",
      "epoch 130 || Epoch_D_Loss :0.1112 || Epoch_G_Loss :0.0862\n",
      "timer : 2.8338 sec.\n",
      "------------\n",
      "Epoch : 131/300\n",
      "------------\n",
      "(train)\n",
      "------------\n",
      "epoch 131 || Epoch_D_Loss :0.1133 || Epoch_G_Loss :0.0831\n",
      "timer : 2.8165 sec.\n",
      "------------\n",
      "Epoch : 132/300\n",
      "------------\n",
      "(train)\n",
      "------------\n",
      "epoch 132 || Epoch_D_Loss :0.1200 || Epoch_G_Loss :0.0835\n",
      "timer : 2.8234 sec.\n",
      "------------\n",
      "Epoch : 133/300\n",
      "------------\n",
      "(train)\n",
      "------------\n",
      "epoch 133 || Epoch_D_Loss :0.1164 || Epoch_G_Loss :0.1099\n",
      "timer : 2.8112 sec.\n",
      "------------\n",
      "Epoch : 134/300\n",
      "------------\n",
      "(train)\n",
      "------------\n",
      "epoch 134 || Epoch_D_Loss :0.1103 || Epoch_G_Loss :0.0809\n",
      "timer : 2.8362 sec.\n",
      "------------\n",
      "Epoch : 135/300\n",
      "------------\n",
      "(train)\n",
      "------------\n",
      "epoch 135 || Epoch_D_Loss :0.1058 || Epoch_G_Loss :0.0919\n",
      "timer : 2.8314 sec.\n",
      "------------\n",
      "Epoch : 136/300\n",
      "------------\n",
      "(train)\n",
      "------------\n",
      "epoch 136 || Epoch_D_Loss :0.1170 || Epoch_G_Loss :0.0981\n",
      "timer : 2.8245 sec.\n",
      "------------\n",
      "Epoch : 137/300\n",
      "------------\n",
      "(train)\n",
      "------------\n",
      "epoch 137 || Epoch_D_Loss :0.1126 || Epoch_G_Loss :0.1198\n",
      "timer : 2.8050 sec.\n",
      "------------\n",
      "Epoch : 138/300\n",
      "------------\n",
      "(train)\n",
      "------------\n",
      "epoch 138 || Epoch_D_Loss :0.1120 || Epoch_G_Loss :0.0959\n",
      "timer : 2.8215 sec.\n",
      "------------\n",
      "Epoch : 139/300\n",
      "------------\n",
      "(train)\n",
      "------------\n",
      "epoch 139 || Epoch_D_Loss :0.1153 || Epoch_G_Loss :0.0877\n",
      "timer : 2.8173 sec.\n",
      "------------\n",
      "Epoch : 140/300\n",
      "------------\n",
      "(train)\n",
      "------------\n",
      "epoch 140 || Epoch_D_Loss :0.1043 || Epoch_G_Loss :0.0772\n",
      "timer : 2.8315 sec.\n",
      "------------\n",
      "Epoch : 141/300\n",
      "------------\n",
      "(train)\n",
      "------------\n",
      "epoch 141 || Epoch_D_Loss :0.1084 || Epoch_G_Loss :0.0993\n",
      "timer : 2.8201 sec.\n",
      "------------\n",
      "Epoch : 142/300\n",
      "------------\n",
      "(train)\n",
      "------------\n",
      "epoch 142 || Epoch_D_Loss :0.1048 || Epoch_G_Loss :0.0883\n",
      "timer : 2.8134 sec.\n",
      "------------\n",
      "Epoch : 143/300\n",
      "------------\n",
      "(train)\n",
      "------------\n",
      "epoch 143 || Epoch_D_Loss :0.1092 || Epoch_G_Loss :0.0925\n",
      "timer : 2.8425 sec.\n",
      "------------\n",
      "Epoch : 144/300\n",
      "------------\n",
      "(train)\n",
      "------------\n",
      "epoch 144 || Epoch_D_Loss :0.1088 || Epoch_G_Loss :0.1034\n",
      "timer : 2.8321 sec.\n",
      "------------\n",
      "Epoch : 145/300\n",
      "------------\n",
      "(train)\n",
      "------------\n",
      "epoch 145 || Epoch_D_Loss :0.1050 || Epoch_G_Loss :0.1021\n",
      "timer : 2.8319 sec.\n",
      "------------\n",
      "Epoch : 146/300\n",
      "------------\n",
      "(train)\n",
      "------------\n",
      "epoch 146 || Epoch_D_Loss :0.1127 || Epoch_G_Loss :0.0880\n",
      "timer : 2.8309 sec.\n",
      "------------\n",
      "Epoch : 147/300\n",
      "------------\n",
      "(train)\n",
      "------------\n",
      "epoch 147 || Epoch_D_Loss :0.1051 || Epoch_G_Loss :0.1010\n",
      "timer : 2.8288 sec.\n",
      "------------\n",
      "Epoch : 148/300\n",
      "------------\n",
      "(train)\n",
      "------------\n",
      "epoch 148 || Epoch_D_Loss :0.0898 || Epoch_G_Loss :0.1008\n",
      "timer : 2.8267 sec.\n",
      "------------\n",
      "Epoch : 149/300\n",
      "------------\n",
      "(train)\n",
      "------------\n",
      "epoch 149 || Epoch_D_Loss :0.1084 || Epoch_G_Loss :0.1037\n",
      "timer : 2.8377 sec.\n",
      "------------\n",
      "Epoch : 150/300\n",
      "------------\n",
      "(train)\n",
      "------------\n",
      "epoch 150 || Epoch_D_Loss :0.0972 || Epoch_G_Loss :0.0857\n",
      "timer : 2.8233 sec.\n",
      "------------\n",
      "Epoch : 151/300\n",
      "------------\n",
      "(train)\n",
      "------------\n",
      "epoch 151 || Epoch_D_Loss :0.1046 || Epoch_G_Loss :0.0933\n",
      "timer : 2.8162 sec.\n",
      "------------\n",
      "Epoch : 152/300\n",
      "------------\n",
      "(train)\n",
      "------------\n",
      "epoch 152 || Epoch_D_Loss :0.0805 || Epoch_G_Loss :0.1004\n",
      "timer : 2.8169 sec.\n",
      "------------\n",
      "Epoch : 153/300\n",
      "------------\n",
      "(train)\n",
      "------------\n",
      "epoch 153 || Epoch_D_Loss :0.1089 || Epoch_G_Loss :0.1043\n",
      "timer : 2.8248 sec.\n",
      "------------\n",
      "Epoch : 154/300\n",
      "------------\n",
      "(train)\n",
      "------------\n",
      "epoch 154 || Epoch_D_Loss :0.1089 || Epoch_G_Loss :0.0793\n",
      "timer : 2.8222 sec.\n",
      "------------\n",
      "Epoch : 155/300\n",
      "------------\n",
      "(train)\n",
      "------------\n",
      "epoch 155 || Epoch_D_Loss :0.0998 || Epoch_G_Loss :0.1068\n",
      "timer : 2.8192 sec.\n",
      "------------\n",
      "Epoch : 156/300\n",
      "------------\n",
      "(train)\n",
      "------------\n",
      "epoch 156 || Epoch_D_Loss :0.0860 || Epoch_G_Loss :0.1110\n",
      "timer : 2.8054 sec.\n",
      "------------\n",
      "Epoch : 157/300\n",
      "------------\n",
      "(train)\n",
      "------------\n",
      "epoch 157 || Epoch_D_Loss :0.1068 || Epoch_G_Loss :0.1023\n",
      "timer : 2.8098 sec.\n",
      "------------\n",
      "Epoch : 158/300\n",
      "------------\n",
      "(train)\n",
      "------------\n",
      "epoch 158 || Epoch_D_Loss :0.0948 || Epoch_G_Loss :0.1141\n",
      "timer : 2.8154 sec.\n",
      "------------\n",
      "Epoch : 159/300\n",
      "------------\n",
      "(train)\n",
      "------------\n",
      "epoch 159 || Epoch_D_Loss :0.0962 || Epoch_G_Loss :0.0902\n",
      "timer : 2.8126 sec.\n",
      "------------\n",
      "Epoch : 160/300\n",
      "------------\n",
      "(train)\n",
      "------------\n",
      "epoch 160 || Epoch_D_Loss :0.0972 || Epoch_G_Loss :0.1153\n",
      "timer : 2.8077 sec.\n",
      "------------\n",
      "Epoch : 161/300\n",
      "------------\n",
      "(train)\n",
      "------------\n",
      "epoch 161 || Epoch_D_Loss :0.0959 || Epoch_G_Loss :0.0991\n",
      "timer : 2.8113 sec.\n",
      "------------\n",
      "Epoch : 162/300\n",
      "------------\n",
      "(train)\n",
      "------------\n",
      "epoch 162 || Epoch_D_Loss :0.0863 || Epoch_G_Loss :0.1067\n",
      "timer : 2.8193 sec.\n",
      "------------\n",
      "Epoch : 163/300\n",
      "------------\n",
      "(train)\n",
      "------------\n",
      "epoch 163 || Epoch_D_Loss :0.0856 || Epoch_G_Loss :0.1038\n",
      "timer : 2.8160 sec.\n",
      "------------\n",
      "Epoch : 164/300\n",
      "------------\n",
      "(train)\n",
      "------------\n",
      "epoch 164 || Epoch_D_Loss :0.0908 || Epoch_G_Loss :0.0964\n",
      "timer : 2.8223 sec.\n",
      "------------\n",
      "Epoch : 165/300\n",
      "------------\n",
      "(train)\n",
      "------------\n",
      "epoch 165 || Epoch_D_Loss :0.0970 || Epoch_G_Loss :0.1148\n",
      "timer : 2.8200 sec.\n",
      "------------\n",
      "Epoch : 166/300\n",
      "------------\n",
      "(train)\n",
      "------------\n",
      "epoch 166 || Epoch_D_Loss :0.0933 || Epoch_G_Loss :0.0997\n",
      "timer : 2.8226 sec.\n",
      "------------\n",
      "Epoch : 167/300\n",
      "------------\n",
      "(train)\n",
      "------------\n",
      "epoch 167 || Epoch_D_Loss :0.0889 || Epoch_G_Loss :0.1110\n",
      "timer : 2.8183 sec.\n",
      "------------\n",
      "Epoch : 168/300\n",
      "------------\n",
      "(train)\n",
      "------------\n",
      "epoch 168 || Epoch_D_Loss :0.0888 || Epoch_G_Loss :0.0954\n",
      "timer : 2.8171 sec.\n",
      "------------\n",
      "Epoch : 169/300\n",
      "------------\n",
      "(train)\n",
      "------------\n",
      "epoch 169 || Epoch_D_Loss :0.0880 || Epoch_G_Loss :0.1119\n",
      "timer : 2.8142 sec.\n",
      "------------\n",
      "Epoch : 170/300\n",
      "------------\n",
      "(train)\n",
      "------------\n",
      "epoch 170 || Epoch_D_Loss :0.0889 || Epoch_G_Loss :0.1144\n",
      "timer : 2.8177 sec.\n",
      "------------\n",
      "Epoch : 171/300\n",
      "------------\n",
      "(train)\n",
      "------------\n",
      "epoch 171 || Epoch_D_Loss :0.0760 || Epoch_G_Loss :0.1097\n",
      "timer : 2.8188 sec.\n",
      "------------\n",
      "Epoch : 172/300\n",
      "------------\n",
      "(train)\n",
      "------------\n",
      "epoch 172 || Epoch_D_Loss :0.0722 || Epoch_G_Loss :0.1409\n",
      "timer : 2.8170 sec.\n",
      "------------\n",
      "Epoch : 173/300\n",
      "------------\n",
      "(train)\n",
      "------------\n",
      "epoch 173 || Epoch_D_Loss :0.0931 || Epoch_G_Loss :0.1071\n",
      "timer : 2.8142 sec.\n",
      "------------\n",
      "Epoch : 174/300\n",
      "------------\n",
      "(train)\n",
      "------------\n",
      "epoch 174 || Epoch_D_Loss :0.0874 || Epoch_G_Loss :0.1307\n",
      "timer : 2.8119 sec.\n",
      "------------\n",
      "Epoch : 175/300\n",
      "------------\n",
      "(train)\n",
      "------------\n",
      "epoch 175 || Epoch_D_Loss :0.0725 || Epoch_G_Loss :0.1318\n",
      "timer : 2.8178 sec.\n",
      "------------\n",
      "Epoch : 176/300\n",
      "------------\n",
      "(train)\n",
      "------------\n",
      "epoch 176 || Epoch_D_Loss :0.1012 || Epoch_G_Loss :0.1162\n",
      "timer : 2.8076 sec.\n",
      "------------\n",
      "Epoch : 177/300\n",
      "------------\n",
      "(train)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------\n",
      "epoch 177 || Epoch_D_Loss :0.0867 || Epoch_G_Loss :0.1207\n",
      "timer : 2.8327 sec.\n",
      "------------\n",
      "Epoch : 178/300\n",
      "------------\n",
      "(train)\n",
      "------------\n",
      "epoch 178 || Epoch_D_Loss :0.0814 || Epoch_G_Loss :0.1380\n",
      "timer : 2.8073 sec.\n",
      "------------\n",
      "Epoch : 179/300\n",
      "------------\n",
      "(train)\n",
      "------------\n",
      "epoch 179 || Epoch_D_Loss :0.0644 || Epoch_G_Loss :0.1252\n",
      "timer : 2.8140 sec.\n",
      "------------\n",
      "Epoch : 180/300\n",
      "------------\n",
      "(train)\n",
      "------------\n",
      "epoch 180 || Epoch_D_Loss :0.0990 || Epoch_G_Loss :0.1315\n",
      "timer : 2.8074 sec.\n",
      "------------\n",
      "Epoch : 181/300\n",
      "------------\n",
      "(train)\n",
      "------------\n",
      "epoch 181 || Epoch_D_Loss :0.0767 || Epoch_G_Loss :0.1132\n",
      "timer : 2.8292 sec.\n",
      "------------\n",
      "Epoch : 182/300\n",
      "------------\n",
      "(train)\n",
      "------------\n",
      "epoch 182 || Epoch_D_Loss :0.0793 || Epoch_G_Loss :0.1201\n",
      "timer : 2.8168 sec.\n",
      "------------\n",
      "Epoch : 183/300\n",
      "------------\n",
      "(train)\n",
      "------------\n",
      "epoch 183 || Epoch_D_Loss :0.0752 || Epoch_G_Loss :0.1351\n",
      "timer : 2.8120 sec.\n",
      "------------\n",
      "Epoch : 184/300\n",
      "------------\n",
      "(train)\n",
      "------------\n",
      "epoch 184 || Epoch_D_Loss :0.0779 || Epoch_G_Loss :0.1248\n",
      "timer : 2.8073 sec.\n",
      "------------\n",
      "Epoch : 185/300\n",
      "------------\n",
      "(train)\n",
      "------------\n",
      "epoch 185 || Epoch_D_Loss :0.0802 || Epoch_G_Loss :0.1100\n",
      "timer : 2.8145 sec.\n",
      "------------\n",
      "Epoch : 186/300\n",
      "------------\n",
      "(train)\n",
      "------------\n",
      "epoch 186 || Epoch_D_Loss :0.0742 || Epoch_G_Loss :0.1366\n",
      "timer : 2.8102 sec.\n",
      "------------\n",
      "Epoch : 187/300\n",
      "------------\n",
      "(train)\n",
      "------------\n",
      "epoch 187 || Epoch_D_Loss :0.0832 || Epoch_G_Loss :0.1260\n",
      "timer : 2.8129 sec.\n",
      "------------\n",
      "Epoch : 188/300\n",
      "------------\n",
      "(train)\n",
      "------------\n",
      "epoch 188 || Epoch_D_Loss :0.0692 || Epoch_G_Loss :0.1299\n",
      "timer : 2.8179 sec.\n",
      "------------\n",
      "Epoch : 189/300\n",
      "------------\n",
      "(train)\n",
      "------------\n",
      "epoch 189 || Epoch_D_Loss :0.0759 || Epoch_G_Loss :0.1388\n",
      "timer : 2.8086 sec.\n",
      "------------\n",
      "Epoch : 190/300\n",
      "------------\n",
      "(train)\n",
      "------------\n",
      "epoch 190 || Epoch_D_Loss :0.0737 || Epoch_G_Loss :0.1243\n",
      "timer : 2.7968 sec.\n",
      "------------\n",
      "Epoch : 191/300\n",
      "------------\n",
      "(train)\n",
      "------------\n",
      "epoch 191 || Epoch_D_Loss :0.0741 || Epoch_G_Loss :0.1321\n",
      "timer : 2.8168 sec.\n",
      "------------\n",
      "Epoch : 192/300\n",
      "------------\n",
      "(train)\n",
      "------------\n",
      "epoch 192 || Epoch_D_Loss :0.0614 || Epoch_G_Loss :0.1506\n",
      "timer : 2.8151 sec.\n",
      "------------\n",
      "Epoch : 193/300\n",
      "------------\n",
      "(train)\n",
      "------------\n",
      "epoch 193 || Epoch_D_Loss :0.0764 || Epoch_G_Loss :0.1306\n",
      "timer : 2.8145 sec.\n",
      "------------\n",
      "Epoch : 194/300\n",
      "------------\n",
      "(train)\n",
      "------------\n",
      "epoch 194 || Epoch_D_Loss :0.0728 || Epoch_G_Loss :0.1328\n",
      "timer : 2.8108 sec.\n",
      "------------\n",
      "Epoch : 195/300\n",
      "------------\n",
      "(train)\n",
      "------------\n",
      "epoch 195 || Epoch_D_Loss :0.0639 || Epoch_G_Loss :0.1281\n",
      "timer : 2.8188 sec.\n",
      "------------\n",
      "Epoch : 196/300\n",
      "------------\n",
      "(train)\n",
      "------------\n",
      "epoch 196 || Epoch_D_Loss :0.0669 || Epoch_G_Loss :0.1386\n",
      "timer : 2.8139 sec.\n",
      "------------\n",
      "Epoch : 197/300\n",
      "------------\n",
      "(train)\n",
      "------------\n",
      "epoch 197 || Epoch_D_Loss :0.0761 || Epoch_G_Loss :0.1397\n",
      "timer : 2.8128 sec.\n",
      "------------\n",
      "Epoch : 198/300\n",
      "------------\n",
      "(train)\n",
      "------------\n",
      "epoch 198 || Epoch_D_Loss :0.0689 || Epoch_G_Loss :0.1254\n",
      "timer : 2.8120 sec.\n",
      "------------\n",
      "Epoch : 199/300\n",
      "------------\n",
      "(train)\n",
      "------------\n",
      "epoch 199 || Epoch_D_Loss :0.0660 || Epoch_G_Loss :0.1185\n",
      "timer : 2.8135 sec.\n",
      "------------\n",
      "Epoch : 200/300\n",
      "------------\n",
      "(train)\n",
      "------------\n",
      "epoch 200 || Epoch_D_Loss :0.0756 || Epoch_G_Loss :0.1569\n",
      "timer : 2.8181 sec.\n",
      "------------\n",
      "Epoch : 201/300\n",
      "------------\n",
      "(train)\n",
      "------------\n",
      "epoch 201 || Epoch_D_Loss :0.0678 || Epoch_G_Loss :0.1196\n",
      "timer : 2.8239 sec.\n",
      "------------\n",
      "Epoch : 202/300\n",
      "------------\n",
      "(train)\n",
      "------------\n",
      "epoch 202 || Epoch_D_Loss :0.0484 || Epoch_G_Loss :0.1348\n",
      "timer : 2.8108 sec.\n",
      "------------\n",
      "Epoch : 203/300\n",
      "------------\n",
      "(train)\n",
      "------------\n",
      "epoch 203 || Epoch_D_Loss :0.0849 || Epoch_G_Loss :0.1354\n",
      "timer : 2.8122 sec.\n",
      "------------\n",
      "Epoch : 204/300\n",
      "------------\n",
      "(train)\n",
      "------------\n",
      "epoch 204 || Epoch_D_Loss :0.0639 || Epoch_G_Loss :0.1588\n",
      "timer : 2.8159 sec.\n",
      "------------\n",
      "Epoch : 205/300\n",
      "------------\n",
      "(train)\n",
      "------------\n",
      "epoch 205 || Epoch_D_Loss :0.0598 || Epoch_G_Loss :0.1469\n",
      "timer : 2.8071 sec.\n",
      "------------\n",
      "Epoch : 206/300\n",
      "------------\n",
      "(train)\n",
      "------------\n",
      "epoch 206 || Epoch_D_Loss :0.0581 || Epoch_G_Loss :0.1495\n",
      "timer : 2.8020 sec.\n",
      "------------\n",
      "Epoch : 207/300\n",
      "------------\n",
      "(train)\n",
      "------------\n",
      "epoch 207 || Epoch_D_Loss :0.0573 || Epoch_G_Loss :0.1530\n",
      "timer : 2.8061 sec.\n",
      "------------\n",
      "Epoch : 208/300\n",
      "------------\n",
      "(train)\n",
      "------------\n",
      "epoch 208 || Epoch_D_Loss :0.0604 || Epoch_G_Loss :0.1497\n",
      "timer : 2.8094 sec.\n",
      "------------\n",
      "Epoch : 209/300\n",
      "------------\n",
      "(train)\n",
      "------------\n",
      "epoch 209 || Epoch_D_Loss :0.0687 || Epoch_G_Loss :0.1408\n",
      "timer : 2.8075 sec.\n",
      "------------\n",
      "Epoch : 210/300\n",
      "------------\n",
      "(train)\n",
      "------------\n",
      "epoch 210 || Epoch_D_Loss :0.0494 || Epoch_G_Loss :0.1454\n",
      "timer : 2.8039 sec.\n",
      "------------\n",
      "Epoch : 211/300\n",
      "------------\n",
      "(train)\n",
      "------------\n",
      "epoch 211 || Epoch_D_Loss :0.0641 || Epoch_G_Loss :0.1631\n",
      "timer : 2.8114 sec.\n",
      "------------\n",
      "Epoch : 212/300\n",
      "------------\n",
      "(train)\n",
      "------------\n",
      "epoch 212 || Epoch_D_Loss :0.0504 || Epoch_G_Loss :0.1683\n",
      "timer : 2.8111 sec.\n",
      "------------\n",
      "Epoch : 213/300\n",
      "------------\n",
      "(train)\n",
      "------------\n",
      "epoch 213 || Epoch_D_Loss :0.0602 || Epoch_G_Loss :0.1413\n",
      "timer : 2.8171 sec.\n",
      "------------\n",
      "Epoch : 214/300\n",
      "------------\n",
      "(train)\n",
      "------------\n",
      "epoch 214 || Epoch_D_Loss :0.0582 || Epoch_G_Loss :0.1681\n",
      "timer : 2.8228 sec.\n",
      "------------\n",
      "Epoch : 215/300\n",
      "------------\n",
      "(train)\n",
      "------------\n",
      "epoch 215 || Epoch_D_Loss :0.0532 || Epoch_G_Loss :0.1703\n",
      "timer : 2.8083 sec.\n",
      "------------\n",
      "Epoch : 216/300\n",
      "------------\n",
      "(train)\n",
      "------------\n",
      "epoch 216 || Epoch_D_Loss :0.0713 || Epoch_G_Loss :0.1581\n",
      "timer : 2.8019 sec.\n",
      "------------\n",
      "Epoch : 217/300\n",
      "------------\n",
      "(train)\n",
      "------------\n",
      "epoch 217 || Epoch_D_Loss :0.0658 || Epoch_G_Loss :0.1512\n",
      "timer : 2.8127 sec.\n",
      "------------\n",
      "Epoch : 218/300\n",
      "------------\n",
      "(train)\n",
      "------------\n",
      "epoch 218 || Epoch_D_Loss :0.0564 || Epoch_G_Loss :0.1563\n",
      "timer : 2.8157 sec.\n",
      "------------\n",
      "Epoch : 219/300\n",
      "------------\n",
      "(train)\n",
      "------------\n",
      "epoch 219 || Epoch_D_Loss :0.0566 || Epoch_G_Loss :0.1489\n",
      "timer : 2.8031 sec.\n",
      "------------\n",
      "Epoch : 220/300\n",
      "------------\n",
      "(train)\n",
      "------------\n",
      "epoch 220 || Epoch_D_Loss :0.0599 || Epoch_G_Loss :0.1592\n",
      "timer : 2.8089 sec.\n",
      "------------\n",
      "Epoch : 221/300\n",
      "------------\n",
      "(train)\n",
      "------------\n",
      "epoch 221 || Epoch_D_Loss :0.0583 || Epoch_G_Loss :0.1447\n",
      "timer : 2.8097 sec.\n",
      "------------\n",
      "Epoch : 222/300\n",
      "------------\n",
      "(train)\n",
      "------------\n",
      "epoch 222 || Epoch_D_Loss :0.0474 || Epoch_G_Loss :0.1679\n",
      "timer : 2.8064 sec.\n",
      "------------\n",
      "Epoch : 223/300\n",
      "------------\n",
      "(train)\n",
      "------------\n",
      "epoch 223 || Epoch_D_Loss :0.0631 || Epoch_G_Loss :0.1655\n",
      "timer : 2.8083 sec.\n",
      "------------\n",
      "Epoch : 224/300\n",
      "------------\n",
      "(train)\n",
      "------------\n",
      "epoch 224 || Epoch_D_Loss :0.0495 || Epoch_G_Loss :0.1656\n",
      "timer : 2.8068 sec.\n",
      "------------\n",
      "Epoch : 225/300\n",
      "------------\n",
      "(train)\n",
      "------------\n",
      "epoch 225 || Epoch_D_Loss :0.0408 || Epoch_G_Loss :0.1863\n",
      "timer : 2.8055 sec.\n",
      "------------\n",
      "Epoch : 226/300\n",
      "------------\n",
      "(train)\n",
      "------------\n",
      "epoch 226 || Epoch_D_Loss :0.0691 || Epoch_G_Loss :0.1370\n",
      "timer : 2.8153 sec.\n",
      "------------\n",
      "Epoch : 227/300\n",
      "------------\n",
      "(train)\n",
      "------------\n",
      "epoch 227 || Epoch_D_Loss :0.0409 || Epoch_G_Loss :0.1724\n",
      "timer : 2.8080 sec.\n",
      "------------\n",
      "Epoch : 228/300\n",
      "------------\n",
      "(train)\n",
      "------------\n",
      "epoch 228 || Epoch_D_Loss :0.0584 || Epoch_G_Loss :0.1670\n",
      "timer : 2.8178 sec.\n",
      "------------\n",
      "Epoch : 229/300\n",
      "------------\n",
      "(train)\n",
      "------------\n",
      "epoch 229 || Epoch_D_Loss :0.0553 || Epoch_G_Loss :0.1590\n",
      "timer : 2.8073 sec.\n",
      "------------\n",
      "Epoch : 230/300\n",
      "------------\n",
      "(train)\n",
      "------------\n",
      "epoch 230 || Epoch_D_Loss :0.0602 || Epoch_G_Loss :0.1606\n",
      "timer : 2.8097 sec.\n",
      "------------\n",
      "Epoch : 231/300\n",
      "------------\n",
      "(train)\n",
      "------------\n",
      "epoch 231 || Epoch_D_Loss :0.0489 || Epoch_G_Loss :0.1600\n",
      "timer : 2.8067 sec.\n",
      "------------\n",
      "Epoch : 232/300\n",
      "------------\n",
      "(train)\n",
      "------------\n",
      "epoch 232 || Epoch_D_Loss :0.0498 || Epoch_G_Loss :0.1698\n",
      "timer : 2.8143 sec.\n",
      "------------\n",
      "Epoch : 233/300\n",
      "------------\n",
      "(train)\n",
      "------------\n",
      "epoch 233 || Epoch_D_Loss :0.0491 || Epoch_G_Loss :0.1833\n",
      "timer : 2.8123 sec.\n",
      "------------\n",
      "Epoch : 234/300\n",
      "------------\n",
      "(train)\n",
      "------------\n",
      "epoch 234 || Epoch_D_Loss :0.0458 || Epoch_G_Loss :0.1645\n",
      "timer : 2.8138 sec.\n",
      "------------\n",
      "Epoch : 235/300\n",
      "------------\n",
      "(train)\n",
      "------------\n",
      "epoch 235 || Epoch_D_Loss :0.0520 || Epoch_G_Loss :0.1746\n",
      "timer : 2.8115 sec.\n",
      "------------\n",
      "Epoch : 236/300\n",
      "------------\n",
      "(train)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------\n",
      "epoch 236 || Epoch_D_Loss :0.0399 || Epoch_G_Loss :0.1892\n",
      "timer : 2.8003 sec.\n",
      "------------\n",
      "Epoch : 237/300\n",
      "------------\n",
      "(train)\n",
      "------------\n",
      "epoch 237 || Epoch_D_Loss :0.0524 || Epoch_G_Loss :0.1595\n",
      "timer : 2.8217 sec.\n",
      "------------\n",
      "Epoch : 238/300\n",
      "------------\n",
      "(train)\n",
      "------------\n",
      "epoch 238 || Epoch_D_Loss :0.0461 || Epoch_G_Loss :0.1853\n",
      "timer : 2.8135 sec.\n",
      "------------\n",
      "Epoch : 239/300\n",
      "------------\n",
      "(train)\n",
      "------------\n",
      "epoch 239 || Epoch_D_Loss :0.0466 || Epoch_G_Loss :0.1590\n",
      "timer : 2.8033 sec.\n",
      "------------\n",
      "Epoch : 240/300\n",
      "------------\n",
      "(train)\n",
      "------------\n",
      "epoch 240 || Epoch_D_Loss :0.0570 || Epoch_G_Loss :0.1766\n",
      "timer : 2.8061 sec.\n",
      "------------\n",
      "Epoch : 241/300\n",
      "------------\n",
      "(train)\n",
      "------------\n",
      "epoch 241 || Epoch_D_Loss :0.0450 || Epoch_G_Loss :0.1800\n",
      "timer : 2.8160 sec.\n",
      "------------\n",
      "Epoch : 242/300\n",
      "------------\n",
      "(train)\n",
      "------------\n",
      "epoch 242 || Epoch_D_Loss :0.0382 || Epoch_G_Loss :0.1854\n",
      "timer : 2.7977 sec.\n",
      "------------\n",
      "Epoch : 243/300\n",
      "------------\n",
      "(train)\n",
      "------------\n",
      "epoch 243 || Epoch_D_Loss :0.0516 || Epoch_G_Loss :0.1703\n",
      "timer : 2.8143 sec.\n",
      "------------\n",
      "Epoch : 244/300\n",
      "------------\n",
      "(train)\n",
      "------------\n",
      "epoch 244 || Epoch_D_Loss :0.0451 || Epoch_G_Loss :0.2021\n",
      "timer : 2.8130 sec.\n",
      "------------\n",
      "Epoch : 245/300\n",
      "------------\n",
      "(train)\n",
      "------------\n",
      "epoch 245 || Epoch_D_Loss :0.0372 || Epoch_G_Loss :0.1819\n",
      "timer : 2.7972 sec.\n",
      "------------\n",
      "Epoch : 246/300\n",
      "------------\n",
      "(train)\n",
      "------------\n",
      "epoch 246 || Epoch_D_Loss :0.0412 || Epoch_G_Loss :0.1967\n",
      "timer : 2.7956 sec.\n",
      "------------\n",
      "Epoch : 247/300\n",
      "------------\n",
      "(train)\n",
      "------------\n",
      "epoch 247 || Epoch_D_Loss :0.0460 || Epoch_G_Loss :0.1746\n",
      "timer : 2.8149 sec.\n",
      "------------\n",
      "Epoch : 248/300\n",
      "------------\n",
      "(train)\n",
      "------------\n",
      "epoch 248 || Epoch_D_Loss :0.0493 || Epoch_G_Loss :0.1796\n",
      "timer : 2.8033 sec.\n",
      "------------\n",
      "Epoch : 249/300\n",
      "------------\n",
      "(train)\n",
      "------------\n",
      "epoch 249 || Epoch_D_Loss :0.0408 || Epoch_G_Loss :0.2026\n",
      "timer : 2.8055 sec.\n",
      "------------\n",
      "Epoch : 250/300\n",
      "------------\n",
      "(train)\n",
      "------------\n",
      "epoch 250 || Epoch_D_Loss :0.0467 || Epoch_G_Loss :0.1641\n",
      "timer : 2.8176 sec.\n",
      "------------\n",
      "Epoch : 251/300\n",
      "------------\n",
      "(train)\n",
      "------------\n",
      "epoch 251 || Epoch_D_Loss :0.0428 || Epoch_G_Loss :0.1887\n",
      "timer : 2.8114 sec.\n",
      "------------\n",
      "Epoch : 252/300\n",
      "------------\n",
      "(train)\n",
      "------------\n",
      "epoch 252 || Epoch_D_Loss :0.0446 || Epoch_G_Loss :0.1619\n",
      "timer : 2.8062 sec.\n",
      "------------\n",
      "Epoch : 253/300\n",
      "------------\n",
      "(train)\n",
      "------------\n",
      "epoch 253 || Epoch_D_Loss :0.0288 || Epoch_G_Loss :0.1893\n",
      "timer : 2.8115 sec.\n",
      "------------\n",
      "Epoch : 254/300\n",
      "------------\n",
      "(train)\n",
      "------------\n",
      "epoch 254 || Epoch_D_Loss :0.0522 || Epoch_G_Loss :0.1872\n",
      "timer : 2.8144 sec.\n",
      "------------\n",
      "Epoch : 255/300\n",
      "------------\n",
      "(train)\n",
      "------------\n",
      "epoch 255 || Epoch_D_Loss :0.0408 || Epoch_G_Loss :0.1818\n",
      "timer : 2.8061 sec.\n",
      "------------\n",
      "Epoch : 256/300\n",
      "------------\n",
      "(train)\n",
      "------------\n",
      "epoch 256 || Epoch_D_Loss :0.0407 || Epoch_G_Loss :0.1915\n",
      "timer : 2.8186 sec.\n",
      "------------\n",
      "Epoch : 257/300\n",
      "------------\n",
      "(train)\n",
      "------------\n",
      "epoch 257 || Epoch_D_Loss :0.0297 || Epoch_G_Loss :0.1832\n",
      "timer : 2.8083 sec.\n",
      "------------\n",
      "Epoch : 258/300\n",
      "------------\n",
      "(train)\n",
      "------------\n",
      "epoch 258 || Epoch_D_Loss :0.0530 || Epoch_G_Loss :0.2025\n",
      "timer : 2.8137 sec.\n",
      "------------\n",
      "Epoch : 259/300\n",
      "------------\n",
      "(train)\n",
      "------------\n",
      "epoch 259 || Epoch_D_Loss :0.0373 || Epoch_G_Loss :0.1722\n",
      "timer : 2.8075 sec.\n",
      "------------\n",
      "Epoch : 260/300\n",
      "------------\n",
      "(train)\n",
      "------------\n",
      "epoch 260 || Epoch_D_Loss :0.0444 || Epoch_G_Loss :0.1976\n",
      "timer : 2.8032 sec.\n",
      "------------\n",
      "Epoch : 261/300\n",
      "------------\n",
      "(train)\n",
      "------------\n",
      "epoch 261 || Epoch_D_Loss :0.0394 || Epoch_G_Loss :0.1816\n",
      "timer : 2.8070 sec.\n",
      "------------\n",
      "Epoch : 262/300\n",
      "------------\n",
      "(train)\n",
      "------------\n",
      "epoch 262 || Epoch_D_Loss :0.0349 || Epoch_G_Loss :0.1908\n",
      "timer : 2.8126 sec.\n",
      "------------\n",
      "Epoch : 263/300\n",
      "------------\n",
      "(train)\n",
      "------------\n",
      "epoch 263 || Epoch_D_Loss :0.0329 || Epoch_G_Loss :0.1921\n",
      "timer : 2.8130 sec.\n",
      "------------\n",
      "Epoch : 264/300\n",
      "------------\n",
      "(train)\n",
      "------------\n",
      "epoch 264 || Epoch_D_Loss :0.0436 || Epoch_G_Loss :0.1835\n",
      "timer : 2.8090 sec.\n",
      "------------\n",
      "Epoch : 265/300\n",
      "------------\n",
      "(train)\n",
      "------------\n",
      "epoch 265 || Epoch_D_Loss :0.0268 || Epoch_G_Loss :0.1880\n",
      "timer : 2.8003 sec.\n",
      "------------\n",
      "Epoch : 266/300\n",
      "------------\n",
      "(train)\n",
      "------------\n",
      "epoch 266 || Epoch_D_Loss :0.0496 || Epoch_G_Loss :0.1943\n",
      "timer : 2.8129 sec.\n",
      "------------\n",
      "Epoch : 267/300\n",
      "------------\n",
      "(train)\n",
      "------------\n",
      "epoch 267 || Epoch_D_Loss :0.0243 || Epoch_G_Loss :0.2079\n",
      "timer : 2.7957 sec.\n",
      "------------\n",
      "Epoch : 268/300\n",
      "------------\n",
      "(train)\n",
      "------------\n",
      "epoch 268 || Epoch_D_Loss :0.0364 || Epoch_G_Loss :0.2165\n",
      "timer : 2.8105 sec.\n",
      "------------\n",
      "Epoch : 269/300\n",
      "------------\n",
      "(train)\n",
      "------------\n",
      "epoch 269 || Epoch_D_Loss :0.0324 || Epoch_G_Loss :0.1874\n",
      "timer : 2.8062 sec.\n",
      "------------\n",
      "Epoch : 270/300\n",
      "------------\n",
      "(train)\n",
      "------------\n",
      "epoch 270 || Epoch_D_Loss :0.0359 || Epoch_G_Loss :0.2044\n",
      "timer : 2.8102 sec.\n",
      "------------\n",
      "Epoch : 271/300\n",
      "------------\n",
      "(train)\n",
      "------------\n",
      "epoch 271 || Epoch_D_Loss :0.0309 || Epoch_G_Loss :0.1995\n",
      "timer : 2.8042 sec.\n",
      "------------\n",
      "Epoch : 272/300\n",
      "------------\n",
      "(train)\n",
      "------------\n",
      "epoch 272 || Epoch_D_Loss :0.0389 || Epoch_G_Loss :0.1919\n",
      "timer : 2.8081 sec.\n",
      "------------\n",
      "Epoch : 273/300\n",
      "------------\n",
      "(train)\n",
      "------------\n",
      "epoch 273 || Epoch_D_Loss :0.0385 || Epoch_G_Loss :0.2034\n",
      "timer : 2.7991 sec.\n",
      "------------\n",
      "Epoch : 274/300\n",
      "------------\n",
      "(train)\n",
      "------------\n",
      "epoch 274 || Epoch_D_Loss :0.0242 || Epoch_G_Loss :0.2238\n",
      "timer : 2.7942 sec.\n",
      "------------\n",
      "Epoch : 275/300\n",
      "------------\n",
      "(train)\n",
      "------------\n",
      "epoch 275 || Epoch_D_Loss :0.0436 || Epoch_G_Loss :0.1836\n",
      "timer : 2.8151 sec.\n",
      "------------\n",
      "Epoch : 276/300\n",
      "------------\n",
      "(train)\n",
      "------------\n",
      "epoch 276 || Epoch_D_Loss :0.0377 || Epoch_G_Loss :0.2116\n",
      "timer : 2.8103 sec.\n",
      "------------\n",
      "Epoch : 277/300\n",
      "------------\n",
      "(train)\n",
      "------------\n",
      "epoch 277 || Epoch_D_Loss :0.0263 || Epoch_G_Loss :0.2189\n",
      "timer : 2.8033 sec.\n",
      "------------\n",
      "Epoch : 278/300\n",
      "------------\n",
      "(train)\n",
      "------------\n",
      "epoch 278 || Epoch_D_Loss :0.0432 || Epoch_G_Loss :0.1893\n",
      "timer : 2.8016 sec.\n",
      "------------\n",
      "Epoch : 279/300\n",
      "------------\n",
      "(train)\n",
      "------------\n",
      "epoch 279 || Epoch_D_Loss :0.0299 || Epoch_G_Loss :0.1963\n",
      "timer : 2.8044 sec.\n",
      "------------\n",
      "Epoch : 280/300\n",
      "------------\n",
      "(train)\n",
      "------------\n",
      "epoch 280 || Epoch_D_Loss :0.0323 || Epoch_G_Loss :0.1987\n",
      "timer : 2.8052 sec.\n",
      "------------\n",
      "Epoch : 281/300\n",
      "------------\n",
      "(train)\n",
      "------------\n",
      "epoch 281 || Epoch_D_Loss :0.0403 || Epoch_G_Loss :0.2141\n",
      "timer : 2.8046 sec.\n",
      "------------\n",
      "Epoch : 282/300\n",
      "------------\n",
      "(train)\n",
      "------------\n",
      "epoch 282 || Epoch_D_Loss :0.0300 || Epoch_G_Loss :0.2282\n",
      "timer : 2.8116 sec.\n",
      "------------\n",
      "Epoch : 283/300\n",
      "------------\n",
      "(train)\n",
      "------------\n",
      "epoch 283 || Epoch_D_Loss :0.0245 || Epoch_G_Loss :0.2231\n",
      "timer : 2.8014 sec.\n",
      "------------\n",
      "Epoch : 284/300\n",
      "------------\n",
      "(train)\n",
      "------------\n",
      "epoch 284 || Epoch_D_Loss :0.0327 || Epoch_G_Loss :0.2146\n",
      "timer : 2.7890 sec.\n",
      "------------\n",
      "Epoch : 285/300\n",
      "------------\n",
      "(train)\n",
      "------------\n",
      "epoch 285 || Epoch_D_Loss :0.0272 || Epoch_G_Loss :0.2050\n",
      "timer : 2.8065 sec.\n",
      "------------\n",
      "Epoch : 286/300\n",
      "------------\n",
      "(train)\n",
      "------------\n",
      "epoch 286 || Epoch_D_Loss :0.0276 || Epoch_G_Loss :0.2182\n",
      "timer : 2.8017 sec.\n",
      "------------\n",
      "Epoch : 287/300\n",
      "------------\n",
      "(train)\n",
      "------------\n",
      "epoch 287 || Epoch_D_Loss :0.0385 || Epoch_G_Loss :0.2235\n",
      "timer : 2.8089 sec.\n",
      "------------\n",
      "Epoch : 288/300\n",
      "------------\n",
      "(train)\n",
      "------------\n",
      "epoch 288 || Epoch_D_Loss :0.0207 || Epoch_G_Loss :0.2213\n",
      "timer : 2.8023 sec.\n",
      "------------\n",
      "Epoch : 289/300\n",
      "------------\n",
      "(train)\n",
      "------------\n",
      "epoch 289 || Epoch_D_Loss :0.0174 || Epoch_G_Loss :0.2095\n",
      "timer : 2.8026 sec.\n",
      "------------\n",
      "Epoch : 290/300\n",
      "------------\n",
      "(train)\n",
      "------------\n",
      "epoch 290 || Epoch_D_Loss :0.0472 || Epoch_G_Loss :0.2297\n",
      "timer : 2.8033 sec.\n",
      "------------\n",
      "Epoch : 291/300\n",
      "------------\n",
      "(train)\n",
      "------------\n",
      "epoch 291 || Epoch_D_Loss :0.0230 || Epoch_G_Loss :0.1942\n",
      "timer : 2.8055 sec.\n",
      "------------\n",
      "Epoch : 292/300\n",
      "------------\n",
      "(train)\n",
      "------------\n",
      "epoch 292 || Epoch_D_Loss :0.0240 || Epoch_G_Loss :0.2126\n",
      "timer : 2.7979 sec.\n",
      "------------\n",
      "Epoch : 293/300\n",
      "------------\n",
      "(train)\n",
      "------------\n",
      "epoch 293 || Epoch_D_Loss :0.0436 || Epoch_G_Loss :0.2401\n",
      "timer : 2.8093 sec.\n",
      "------------\n",
      "Epoch : 294/300\n",
      "------------\n",
      "(train)\n",
      "------------\n",
      "epoch 294 || Epoch_D_Loss :0.0161 || Epoch_G_Loss :0.2060\n",
      "timer : 2.7941 sec.\n",
      "------------\n",
      "Epoch : 295/300\n",
      "------------\n",
      "(train)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------\n",
      "epoch 295 || Epoch_D_Loss :0.0343 || Epoch_G_Loss :0.2405\n",
      "timer : 2.7992 sec.\n",
      "------------\n",
      "Epoch : 296/300\n",
      "------------\n",
      "(train)\n",
      "------------\n",
      "epoch 296 || Epoch_D_Loss :0.0361 || Epoch_G_Loss :0.2018\n",
      "timer : 2.8058 sec.\n",
      "------------\n",
      "Epoch : 297/300\n",
      "------------\n",
      "(train)\n",
      "------------\n",
      "epoch 297 || Epoch_D_Loss :0.0254 || Epoch_G_Loss :0.2321\n",
      "timer : 2.8023 sec.\n",
      "------------\n",
      "Epoch : 298/300\n",
      "------------\n",
      "(train)\n",
      "------------\n",
      "epoch 298 || Epoch_D_Loss :0.0263 || Epoch_G_Loss :0.2363\n",
      "timer : 2.7992 sec.\n",
      "------------\n",
      "Epoch : 299/300\n",
      "------------\n",
      "(train)\n",
      "------------\n",
      "epoch 299 || Epoch_D_Loss :0.0190 || Epoch_G_Loss :0.2204\n",
      "timer : 2.7956 sec.\n"
     ]
    }
   ],
   "source": [
    "# 学習・検証の実施\n",
    "num_epochs = 300\n",
    "G, D = train_model(\n",
    "    G, D, dataloader=train_dataloader, num_epochs=num_epochs)\n",
    "\n",
    "# Discriminatorのlossが最初から０になっている -> g_lossの式にマイナスを付け忘れていたため"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Invalid shape (1, 64, 64) for image data",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-c69c9ac3ddbb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;31m# 下段に生成データ\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfake_images\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'gray'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36mimshow\u001b[0;34m(X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, shape, filternorm, filterrad, imlim, resample, url, data, **kwargs)\u001b[0m\n\u001b[1;32m   2682\u001b[0m         \u001b[0mfilternorm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfilternorm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilterrad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfilterrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimlim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimlim\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2683\u001b[0m         resample=resample, url=url, **({\"data\": data} if data is not\n\u001b[0;32m-> 2684\u001b[0;31m         None else {}), **kwargs)\n\u001b[0m\u001b[1;32m   2685\u001b[0m     \u001b[0msci\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__ret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2686\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m__ret\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/matplotlib/__init__.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(ax, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1597\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1598\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1599\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msanitize_sequence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1600\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1601\u001b[0m         \u001b[0mbound\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_sig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/matplotlib/cbook/deprecation.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    367\u001b[0m                 \u001b[0;34mf\"%(removal)s.  If any parameter follows {name!r}, they \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    368\u001b[0m                 f\"should be pass as keyword, not positionally.\")\n\u001b[0;32m--> 369\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    370\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    371\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/matplotlib/cbook/deprecation.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    367\u001b[0m                 \u001b[0;34mf\"%(removal)s.  If any parameter follows {name!r}, they \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    368\u001b[0m                 f\"should be pass as keyword, not positionally.\")\n\u001b[0;32m--> 369\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    370\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    371\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/matplotlib/axes/_axes.py\u001b[0m in \u001b[0;36mimshow\u001b[0;34m(self, X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, shape, filternorm, filterrad, imlim, resample, url, **kwargs)\u001b[0m\n\u001b[1;32m   5677\u001b[0m                               resample=resample, **kwargs)\n\u001b[1;32m   5678\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5679\u001b[0;31m         \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5680\u001b[0m         \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_alpha\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5681\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_clip_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/matplotlib/image.py\u001b[0m in \u001b[0;36mset_data\u001b[0;34m(self, A)\u001b[0m\n\u001b[1;32m    688\u001b[0m                 or self._A.ndim == 3 and self._A.shape[-1] in [3, 4]):\n\u001b[1;32m    689\u001b[0m             raise TypeError(\"Invalid shape {} for image data\"\n\u001b[0;32m--> 690\u001b[0;31m                             .format(self._A.shape))\n\u001b[0m\u001b[1;32m    691\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    692\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_A\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Invalid shape (1, 64, 64) for image data"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x432 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 生成画像と訓練データを可視化する\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 入力の乱数生成\n",
    "batch_size = 8\n",
    "z_dim = 20\n",
    "fixed_z = torch.randn(batch_size, z_dim)\n",
    "fixed_z = fixed_z.view(fixed_z.size(0), fixed_z.size(1), 1, 1)\n",
    "\n",
    "# 画像生成\n",
    "G.eval()\n",
    "fake_images = G(fixed_z.to(device))\n",
    "\n",
    "# 訓練データ\n",
    "batch_iterator = iter(train_dataloader)\n",
    "images = next(batch_iterator)\n",
    "\n",
    "# 出力\n",
    "fig = plt.figure(figsize=(15, 6))\n",
    "for i in range(0,5):\n",
    "    # 上段に訓練データ\n",
    "    plt.subplot(2, 5, i+1)\n",
    "    plt.imshow(images[i][0].cpu().detach().numpy(), 'gray')\n",
    "    \n",
    "    # 下段に生成データ\n",
    "    plt.subplot(2, 5, i+6)\n",
    "    plt.imshow(fake_images[i][0].cpu().detach().numpy(), 'gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# attention mapの表示\n",
    "fig = plt.figure(figsize=(15, 6))\n",
    "\n",
    "for i in range(0,5):\n",
    "    # 上段に生成したデータを\n",
    "    plt.subplot(2, 5, i+1)\n",
    "    plt.imshow(fake_images[i][0].cpu().detach().numpy(), 'gray')\n",
    "    \n",
    "    # 下段にattention map１の画像中央のピクセルのデータを\n",
    "    plt.subplot(2, 5, i+6)\n",
    "    am = am[i].view(16,16,16)\n",
    "    am = am[7][7]  # 中央に着目\n",
    "    plt.imshow(am.cpu().detach().numpy(), 'Reds')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
