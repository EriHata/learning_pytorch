{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERTを用いたレビュー文章に対する感情分析モデルの実装と学習・推論\n",
    "\n",
    "BERTを使用し、IMDbデータのポジ・ネガを分類するモデルを学習させ、推論する。<br>\n",
    "また推論時のSelf-Attentionを可視化する。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 学習目標\n",
    "\n",
    "1.\tBERTのボキャブラリーをtorchtextで使用する実装方法を理解する\n",
    "2.\tBERTに分類タスク用のアダプターモジュールを追加し、感情分析を実施するモデルを実装できる\n",
    "3.\tBERTをファインチューニングして、モデルを学習できる\n",
    "4.  BERTのSelf-Attentionの重みを可視化し、推論の説明を試みることができる"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "dlopen(/Users/eri/opt/anaconda3/lib/python3.7/site-packages/torchtext/_torchtext.so, 6): Library not loaded: @rpath/libtorch_cpu.dylib\n  Referenced from: /Users/eri/opt/anaconda3/lib/python3.7/site-packages/torchtext/_torchtext.so\n  Reason: image not found",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-df1b23a892ec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorchtext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/torchtext/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m \u001b[0m_init_extension\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/torchtext/__init__.py\u001b[0m in \u001b[0;36m_init_extension\u001b[0;34m()\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mext_specs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"torchtext C++ Extension is not found.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_library\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mext_specs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morigin\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_library\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mext_specs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morigin\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/torch/_ops.py\u001b[0m in \u001b[0;36mload_library\u001b[0;34m(self, path)\u001b[0m\n\u001b[1;32m    104\u001b[0m             \u001b[0;31m# static (global) initialization code in order to register custom\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m             \u001b[0;31m# operators with the JIT.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m             \u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCDLL\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloaded_libraries\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/ctypes/__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode, handle, use_errno, use_last_error)\u001b[0m\n\u001b[1;32m    362\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    363\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 364\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_dlopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    365\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    366\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: dlopen(/Users/eri/opt/anaconda3/lib/python3.7/site-packages/torchtext/_torchtext.so, 6): Library not loaded: @rpath/libtorch_cpu.dylib\n  Referenced from: /Users/eri/opt/anaconda3/lib/python3.7/site-packages/torchtext/_torchtext.so\n  Reason: image not found"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import time\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "import torchtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 乱数のシードを設定\n",
    "torch.manual_seed(1234)\n",
    "np.random.seed(1234)\n",
    "random.seed(1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMDbデータを読み込み、DataLoaderを作成（BERTのTokenizerを使用）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 前処理と単語分割をまとめた関数\n",
    "\n",
    "import re\n",
    "import string\n",
    "from utils.bert import BertTokenizer\n",
    "\n",
    "def preprocessing_text(text):\n",
    "    '''\n",
    "    IMDbの前処理\n",
    "    '''\n",
    "    # 改行コードを消去\n",
    "    text = re.sub('<br />', '', text)\n",
    "    \n",
    "    # カンマ、ピリオド以外の記号をスペースに置換\n",
    "    for p in string.punctuation:\n",
    "        if (p == '.') or (p == ','):\n",
    "            continue\n",
    "        else:\n",
    "            text = text.replace(p, ' ')\n",
    "            \n",
    "    # ピリオドとカンマの前後にスペースを入れておく\n",
    "    text = text.replace('.', ' . ')\n",
    "    text = text.replace(',', ' , ')\n",
    "    return text\n",
    "\n",
    "# 単語分割用のTokenizerを用意\n",
    "# 大文字は小文字に変換する\n",
    "tokenizer_bert = BertTokenizer(vocab_file='./vocab/bert-base-uncased-vocab.txt', do_lower_case=True)\n",
    "\n",
    "# 前処理と単語分割をまとめた関数\n",
    "# 単語分割の関数を渡すので、tokenizer_bertではなくtokenizer_bert.tokenizeを渡す！\n",
    "def tokenizer_with_preprocessing(text, tokenizer=tokenizer_bert.tokenize):\n",
    "    text = preprocessing_text(text)\n",
    "    ret = tokenizer(text)\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データを読み込んだときに行う処理\n",
    "max_length = 256\n",
    "TEXT = torchtext.data.Field(sequential=True, tokenize=tokenizer_with_preprocessing, use_vocab=True,\n",
    "                           lower=True, include_lengths=True, batch_first=True, fix_lnegth=max_length)\n",
    "LABEL = torchtext.data.Field(sequential=False, use_vocab=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 各tsvファイルの読み込み\n",
    "train_val_ds, test_ds = torchtext.data.TabularDataset.splits(\n",
    "    path='./data/',\n",
    "    train='IMDb_train.tsv',\n",
    "    test='IMDb_test.tsv',\n",
    "    format='tsv',\n",
    "    fields=[('Text', TEXT), ('Label', LABEL)])\n",
    "\n",
    "# torchtext.data.Datasetのsplit関数で訓練データとvalidationデータを分ける\n",
    "train_ds, val_ds = train_val_ds.split(split_ratio=0.8, random_state=random.seed(1234))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bertのvocabraryは全単語を使用する\n",
    "# 訓練データからvocabraryは作成しない\n",
    "\n",
    "# bert用の単語辞書を辞書型変数に変換する\n",
    "from utils.bert import BertTokenizer, load_vocab\n",
    "\n",
    "vocab_bert, ids_to_tokens_bert = load_vocab(vocab_file='./vocab/bert-base-uncased-vocab.txt')\n",
    "\n",
    "# 一度bulild_vocabを実行しないとTEXTオブジェクトがvocabのメンバ変数をもってくれない,　エラーを吐く\n",
    "# 一度適当にbuild_vocabでボキャブラリーを作成してからBERTのボキャブラリーを上書きする\n",
    "TEXT.build_vocab(train_ds, min_freq=1)\n",
    "TEXT.vocab.stoi = vocab_bert\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataLoaderを作成する\n",
    "batch_size = 32  # bertでは16, 32あたりを使う\n",
    "\n",
    "train_dl = torchtext.data.Iterator(train_ds, batch_size=batch_size, train=True)\n",
    "val_dl = torchtext.data.Iterator(val_ds, batch_size=batch_size, train=False, sort=False)\n",
    "test_dl = torchtext.data.Iterator(test_ds, batch_size=batch_size, train=False, sort=False)\n",
    "\n",
    "\n",
    "# 辞書オブジェクトにまとめる\n",
    "dataloaders_dict = {'train': train_dl, 'val': val_dl}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 動作確認　検証データのデータセットで確認\n",
    "batch = next(iter(val_dl))\n",
    "print(batch.Text)\n",
    "print(batch.Label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ミニバッチの１文目を確認してみる\n",
    "text_minibatch_1 = (batch.Text[0][1]).numpy()  # なぜ[1]\n",
    "\n",
    "# IDを単語に戻す\n",
    "text = tokenize_bert.convert-_ids_to_tokens(text_minibatch_1)\n",
    "\n",
    "print(text)\n",
    "# サブワードで分割されていることがわかる"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 感情分析用のBERTモデルを構築"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.bert import get_config, BertModel, set_learned_params\n",
    "\n",
    "# モデルの設定のJSONファイルをオブジェクトとして読み込み\n",
    "config = get_config(file_path='./weights/pytorch_config.bin')\n",
    "# BERTの基本モデルを作成\n",
    "net_bert = BertModel(config)\n",
    "\n",
    "# BERTモデルに学習済みパラメータをセットする\n",
    "net_bert = set_learned_params(net_bert, weights_path='./weights/pytorch_model.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-b8357f800135>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mBertForIMDb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     '''\n\u001b[1;32m      3\u001b[0m     \u001b[0mBERTモデルにIMDｂのネガ\u001b[0m\u001b[0;31m・\u001b[0m\u001b[0mポジを判定する部分\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m全結合層\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0mをつなげたモデル\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     '''\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nn' is not defined"
     ]
    }
   ],
   "source": [
    "class BertForIMDb(nn.Module):\n",
    "    '''\n",
    "    BERTモデルにIMDｂのネガ・ポジを判定する部分(全結合層)をつなげたモデル\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, net_bert):\n",
    "        super(BertForIMDb, self).__init__()\n",
    "        \n",
    "        # bertモジュール\n",
    "        self.bert = net_bert\n",
    "        self.cls = nn.Linear(in_features=768, out_features=2)\n",
    "        \n",
    "        # 重み初期化\n",
    "        # モジュールによって場合分けが必要なければこんなふうにシンプルに書ける\n",
    "        nn.init.normal_(self.cls.weight, std=0.02)\n",
    "        nn.init.normal_(self.cls.bias, 0)\n",
    "        \n",
    "    def forward(self, input_ids, token_type_ids=None, attention_mask=None, output_all_encoded_layers=False, attention_show_flg=False):\n",
    "        '''\n",
    "        input_ids： [batch_size, sequence_length]の文章の単語IDの羅列\n",
    "        token_type_ids： [batch_size, sequence_length]の、各単語が1文目なのか、2文目なのかを示すid\n",
    "        attention_mask：Transformerのマスクと同じ働き\n",
    "        output_all_encoded_layers：最終出力に12段のTransformerの全部をリストで返すか、最後だけかを指定\n",
    "        attention_show_flg：Self-Attentionの重みを返すかのフラグ\n",
    "        '''\n",
    "        \n",
    "        # bertの基本モデル部分の順伝播\n",
    "        # 今回pooled_outputは使用しない\n",
    "        if attention_show_flg == True:\n",
    "            encoded_layers, pooled_output, attention_probs = self.bert(\n",
    "                input_ids, token_type_ids, attention_mask, output_all_encoded_layers, attention_show_flg)\n",
    "        elif attention_show_flg == False:\n",
    "            encoded_layers, pooled_output = self.bert(\n",
    "                input_ids, token_type_ids, attention_mask, output_all_encoded_layers, attention_show_flg)\n",
    "            \n",
    "        # 入力文章の１単語目[CLS]　の特徴量を使用してポジ・ネガを分類する\n",
    "        vec_0 = encoded_layers[:,0,:]\n",
    "        vec_0 = vec_0.view(-1, 768)  # [batch_size, hidden_size]\n",
    "        out = self.cls(vec_0)\n",
    "        \n",
    "        # attention_showのときはattention_probsもリターン\n",
    "        if attention_show_flg == True:\n",
    "            return out, attention_probs\n",
    "        elif attention_show_flg == False:\n",
    "            return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# モデル構築\n",
    "net = BertForIMDb(net_bert)\n",
    "net.train()\n",
    "print('ネットワーク設定完了')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERTのファインチューニングに向けた設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 勾配計算を最後のBertLayerモジュールと追加した分類アダプターのみ実行\n",
    "\n",
    "# 全てのモジュールを勾配計算Falseにする\n",
    "for name, param in net.named_parameters():\n",
    "    param.requires_grad = False\n",
    "    \n",
    "# 最後のBertLayerモジュールの勾配計算をTrueにする\n",
    "for name, param in net.bert.encoder.layer[-1].named_parameters():\n",
    "    param.requires_grad = True\n",
    "    \n",
    "# 識別器を勾配計算Trueに変更\n",
    "for name, param in net.cls.named_parameters():\n",
    "    param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 最適化手法の設定\n",
    "# bertの元の部分(識別機でない部分)はファインチューニング\n",
    "optimizer = optim.Adam([\n",
    "    {'params': net.bert.encoder.layer[-1].parameters(), 'lr': 5e-5},\n",
    "    {'params': net.cls.parameters(), 'lr': 5e-5}\n",
    "], betas=(0.9, 0.999))\n",
    "\n",
    "# 損失関数の設定\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 学習・検証の実施"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(net, dataloaders_dict, criterion, optimizer, num_epochs):\n",
    "    # GPUが使えるか確認\n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "    print('使用デバイス：', device)\n",
    "    print('-----start-----')\n",
    "    \n",
    "    net.to(device)\n",
    "    \n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    \n",
    "    batch_size = dataloaders_dict['train'].batch_size\n",
    "    \n",
    "    # epochのループ\n",
    "    for epoch in range(num_epochs):\n",
    "        # epochごとの訓練と検証のループ, モデルのモードを変更する\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                net.train()\n",
    "            else:\n",
    "                net.eval()\n",
    "                \n",
    "            epoch_loss = 0.0\n",
    "            epoch_corrects = 0  # 分類なので\n",
    "            iteration = 1\n",
    "\n",
    "            t_epoch_start = time.time()\n",
    "            t_iter_start = time.time()\n",
    "\n",
    "            # ミニバッチを取り出すループ\n",
    "            for batch in (dataloaders_dict[phase]):\n",
    "                # batchはTextとLabelの辞書オブジェクト\n",
    "\n",
    "                # データをデバイスに送る\n",
    "                inputs = batch.Text[0].to(device)\n",
    "                labels = batch.Label.to(device)\n",
    "\n",
    "                # optimizerの初期化  これはvalのときは必要なさそう\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                with torch.set_grad_enabled(phase=='train'):\n",
    "                    # BertForIMDbに入力\n",
    "                    outputs = net(inputs,  token_type_ids=None, attention_mask=None, \n",
    "                                 output_all_encoded_layers=False, attention_show_flg=False)\n",
    "\n",
    "                    loss = criterion(oututs, labels)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "\n",
    "                    # 訓練時はbackpropagation\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                        if (iteration%10 == 0):\n",
    "                            t_iter_finish = time.time()\n",
    "                            duration = t_iter_finish - t_iter_start\n",
    "                            acc = (torch.sum(preds==labels.data)).double()/batch_size\n",
    "                            print('イテレーション　{} || Loss: {:.4f} || 10iter: {:.4f} sec. || 本イテレーションの正解率: {}'.format(iteration, loss.item(), duration, acc))\n",
    "                            t_iter_start = time.time()\n",
    "\n",
    "                    iteration += 1\n",
    "                \n",
    "                    # 損失と正解数の合計を計算\n",
    "                    # lossは平均で返ってきているからbatch_sizeをかける\n",
    "                    # もともとのbatch＿sizeじゃなくてDataloaderから取ってきたbatch_sizeのほうがいいのでは？\n",
    "                    epoch_loss += loss.item()*batch_size  \n",
    "                    epoch_corrects += torch.sum(preds=labels.data)\n",
    "\n",
    "            # epochごとのlossと正解率\n",
    "            t_epoch_finish = time.time()\n",
    "            epoch_loss = epoch_loss / len(dataloaders_dict[phase].dataset)\n",
    "            epoch_acc = epoch_corrects.double() / len(dataloaders_dict[phase].dataset)\n",
    "            \n",
    "            print('Epoch{} / {} | {:.5f} | Loss: {:.4f} Acc: {:.4f}'.format(epoch+1, num_epochs, phase, epoch_loss, epoch_acc))\n",
    "            t_epoch_start = time.time()\n",
    "            \n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 学習・検証\n",
    "# 2epochで４０分ほど\n",
    "num_epochs = 2\n",
    "net_trained = train_model(net, dataloaders_dict, criterion, optimizer, num_epochs=num_epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 学習したパラメータを保存\n",
    "save_path = './weights/bert_fine/tuning_IMDb.pth'\n",
    "torch.save(net_trained.state_dict(), save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# テストデータでの正解率を求める\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "net_trained.eval()  # モデルを検証モードに\n",
    "net_trained.to(device)\n",
    "\n",
    "# epochの正解数を記録する変数\n",
    "epoch_corrects = 0\n",
    "\n",
    "for batch in tqdm(test_dl):  # testデータのDataLoader\n",
    "    # batchはTextとLabelの辞書オブジェクト\n",
    "\n",
    "    inputs = batch.Text[0].to(device)\n",
    "    labels = batch.Label.to(device)\n",
    "    \n",
    "    with torch.set_grad_enable(False):\n",
    "        # BertForIMDbに入力\n",
    "        outputs = net_trained(inputs, token_type_ids=None, attention_mask=None, output_all_encoded_layers=False, attention_show_flg=False)\n",
    "        loss = criterion(outputs, labels)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        epoch_corrects += torch.sum(preds==lables.data)  # すべてのデータに対して正解した合計を求めることになる\n",
    "        \n",
    "# 正解率\n",
    "epoch_acc = epoch_corrects.double() / len(test_dl.dataset)\n",
    "print('テストデータ{}個での正解率：{:.4f}'.format(len(test_dl.dataset), epoch_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attentionの可視化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_sizeを６４にしたテストデータでDataLoaderを作成\n",
    "batch_size = 64\n",
    "test_dl = torchtext.data.Iterator(test_ds, batch_size=batch_size, train=False, sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(test_dl))\n",
    "\n",
    "inputs = batch.Text[0].to(device)\n",
    "labels = batch.Label.to(device)\n",
    "\n",
    "outputs, attention_probs = net_trained(inputs, token_type_ids=None, attention_mask=None,\n",
    "                                      output_all_encoded_layers=False, attention_show_flg=True)\n",
    "\n",
    "_, preds = torch.max(outputs, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HTMLを作成する関数を実装\n",
    "def highlight(word, attn):\n",
    "    '''\n",
    "    Attentionの値が大きいと文字の背景が濃い赤になるhtmlを出力させる関数\n",
    "    '''\n",
    "    \n",
    "    html_color = '#%02X%02X%02X' % (255, int(255*(1 - attn)), int(255*(1 - attn)))\n",
    "    return '<span style=\"background-color: {}\"> {}</span>'.format(html_color, word)\n",
    "\n",
    "def mk_html(index, batch, preds, normalized_weights, TEXT):\n",
    "    '''\n",
    "    HTMLデータを作成する\n",
    "    '''\n",
    "    \n",
    "    # indexの結果を抽出\n",
    "    sentence = batch.Text[0][index]\n",
    "    label = batch.Label[index]\n",
    "    pred = preds[index]\n",
    "    \n",
    "    # ラベルと予測結果を文字に置き換え\n",
    "    if label == 0:\n",
    "        label_str = 'Negative'\n",
    "    else:\n",
    "        label_str = 'Positive'\n",
    "        \n",
    "    if pred == 0:\n",
    "        pred_str = 'Negative'\n",
    "    else:\n",
    "        pred_str = 'Positive'\n",
    "        \n",
    "    # 表示用のhtmlを作成\n",
    "    html = '正解ラベル:{}<br>推論ラベル: {}<br><br>'.format(label_str, pred_str)\n",
    "    \n",
    "    # self-attentionの重みを可視化. multi-headが１２個なので１２種類のアテンションが存在する\n",
    "    for i in range(12):\n",
    "        # indexのAttentionを抽出と規格化\n",
    "        # ０単語目[CLS]のi番目のmulti-head attentionを取り出す\n",
    "        # indexはミニバッチの何番目のデータ化を示す\n",
    "        attens = normalized_weights[index, i, 0, :]\n",
    "        attens /= attens.max()\n",
    "        \n",
    "        html += '[BERTのAttentionを可視化_' + str(i+1) + ']<br>'\n",
    "        for word, attn in zip(sentence, attens):\n",
    "            \n",
    "            # 単語が[SEP]の場合は文章が終わりなのでbreak\n",
    "            if tokenizer_bert.convert_ids_to_tokens([word,numpy().tolist()])[0] == '[SEP]':\n",
    "                break\n",
    "            \n",
    "            # 関数hightlightで色を付ける\n",
    "            # 関数tokenizer_bert.convert_ids_to_tokensでIDを単語に戻す\n",
    "            html += highlight(tokenizer_bert.convert_ids_to_tokens([word.numpy().tolist()])[0], attn)\n",
    "        html += '<br><br>'\n",
    "        \n",
    "    # 12種類のAttentionの平均を求める。最大値で規格化\n",
    "    all_attens = attens*0\n",
    "    for i in range(12):\n",
    "        attens += normalized_weights[index, i, 0, :]\n",
    "    attens /= attens.max()\n",
    "    \n",
    "    html += '[BERTのAttentionを可視化_ALL]<br>'\n",
    "    for word, attn in zip(sentence, attens):\n",
    "        # 単語が[SEP]だと文章終わりなのでbreak\n",
    "        if tokenizer_bert.convert_ids_to_tokens([word.numpy().tolist()])[0] == '[SEP]':\n",
    "            break\n",
    "            \n",
    "        # 関数hightlightで色を付ける\n",
    "        # 関数tokenizer_bert.convert_ids_to_tokensでIDを単語に戻す\n",
    "        html += highlight(tokenizer_bert.convert_ids_to_tokens([word.numpy().tolist()])[0], attn)\n",
    "    html += '<br><br>'\n",
    "        \n",
    "    return html\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "\n",
    "index = 3\n",
    "html_output = mk_html(index, batch, preds, attention_probs, TEXT)  # HTML作成\n",
    "HTML(html_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformerではうまくいかなかった文章の推論結果とAttention\n",
    "index = 61\n",
    "html_output = mk_html(index, batch, preds, attention-probs, TEXT)\n",
    "HTML(html_output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
